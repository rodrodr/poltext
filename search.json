[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Análisis de textos políticos con R",
    "section": "",
    "text": "Prefacio\nAquí el prefacio del libro\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introducción",
    "section": "",
    "text": "Aquí la introducción."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Resumen",
    "section": "",
    "text": "En resumen, este libro se trata aún de un trabajo en preparación."
  },
  {
    "objectID": "01_explora_inductivo.html#introducción",
    "href": "01_explora_inductivo.html#introducción",
    "title": "\n2  Un primer acercamientoExploración inductiva de textos\n",
    "section": "\n2.1 Introducción",
    "text": "2.1 Introducción\nLa Teogonía de Hesíodo describe el paso del Caos -el estado de desorden originario- hacia un orden divino por medio de un catálogo de nacimientos y creación de diferentes dioses y seres mitológicos. Los filósofos presocráticos, por otra parte, intentaban explicar el origen del mundo a partir de un principio ordenador fundamental (ἀρχή), una idea clave a partir de la cuál se derivaba todo lo demás. En la primera visión, el mundo se delinea a partir de una serie de eventos sucesivos y caprichos de los dioses en su lucha por protagonismo. No existe un principio claro, sino que tal empresa depende de actos de voluntad, poder y negociación entre los participantes. En la segunda, existe una ley inexorable que estructura nuestra realidad y cabe al pensador desvelarla por medio de la razón.\nEn lo que tange al análisis de textos también podemos encontrar dos acercamientos análogos. Uno puede adentrar en sus “misterios” desproveído de nociones anteriores y explorar los patrones y estructuras que puedan emerger sin el auxilio de guías previos que orienten tal aventura. El otro modo consiste en tener un norte claro desde el principio y emplear nociones e hipótesis como lentes que orientan el examen de los documentos. Al primero lo llamamos inductivo y al segundo deductivo.\nCada una de esas maneras de mirar hacia los textos subraya una forma alternativa de aprender. El método inductivo conduce a un viaje errante, sin destino cierto, pero plagado de sorpresas y nuevos descubrimientos. El deductivo, por su parte, supone un destino y un rumbo a la vista, permite pocos desvíos. Aunque esté abierto a la serendipia, los principios de partida y los objetivos suelen ser definidos de antemano.\nSe consideran antagónicos solamente bajo sus formas típicas o ideales. En el trabajo de análisis, sin embargo, resulta muy poco frecuente examinar cualquier documento sin nociones previas o al menos cierta intuición de qué se podría encontrar. Tampoco abundan trabajos que germinan provistos de un catálogo meticuloso de los instrumentos adecuados y rutas correctas hacia los objetivos. La ciencia es una labor errante. Conocer a partir de textos es un proceso iterativo, que supone múltiples acercamientos sucesivos, idas y venidas constantes. La combinación de ambos acercamientos, aunque predomine uno de ellos, resulta inevitable.\nEsta parte del trabajo se centra en algunas estrategias inductivas para el análisis de textos. Se ha decidido solamente emplear aquí métodos que no requieran la lectura previa de los textos por dos razones. La primera es didáctica y evitar confundir tales métodos con otros de carácter deductivo. La segunda, igualmente importante, consiste en señalar la utilidad del método inductivo para la generación de hipótesis, desarrollo de diccionarios e identificación de temas en corpus formados por un elevado número de documentos. Se trata de una combinación entre una introducción metodológica y su aplicación práctica inmediata. Empezaremos con la apertura de los textos, la creación de un corpus para luego aplicar distintas técnicas preparatorias que ayudarán a viabilizar el análisis."
  },
  {
    "objectID": "01_explora_inductivo.html#primeros-pasos",
    "href": "01_explora_inductivo.html#primeros-pasos",
    "title": "\n2  Un primer acercamientoExploración inductiva de textos\n",
    "section": "\n2.2 Primeros pasos",
    "text": "2.2 Primeros pasos\n\n2.2.1 Creaccion de un corpus\nUna vez los documentos han sido preparados y pre-procesados, pueden ser abiertos en R. La función readtext del paquete con el mismo nombre permite importar (o “abrir”) textos individuales o carpetas enteras. Los documentos pueden ser de diferentes formatos: txt, doc(x), pdf, html, csv, tab, tsv, xml, xls(x), json, odt, o rtf. Se trata de una función muy útil para importar volúmenes grandes de texto.\n\nCódigo# Obtiene una lista de archivos en\n# una carpeta online de Github\nlibrary(jsonlite)\n\nurl &lt;- \"https://api.github.com/repos/rodrodr/tenet_texts/contents/spa.inaugural\"\n\nnm &lt;- read_json(url)\nnm &lt;- list2DF(nm)\nnm &lt;- sort(as.character(unlist(nm[8,])))\n\n# Carga el paquete\nlibrary(readtext)\n\n# Importa los textos\ntx &lt;- readtext(nm)\n\n# Ordena por nombre de archivo\ntx &lt;- tx[order(tx$doc_id),]\n\n# Visualiza los resultados\nreactable::reactable(tx,\n                     resizable = T, \n                     wrap = F)\n\n\n\n\n\n\n\nComo se puede observar, se cargan 15 discursos de investidura de los Presidentes de gobierno de España desde 1979 hasta la actualidad. Se trata de un objeto de tipo data.frame con dos columnas: doc_id, en general el nombre del archivo, y text, que contiene el texto integral. Este formato servirá de base y resulta obligatorio para la transformación de esos textos en un objeto de tipo corpus perteneciente al paquete quanteda, base o infraestructura de la mayor parte de los análisis realizados durante todo el curso.\nAdemás de doc_id y text, el data.frame, uno puede añadir más variables que ayuden a contextualizar los documentos y suministren información útil para el posterior análisis. No obstante, hay que tener claro que la función readtext solamente genera las dos primeras variables. Los metadatos adicionales deben ser añadidos a posteriori, sea justo después de la importación o, luego, como documentación del corpus, como veremos más adelante.\nEl hecho de que utilicemos textos guardados en una carpeta en la nube hace con que el código arriba sea un poco más complejo del que sería necesario. En el caso de que los archivos estén en el disco duro bastaría con informar el camino hacia la carpeta:\n\nCódigo# Carga el paquete\nlibrary(readtext)\n\n# Importa los textos\ntx &lt;- readtext(\"/Escritorio/Carpeta/\")\n\n# Ordena por nombre de archivo\ntx &lt;- tx[order(tx$doc_id),]\n\n# Visualiza los resultados\nreactable::reactable(tx,\n                     resizable = T, \n                     wrap = F)\n\n\nUna vez abiertos los datos, existen dos opciones. La primera es tratar los datos para extraer metadatos o agregar/fragmentar los textos en otras unidades de observación (como los tweets de un mismo partido, o fragmentar un libro por capítulos). La segunda consiste en transformar el data.frame en un objeto corpus y seguir con el análisis:\n\nCódigo# Carga el paquete quanteda\nlibrary(quanteda)\n\n# Transforma los textos en corpus\ncp &lt;- corpus(tx)\n\n# Visualiza los resultados\nreactable::reactable(summary(cp),\n                     resizable = T, \n                     wrap = F)\n\n\n\n\n\n\n\nAl explorar el objeto corpus por medio de la función summary(cp), vemos un conjunto de variables descriptivas: Text, nombre del documento; Types, señala el número de palabras y símbolos únicos en el documento; Tokens, número total de palabras y símbolos; y Sentences, cantidad de frases en el texto.\n\n2.2.2 Adicionar metadatos\nEl siguiente paso consiste en adicionar más información contextual (metadatos) sobre los textos. Tales informaciones resultarán de mucha utilidad en las siguientes etapas de análisis, puesto que permitirán agregar las informaciones según distintas características. Por ejemplo, podemos decidir agrupar los textos según presidente (y no gestión o legislatura). También podríamos organizar el análisis según partido del presidente o por su ideología.\nCuanto mayor la documentación de los textos, mayores las posibilidades de reagrupar, fragmentar o reordenar los textos según distintas categorías analíticas. Además, se posibilitan distintas comparaciones entre grupos y entre éstos con el patrón general.\nLa función docvars posibilita crear nuevas variables contextuales o de metadatos en un corpus. Su sintaxe resulta muy sencilla:\ndocvars(corpus,“variable name” ) &lt;- variable con el contenido.\n\nCódigodocvars(cp, \"Presidente\") &lt;- c(\"Adolfo Suárez\",\n                               \"Leopoldo Calvo Sotelo\",\n                               \"Felipe González\",\n                               \"Felipe González\",\n                               \"Felipe González\",\n                               \"Felipe González\",\n                               \"José María Aznar\",\n                               \"José María Aznar\",\n                               \"José Luis Zapatero\",\n                               \"José Luis Zapatero\",\n                               \"Mariano Rajoy\",\n                               \"Mariano Rajoy\",\n                               \"Mariano Rajoy\",\n                               \"Pedro Sánchez\",\n                               \"Pedro Sánchez\")\n\ndocvars(cp, \"Nombramiento\") &lt;- c(\"1979-03-31\",\n                                 \"1981-02-26\",\n                                 \"1982-12-02\",\n                                 \"1986-06-23\",\n                                 \"1989-12-05\",\n                                 \"1993-07-09\",\n                                 \"1996-05-04\",\n                                 \"2000-04-26\",\n                                 \"2004-04-17\",\n                                 \"2008-04-11\",\n                                 \"2011-12-20\",\n                                 \"2015-12-21\",\n                                 \"2016-10-30\",\n                                 \"2018-06-01\",\n                                 \"2020-01-07\")\n\n\ndocvars(cp, \"Cese\") &lt;- c(\"1981-02-26\",\n                         \"1982-12-02\",\n                         \"1986-06-23\",\n                         \"1989-10-30\",\n                         \"1993-06-07\",\n                         \"1996-03-04\",\n                         \"2000-03-13\",\n                         \"2004-03-15\",\n                         \"2008-03-10\",\n                         \"2011-11-21\",\n                         \"2015-12-21\",\n                         \"2016-10-29\",\n                         \"2018-06-01\",\n                         \"2019-04-29\",\n                         NA)\n\n\ndocvars(cp, \"Partido\") &lt;- c(\"UCD\", \"UCD\", \"PSOE\", \n                            \"PSOE\", \"PSOE\",\"PSOE\",\"PP\",\n                            \"PP\",\"PSOE\",\"PSOE\",\n                            \"PP\",\"PP\",\"PP\",\n                            \"PSOE\",\"PSOE\")\n\n\n\ndocvars(cp, \"Ideología\") &lt;- c(\"Derecha\", \"Derecha\", \"Izquierda\", \n                            \"Izquierda\", \"Izquierda\",\"Izquierda\",\n                            \"Derecha\", \"Derecha\", \"Izquierda\",\n                            \"Izquierda\", \"Derecha\", \"Derecha\", \n                            \"Derecha\", \"Izquierda\", \"Izquierda\")\n\n\n# Visualiza los resultados\nreactable::reactable(summary(cp),\n                     resizable = T, \n                     wrap = F)\n\n\n\n\n\n\nComo se puede observar en la tabla arriba, se han añadido las variables con el nombre del presidente, fecha de nombramiento, cese, el partido político al que pertenecía y la ideología de la mayor parte de los miembros de los partidos. Estas categorías permitirán separar en la fase de análisis diferentes perfiles de grupo como, por ejemplo, los conceptos o expresiones más utilizados por líderes de derecha e izquierda o por cada partido.\n\n2.2.3 Transformar un corpus\nOtra opción consiste en reorganizar el texto según nuevas unidades de análisis. A veces, algunos aspectos del discurso se desvelan de modo más claro cuando las información se organiza desde una perspectiva distinta. Esa pseudo-alteridad se puede alcanzar a veces por mirar a un mismo texto desde otro ángulo. ¿Qué cambios se pueden observar en la importancia de los conceptos cuando organizamos los textos según partido o ideología y no más de acuerdo con cada una de las legislaturas? ¿Aparece algo nuevo? ¿Existen contradicciones o patrones distintos frente a lo que habíamos percibido en el análisis anterior?\nSe pueden adoptar dos estrategias fundamentales. La primera consiste en fragmentar los textos en unidades menores como párrafos o sentencias, por ejemplo. La segunda trata de agregar los textos a partir de características comunes, como juntar todos los documentos de una misma ideología. Además, se pueden combinar entre sí. Podemos juntar todos los textos por partido y luego fragmentarlos por frase. De cualquier forma, el cambio en la unidad de observación debe tener un propósito analítico claro. ¿Qué se quiere aprender al estructurar los textos de una manera determinada?\nEmpecemos con la fragmentación. Utilicemos el corpus de discursos de inauguración de los presidentes de gobierno españoles y dividamos el corpus por párrafo. Esto se puede hacer con la función corpus_reshape de quanteda.\n\nCódigo# Reorganiza el corpus segun frases\ncs &lt;- corpus_reshape(x = cp, to = \"sentences\")\n\n# Visualiza los 100 primeros resultados\nreactable::reactable(summary(cs),\n                     resizable = T, \n                     wrap = F)\n\n\n\n\n\n\n\nPara agregar los textos, hay que dar un paso atrás y aunar los textos de un mismo grupo en un único documento. Para ello, podemos convertir el corpus documentado en un objeto de tipo data.frame y, luego, agregar los textos y volver a crear un corpus con la nueva unidad de observación. Utilizaremos ahora los presidentes como unidad.\n\nCódigo# Convierte el corpus documentado en un data.frame\ntd &lt;- convert(cp, to=\"data.frame\")\n\n# Unifica los textos en un solo documento a partir\n# de las funciones aggregate (que agrega por grupos)\n# y paste0, que colapsa textos.\n# Hemos decidido utilizar dos separadores de linea (\\n\\n)\n# para indicar la separacion entre un texto y otro\ntd &lt;- aggregate(list(text=td$text), by=list(Presidente=td$Presidente,\n                                      Partido=td$Partido,\n                                      Ideologia=td$Ideología),\n                                paste0,\n                                collapse=\"\\n\\n\")\n\n# vuelve a crear un corpus con el nuevo\n# objeto agregado\ncx &lt;- corpus(td)\n\n# Visualiza los 100 primeros resultados\nreactable::reactable(summary(cx),\n                     resizable = T, \n                     wrap = F)\n\n\n\n\n\n\n\nAhora mismo tenemos solamente siete documentos en el corpus. Corresponden a la nueva unidad de agregación: Presidente. Con dichas informaciones, la comparación se hace entre estilos discursivos de los líderes, más que un período sobre otro. Haríamos lo mismo para los partidos o la ideología. Incluso podríamos utilizar un corpus para cada unidad y comparar los resultados."
  },
  {
    "objectID": "01_explora_inductivo.html#el-arte-de-contar-palabras",
    "href": "01_explora_inductivo.html#el-arte-de-contar-palabras",
    "title": "\n2  Un primer acercamientoExploración inductiva de textos\n",
    "section": "\n2.3 El arte de contar palabras",
    "text": "2.3 El arte de contar palabras\nUna vez terminada la preparación del corpus, toca empezar el análisis. El modo más sencillo consiste en identificar qué palabras, conceptos o términos aparecen con mayor frecuencia y averiguar si hay diferencias sustantivas en su uso entre los documentos del corpus. Se trata de un método de análisis aplicable tanto a conjuntos pequeños de textos, que pueden ser leídos con antelación por el investigador, como a grandes repositorios imposibles de leer sin un proceso previo de análisis, clasificación y muestreo.\nEn esta parte del trabajo trataremos de cuatro temas relacionados con el recuento directo de palabras. El primero abarca las técnicas de preparación y cálculo de frecuencias de palabras, tanto para el corpus como un todo como para cada documento o grupo en particular. El segundo repite las operaciones, pero en lugar de palabras completas, se emplearán sus raíces (stemming). El tercero describe la ponderación de las frecuencias por su ocurrencia en todos los documentos. El cuarto se dedica a visualizaciones, como las nubes de palabras.\n\n2.3.1 Frecuencia de palabras\nEl primer paso para contar palabras o expresiones consiste en tokenizar el corpus y, a continuación, crear una matriz documento-atributo (dfm, en su acrónimo en inglés). Se trata de un procedimiento sencillo que fragmenta cada texto en palabras, n-gramas (conjunto de n-palabras que aparecen juntas como en el bigrama “economía política”, por ejemplo) o incluso frases.\n\nCódigo# Crea un objeto de tipo tokens por palabra\ntk &lt;- tokens(cp)\n\n# Crea un objeto dfm\nfm &lt;- dfm(tk)\n\n# Buscamos las 10 palabras más frecuentes\ntopfeatures(fm)\n\n   de     ,    la     .     y   que    en    el     a   los \n11069 10122  7332  5247  5168  5022  4535  4095  3218  2974 \n\n\nComo podemos ver, no aprendemos nada de la política española mirando hacia los 10 términos más frecuentes. Todos son conectores o puntuación que se repiten sistemáticamente en cualquier texto. Probablemente, “de”, “que”, “y”, “la”, así como la coma o el punto y aparte serán las palabras y los símbolos más comunes en cualquier texto escrito en español. Tales palablas se conocen como stop words o “palabras vacías” de contenido que suelen ser muy frecuentes en cualquier idioma. Para evitar que ellas supongan un problema, lo mejor es quitarlas del medio y recrear la matriz de frecuencias.\n\nCódigo# Crea un objeto de tipo tokens por palabra eliminando la punctuacion\ntk &lt;- tokens(cp, remove_punct = T)\n\n# Elimina las stopwords\ntk &lt;- tokens_remove(tk, stopwords(language = \"es\"))\n\n# Crea un objeto dfm\nfm &lt;- dfm(tk)\n\n# Buscamos las 10 palabras más frecuentes\ntopfeatures(fm)\n\n gobierno    españa  política  señorías    social      país  sociedad españoles \n      720       589       483       463       316       289       288       263 \n   empleo   sistema \n      255       253 \n\n\nAhora el panorama ha cambiado. Aparecen nuevos términos, como “gobierno”, “España”, “españoles”, “política”, “social”, “sociedad”, “empleo” o “sistema”. También palabras específicas de tratamiento formal en los discursos inaugurales o en intervenciones parlamentarias, como es el caso de “señorías”.\nUno puede visualizar la frecuencia de palabras en un corpus por medio de una nube de palabras. Aunque sea un recurso más estético que informativo, sirve para tener una idea somera e inicial del peso relativo de los términos en un corpus o documento específico. El código abajo utiliza la función wordcloud del paquete homónimo para generar el gráfico:\n\nCódigolibrary(quanteda.textplots)\nlibrary(tenet)\nlibrary(wordcloud)\n\nft &lt;-topfeatures(fm, 50)\n\npar(mar=rep(0,4))\nwordcloud(names(ft), freq = ft, colors = returnPalette(\"PonyoMedium\"))\n\n\n\n\nAbajo, buscamos las 25 palabras más comunes, calculamos su frecuencia relativa y, además, creamos dos gráficos para representarlas. Utilizamos la función dfm_weight para obtener el peso relativo de los términos en el corpus. Esta última medida ponderada resulta especialmente importante: (a) cuando comparamos su peso en cada uno de los textos y (b) cuando la extensión de los documentos resulta muy distinta.\n\nCódigo# Buscamos las 25 palabras más frecuentes\nft &lt;- topfeatures(fm, n = 25)\n\n# Añadimos la frecuencia relativa \nfp &lt;- dfm_weight(fm, \"prop\")\n\n# Repite la búsqueda para la frecuencia relativa\nfr &lt;- topfeatures(fp, n = 25)\n\n# Convierte los resultados en un data.frame\nxx &lt;- data.frame(Palabra=names(ft), Frec.Abs=ft, Frec.Rel=fr)\n\n# Carga el paquete ggplot2\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(grid)\n\n# Genera un gráfico de barras para visualizar la frecuencia de las palabras\np1 &lt;- ggplot(xx, aes(x=Frec.Abs, y=reorder(Palabra, Frec.Abs)))+\n  geom_bar(stat=\"identity\", fill=\"darkgreen\")+\n  theme_classic()+\n  labs(title=\"Frecuencia ABSOLUTA\")+\n  ylab(\"\")+\n  xlab(\"Frecuencia Absoluta\")\n\np2 &lt;- ggplot(xx, aes(x=Frec.Rel, y=reorder(Palabra, Frec.Rel)))+\n  geom_bar(stat=\"identity\", fill=\"orange\")+\n  theme_classic()+\n  labs(title=\"Frecuencia RELATIVA\")+\n  ylab(\"\")+\n  xlab(\"Frecuencia Relativa\")\n\n# La función grid.arrange permite posicionar varios gráficos lado a lado o uno en cima del otro\ngrid.arrange(p1,p2, ncol=2)\n\n\n\n\nEl próximo paso sería calcular las frecuencias según un grupo o atributo del corpus, como el presidente, por ejemplo. El código abajo utiliza las funciones dfm_group para generar una matriz de frecuencia y dfm_weight para ponderar las palabras y obtener los valores relativos.\n\nCódigo# Crea un objeto dfm\nfg &lt;- dfm_group(fm, groups = quanteda::docvars(cp, \"Presidente\"))\n\n\n# Buscamos las 25 palabras más frecuentes para cada presidente\nft &lt;- topfeatures(fg, n = 25, \n                  groups = quanteda::docvars(fg, \"Presidente\"))\n\n# Genera una frecuencia relativa\nfgw &lt;- dfm_weight(fg, scheme=\"prop\")\n\nftg &lt;- topfeatures(fgw, n = 25, \n                  groups = quanteda::docvars(fg, \"Presidente\"))\n\n# Crea una base de datos a partir de esas informaciones\nnm &lt;- names(ft)\nxx &lt;- data.frame()\nfor(i in 1:length(nm)){\n  xx &lt;- rbind(xx, data.frame(\n                          Presidente=nm[i], \n                          Palabras=names(ft[[i]]), \n                          Freq=as.numeric(ft[[i]]),\n                          Freq.Rel=round(as.numeric(ftg[[i]]),3)))\n}\n\n# Visualiza\nlibrary(htmltools)\n\n# Render a bar chart with a label on the left\nbar_chart &lt;- function(label, width = \"100%\", height = \"1rem\", fill = \"#00bfc4\", background = NULL) {\n  bar &lt;- div(style = list(background = fill, width = width, height = height))\n  chart &lt;- div(style = list(flexGrow = 1, marginLeft = \"0.5rem\", background = background), bar)\n  div(style = list(display = \"flex\", alignItems = \"center\"), label, chart)\n}\n\nlibrary(reactable)\n\nreactable(\n  xx,\n  filterable = T,\n  columns = list(\n    Presidente=colDef(name=\"Presidente\"),\n    Freq = colDef(name = \"Frecuencia\", align = \"left\", cell = function(value) {\n      width &lt;- paste0(value / max(xx$Freq) * 100, \"%\")\n      bar_chart(value, width = width)\n    }),\n    Freq.Rel = colDef(name = \"Frec. Relativa\", align = \"left\", cell = function(value) {\n      width &lt;- paste0(value / max(xx$Freq.Rel) * 100, \"%\")\n      bar_chart(value, width = width, fill=\"red\")\n    })\n  )\n)\n\n\n\n\n\n\nPodemos ver en la tabla resultante que una misma expresión puede tener pesos distintos en los discursos de diferentes presidentes. Por ejemplo, el término política tiene un peso de 0.012 en el discurso de Calvo-Sotelo, pero una incidencia seis veces menor en el de Pedro Sánchez. Algo parecido sucede con la palabra señorías, mucho más común en los documentos de Sánchez si comparados con los de Felipe González.\n\n2.3.2 Raíces (stemming)\nNo obstante, muchas de las palabras que aparecen en la tabla comparten una misma raíz como, por ejemplo, España, españoles, españolas o económico, económica o económicas. Contarlas de forma separada, en realidad, puede fragmentar o ocultar un patrón o un tema más relevante bajo un sinfín de pequeñas variantes de un mismo concepto.\nEn esos casos, una técnica muy útil es la conversión de las palabras a sus raíces (stemming). Este procedimiento sencillo permite justamente evitar que matices entre términos impidan la identificación de un patrón claro dentro del corpus o en algunos de sus textos componentes.\nLa función dfm_wordstem extrae la raiz de los términos de una matriz de frecuencia. Su empleo es muy sencillo, sin embargo, se debe establecer la lengua adecuada de los textos del corpus para que la transformación funcione. La función establece el inglés por defecto. En nuestro ejemplo, definiremos el parámetro language como “es” para definir que se trata de español.\n\nCódigo# Convierte las palabras a sus raices\nfw &lt;- dfm_wordstem(fm, language = \"es\")\n\n# Buscamos las 25 palabras más frecuentes\ntopfeatures(fw, n = 25)\n\n    polit   gobiern     señor     españ    econom    social   español       deb \n      769       724       636       590       565       497       491       444 \n     pais    public       hac     mejor      nuev     comun      part desarroll \n      426       402       401       356       355       344       339       308 \n     pued     emple    socied  ciudadan     mayor    reform   autonom    sistem \n      300       289       288       285       285       280       277       276 \n    objet \n      264 \n\n\nLos resultados reducen la variedad, político, política, políticas se transforman en polit. Gobierno, gobierna, gobiernan en gobiern. Sin embargo, los resultados pueden mejorar. España está de un lado como españ, mientras que español, españoles y españolas se reducen a español. Sin embargo, el investigador siempre puede utilizar esos resultados como punto de partida y, en seguida, agregar o corregir lo que crea necesario.\n\n2.3.3 TF-IDF\nOtro método de selección de términos relevantes es llamado Term Frequency-Inverse Document Frequency (TF-IDF). La fórmula es intuitiva y premia aquellos casos que aparecen con mucha frecuencia, pero en relativamente pocos documentos, al mismo tiempo que penaliza los que están por todas partes:\n\\[tf/idf = freq_{td} * log(\\frac{D}{d_t}) \\]\nDonde:\nfreqtd es la frecuencia absoluta (o relativa) del término t en el cada documento d.\nD corresponde al número total de documentos.\ndt representa el número de documentos que contienen el término t.\nImaginemos un corpus con 10 documentos y dos palabras “la” y “pobreza”, ambas con una frecuencia de 20. La única diferencia es que “la” aparece en todos los 10 documentos con una frecuencia de 2 en cada uno, mientras que “pobreza” se menciona en solamente dos textos, uno 14 veces y otro 6. Al calcular el tf-idf para cada una, tenemos los siguientes resultados:\n\nPara cada uno de los documentos de “la”: 2*log(10/10) = 0\nPara el primer documento de “pobreza”: 14*log(10/2) = 22,5\nPara el segundo documento de “pobreza”: 6*log(10/2) = 9,7\n\nAl final, se observa que el peso de “la” se anula completamente (tf-idf = 0) tanto por la dispersión de la frecuencia total como por su aparición en muchos documentos. Lo inverso ocurre con “pobreza”, que tiene su frecuencia concentrada en dos textos y con mayor preponderancia en uno en concreto (un tf-idf total de 22,5 + 9,7 = 32,2).\nEn líneas generales permite identificar aquellas palabras que aparecen mucho, pero que no tanto para reducir su poder informativo. Por ejemplo, “la”, “de”, “el” o “ser”, “hacer” aparecen un número elevado de veces. Esta medida permite ponderar su peso por un factor que penaliza el hecho de aparezcan mucho en todos los documentos. El resultado son indicadores más elevados para conceptos que se destacan sin ser preponderantes o muy comunes en todos los elementos del corpus.\n\nCódigo# Convierte las palabras a sus raices\nfw &lt;- dfm_tfidf(fm)\n\n# Buscamos las 25 palabras más frecuentes\ntopfeatures(fw, n = 25)\n\n     digital consiguiente  progresista    coalición        vista      ustedes \n    18.94303     18.70318     17.79497     16.69924     16.65308     15.91760 \n       vamos           ss        euros       género            `      ejemplo \n    15.89324     15.26788     14.72378     14.31364     14.11310     13.92790 \n  presidenta       señora   transición        pacto  lógicamente  comunitaria \n    13.77675     13.20272     12.73408     11.79811     11.48063     11.48063 \n   ecológica         acta      pobreza         idea   revolución         2030 \n    11.37580     11.18352     10.97379     10.92005     10.90659     10.50074 \n        digo \n    10.48455 \n\n\nVemos que otros términos aparecen: digital, progresista, coalición, euros, género, transición, pacto, comunitaria, ecológica, pobreza y 2030. Tales términos sugieren contenido programático de la política y despiertan mayor interés que el lenguaje más formal que hemos visto hasta ahora. Por otra parte, verbos y expresiones muy peculiares de cada presidente, como el vamos de Pedro Sánchez o el consiguiente de Felipe González, saltan a la vista. Tales ejemplos evidencían cómo la medida tf-idf puede ayudar a singularizar el discurso de un presidente o de un partido político tanto por el contenido político que por las fórmulas lingüísticas empleadas para dirigirse a los miembros del Poder Legislativo. Además, como en los ejemplos anteriores, se pueden detallar los resultados por grupo (Presidente, partido, ideología, entre otras variables de contexto disponibles).\n\n2.3.4 N-gramas\nEn varias ocasiones conviene explorar la combinación de palabras en búsqueda de expresiones comunes o recurrente. Algunos ejemplos claros en la política son “seguridad social”, “fuerzas armadas”, “políticas públicas”, “seguridad ciudadana”, “partido político”, entre otras. Para ello, utilizamos n-gramas, que son secuencias de n-palabras seguidas. Se llaman así porque pueden ser dos (bigramas), tres (trigramas) o más términos sucesivos.\nEn R, se trata de fransformar los tokens en n-gramas utilizando la función tokens_ngrams y, a continuación, calcular las frecuencias:\n\nCódigo# Convierte los tokens en bigramas\ntn &lt;- tokens_ngrams(tk, n=2)\n\n# Crea una matriz de frecuencia\nfk &lt;- dfm(tn)\n\n# Extrae los 25 mas comunes\ntopfeatures(fk, 25)\n\n    comunidades_autónomas             unión_europea         política_exterior \n                       94                        67                        60 \n          señoras_señores               punto_vista         señores_diputados \n                       51                        51                        49 \n        sociedad_española        política_económica               cuatro_años \n                       48                        45                        43 \n         seguridad_social             próximos_años                  cada_vez \n                       41                        40                        40 \n         confianza_cámara           creación_empleo          señor_presidente \n                       40                        40                        39 \n          acción_gobierno         fuerzas_políticas administraciones_públicas \n                       38                        38                        36 \n             primer_lugar                  debe_ser     formación_profesional \n                       35                        34                        34 \n               si_obtengo              últimos_años              mismo_tiempo \n                       34                        34                        31 \n        economía_española \n                       30 \n\n\nVarios bigramas interesantes saltan a la vista: comunidades autónomas, unión europea, política exterior, política económica, seguridad social, creación empleo, entre otros. También aparecen fórmulas retóricas como señoras señores, si obtengo, confianza cámara, por ejemplo.\nPodemos repetir el mismo procedimiento, pero ahora utilizando tres palabras en lugar de dos para ver qué resultados obtenemos. Este juego de ir subiendo el número de palabras en la expresión puede seguir indefinidamente hasta que no aporte ningún dato nuevo o interesante.\n\nCódigo# Convierte los tokens en trigramas\ntn &lt;- tokens_ngrams(tk, n=3)\n\n# Crea una matriz de frecuencia\nfk &lt;- dfm(tn)\n\n# Extrae los 25 mas comunes\ntopfeatures(fk, 25)\n\n          señoras_señores_diputados                si_obtengo_confianza \n                                 48                                  23 \n           obtengo_confianza_cámara                próximos_cuatro_años \n                                 18                                  17 \n            producto_interior_bruto         comunidad_económica_europea \n                                 17                                  15 \n          sistema_público_pensiones            señor_presidente_señoras \n                                 13                                  12 \n         presidente_señoras_señores    legislatura_discurso_investidura \n                                 12                                  11 \n          solicito_confianza_cámara                 últimos_cuatro_años \n                                 11                                  11 \n                   idea_españa_país            artículo_99_constitución \n                                 11                                  10 \n            todas_fuerzas_políticas           fuerzas_cuerpos_seguridad \n                                  9                                   9 \n           creación_puestos_trabajo              si_obtengo_investidura \n                                  9                                   9 \n           españa_necesita_gobierno            sistema_seguridad_social \n                                  9                                   8 \n                gobierno_si_obtengo              sistema_nacional_salud \n                                  8                                   8 \n         señora_presidenta_señorías comunidades_autónomas_ayuntamientos \n                                  8                                   7 \n    todas_administraciones_públicas \n                                  7 \n\n\nLa obtención de la confianza del parlamento aparece en más de una vez. El artículo 99 de la Constitución española (que rije el proceso de voto de confianza en el Presidente) resulta la novedad más clara en ese apartado. No obstante, otros términos relacionados a las políticas públicas -como producto interior bruto, sistema público pensiones, fuerzas cuerpos seguridad- o a la organización administrativa del Estado -comunidades autónomas ayuntamientos o todas administraciones públicas- también se destacan.\nSi llegamos a aumentar el n a 5, por ejemplo, aparece el I+D+I. En resumen, se trata de un recurso exploratorio bastante interesante para determinar qué expresiones compuestas o frases aparecen de forma repetitiva en los textos y que puedan incitar nuevas perspectivas sobre el contenido de los mismos.\n\n2.3.5 Keyness\nEl keyness es otro método que compara la distribución desigual de términos entre textos. A partir de un texto de referencia, utiliza métodos estadísticos como el chi-cuadrado o la likelihood ratio para determinar cuáles palabras se acercan más a un documento y las que menos. A partir de esas informaciones podemos encontrar elementos útiles para caracterizar un discurso concreto.\nla función textstat_keyness del paquete quanteda.textstats permite calcular el keyness de los términos de un corpus con relación a un documento de referencia concreto. Utilicemos, por ejemplo, los discursos de Pedro Sánchez como referencia:\n\nCódigo# Nueva matriz de fecuencia\npfm &lt;- dfm(tk)\n\n# Atribuimos el nombre del presidente como grupo\npfm &lt;- dfm_group(pfm, groups = quanteda::docvars(pfm, \"Presidente\"))\n\n# Calcula el keyness\nkn &lt;- textstat_keyness(pfm, target = \"Pedro Sánchez\")\n\n# Visualiza los resultados en una tabla\nreactable(kn, \n          columns = list(\n                    chi2=colDef(\n                            format=colFormat(\n                            digits=2)),\n                    p=colDef(\n                            format=colFormat(\n                            digits=2))))\n\n\n\n\n\n\nVemos que las palabras que más se asocian al discurso de Sánchez son vamos, señorías, progresista, digital, avanzar y género. Las que menos son política, económica, esfuerzo, ciudadanos, proceso, exterior y cooperación. Podemos también compararlas visualmente utilizando la función textplot_keyness del paquete quanteda.textplots. En el gráfico abajo, se seleccionan las 20 palabras que más y menos caracterizan los textos de Pedro Sánchez.\n\nCódigolibrary(quanteda.textplots)\n\ntextplot_keyness(kn, color = c(\"red3\",\"blue\"))\n\n\n\n\nComo se trata de un gráfico basado en la arquitectura ggplot2, se pueden añadir elementos como títulos, temas, nuevos colores y otros elementos visuales."
  },
  {
    "objectID": "01_explora_inductivo.html#asociación-entre-palabras",
    "href": "01_explora_inductivo.html#asociación-entre-palabras",
    "title": "\n2  Un primer acercamientoExploración inductiva de textos\n",
    "section": "\n2.4 Asociación entre palabras",
    "text": "2.4 Asociación entre palabras\n\n2.4.1 Co-ocurrencias\nEl primer método de análisis de la asociación entre palabras explora la cantidad de veces en que dos palabras aparecen juntas en un corpus. Este fenómeno se denomina co-ocurrencia. Se calcula a partir de la función fcm de quanteda que genera una matriz de co-ocurrencia. Básicamente, se trata de una matriz NxN, donde N corresponde al número de palabras en el corpus.\n\nCódigo# Crea una matriz de co-ocurrencia\nfc &lt;- fcm(tk)\n\n# Selecciona las 50 co-ocurrencias mas frecuentes\nfeat &lt;- names(topfeatures(fc, 50))\n\nfc &lt;- fcm_select(fc, pattern = feat) \n\n# carga el paquete\nlibrary(quanteda.textplots)\n\n# genera la red\nlibrary(ggplot2)\nset.seed(pi)\ntextplot_network(fc, \n                 edge_color = \"red\", \n                 edge_alpha = 0.05, omit_isolated = T)\n\n\n\n\nEl código más arriba calcula la matriz de co-ocurrencia para el corpus de los discursos de investidura de los presidentes de gobierno de España y selecciona los 50 pares de términos más frecuentes. Con esos datos, genera un sociograma que representa las asociaciones más comunes entre palabras. El grosor de los vínculos revela la intensidad de su asociación y la centralidad de los nodos su peso o importancia en el conjunto de elementos seleccionados.\nComo podemos observar, España aparece en el centro, seguida por señorías, ley, vamos, partido y sistema. La red posee un núcleo más denso de palabras interconectadas entre sí y otro conjunto de términos periféricos, con escasos vínculos con este centro de la red.\n\n2.4.2 Co-localizaciones\nUn método adicional para el análisis de los vínculos entre términos es la co-localización. A diferencia de las co-ocurrencias, que se basan exclusivamente en las frecuencias, la función textstat_collocations del paquete quanteda.textstats utiliza un modelo log-linear para comparar la incidencia de un grupo de palabras y definir su grado de asociación. El coeficiente lambda (\\(\\lambda\\)) representa dicha estimación.\nLa co-localización define el grado de asociación de otra palabra cerca. Así se puede precedir qué palabra viene después a partir del conjunto que viene antes.\n\nCódigo# Genera una lista de 2 palabras que aparecen en secuencia\ncc &lt;- textstat_collocations(tk, size = 2)\n\nreactable(cc, \n          resizable=T, \n          rownames = F, \n          columns = list(\n                        lambda=colDef(format=colFormat(digits=2)),\n                        z=colDef(format=colFormat(digits = 2))))\n\n\n\n\n\n\n\n2.4.3 Correlación\nLa correlación corresponde a un método clásico de medir la asociación entre dos variables. Cuando se trata de la correlación entre términos podemos utilizar diferentes algoritmos. Silge y Robinson (2017), por ejemplo, emplean el coeficiente phi (\\(\\phi\\)) de Yule, un método de asociación a partir de la coincidencia binaria (1/0, Sí/No) entre palabras en un mismo documento. Como el rho (\\(\\rho\\)) de Pearson, posee un intervalo entre -1 y 1 y se interpreta del mismo modo.\nEsta medida puede resultar útil para textos cortos, como tweets o un corpus organizado según sentencias. En esos casos, importa menos la cantidad de las palabras que el hecho de que ambas aparezcan en un mismo documento. Se valora la coincidencia de dos conceptos o ideas y tiene poco sentido evaluar su intensidad. La probabilidad de encontrar una palabra con frecuencia superior a 1 en una misma frase u oración es pequeña.\nNo obstante, ese razonamiento también revela la principal limitación del coeficiente phi. Al tratarse de un test binario, no lleva en cuenta diferencias cuantitativas que pueden observarse en documentos más extensos como libros, capítulos, entrevistas, leyes, manifiestos o discursos. En estos casos, se requieren métodos más precisos que, además de la presencia o ausencia de los términos, ponderen la intensidad de asociación según su frecuencia o rango.\nLa correlación de orden de rango (rank-order correlation) o rho (\\(\\rho\\)) de Spearman resulta más indicada para esos casos. Se trata de una medida que ordena los documentos según el rango de frecuencia de cada palabra en concreto y compara el grado de similitud o diferencia entre los rangos. La fórmula es la siguiente:\n\\[S\\rho = 1-\\frac{6 \\sum R(X_i)-R(Yi)}{n(n^2-1)}\\]\nDonde:\n\nR(Xi) indica el rango (ranking) de Xi en los valores de X.\nR(Yi) indica el rango (ranking) de Yi en los valores de Y.\nn corresponde al número de observaciones. En nuestro caso, indica el número de documentos en el corpus.\n\n\nImportantePuesto que la distribución de las frecuencias de palabras no es normal, no se recomienda el empleo del rho (\\(\\rho\\)) de Pearson para avaliar su asociación. Tampoco resulta indicable el cálculo de la correlación para un corpus con un número muy reducido de documentos (menos de 10, por ejemplo). En tales escenarios, quizás sería mejor reestructurar el corpus según unidades menores -como sentencias o párrafos, por ejemplo- y emplear el phi (\\(\\phi\\)) como alternativa.\n\n\nEl código abajo crea una lista con nodos correspondientes a las palabras del corpus y vínculos que expresan la intensidad de su asociación y un sociograma representando la asociación entre las palabras. Por defecto, el método empleado es el rho de Spearman.\n\nCódigo# Carga el paquete tenet\nlibrary(tenet)\n\n# Crea una lista de correlaciones\nll &lt;- corTerms(cp, \n               min.freq = 100, \n               n.terms = 100)\n\n# Genera el sociograma\ncorNet(ll)\n\n\n\n\nCuanto más grandes los puntos, mayor la frecuencia de la palabra en el corpus. El grosor del vínculo revela la intensidad de asociación y el color su dirección. En el ejemplo arriba, correlaciones negativas se representan en rojo y positivas en azul. De ese modo, vemos que países y compromiso se relacionan de forma negativa. Fuerzas y social presentan una correlación positiva."
  },
  {
    "objectID": "01_explora_inductivo.html#consideraciones-finales",
    "href": "01_explora_inductivo.html#consideraciones-finales",
    "title": "\n2  Un primer acercamientoExploración inductiva de textos\n",
    "section": "\n2.5 Consideraciones finales",
    "text": "2.5 Consideraciones finales\nEn este documento hemos visto cómo abrir los textos en R y trabajar con distintas técnicas de análisis exploratorio. Nos hemos concentrado en métodos inductivos, sin una lectura anterior y profunda que orientara el análisis. Los ejemplos se han concentrado fundamentalmente en entender qué términos ocurren con mayor frecuencia y cómo se asocian entre ellos.\nEste tipo de análisis dista mucho de ser mínimamente aceptable dentro de una perspectiva cualitativista pura. Contar palabras constituye una aproximación somera al análisis de textos. No obstante, no tiene el propósito de reemplazar nada. Su utilidad reside en suministrar recursos y una primera aproximación a técnicas más profundas y sofisticadas. Los términos encontrados aquí sirven para la creación de diccionarios y la codificación temática. También ayudan a desvelar patrones no completamente observables desde una perspectiva cualitativa. Además, su poder reside en permitir extraer patrones de amplios volúmenes de texto, imposibles de leer uno a uno.\nEn la próxima sesión utilizaremos técnicas deductivas y otros métodos exploratorios para profundizar en el conocimiento de los textos. Trataremos de la codificación temática, la selección de textos según términos o atributos para un análisis más detallado y la creación de diccionarios como base para tareas de clasificación y descubierta."
  },
  {
    "objectID": "01_explora_inductivo.html#ejercicios",
    "href": "01_explora_inductivo.html#ejercicios",
    "title": "\n2  Un primer acercamientoExploración inductiva de textos\n",
    "section": "\n2.6 Ejercicios",
    "text": "2.6 Ejercicios\n\nUtilice el data.frame cis.corrupt del paquete tenet para crear un nuevo corpus. Realice el análisis del nuevo corpus utilizando la función summary. Guarde el resultado de summary en un data.frame llamado d.\n\n\nSolución# Carga los paquetes tenet y quanteda\nlibrary(tenet)\nlibrary(quanteda)\n\n# Convierte cis.corrupt en un corpus\ncx &lt;- corpus(cis.corrupt)\n\n# Resume los resultados\nd &lt;- summary(cx, n = nrow(cis.corrupt))\n\n\n\nAñada una variable de documentación (docvars) al corpus recién creado con la densidad o la diversidad léxica de cada texto dividiendo el número de types por tokens y multiplicando por 100. Formalmente, este término se denomina Type-Token Ratio (TTR). También añada otras dos variables a la documentación del corpus: (a) el número de palabras por sentencia y (b) el número de types por sentencia.\n\n\nSolución# Calcula el TTR\nd$TTR &lt;- round((d$Types/d$Tokens)*100,1)\n\n# Calcula el número de palabras por frase\nd$tokens_sentence &lt;- round(d$Tokens/d$Sentences,1)\n\n# Calcula el número de tipos por frase\nd$types_sentence &lt;- round(d$Types/d$Sentences,1)\n\n# Añade las tres variables como documentación del corpus\ndocvars(cx,\"TTR\") &lt;- d$TTR\ndocvars(cx,\"tokens_sentence\") &lt;- d$tokens_sentence\ndocvars(cx,\"types_sentence\") &lt;- d$types_sentence\n\n\n\nDivida el corpus en tokens bajo la forma de palabras. Excluyas puntuación, símbolos y palabras vacías (stop words). Cree un nuevo objeto con bi-gramas en lugar de solo palabras como tokens. Finalmente, genere una nueva lista de términos, pero ahora con solamente las raíces.\n\n\nSolución# Crea los tokens removiento puntuación y símbolos\ntk &lt;- tokens(cx,\n             remove_punct = T,\n             remove_symbols = T)\n\n# Elimina los stop words\ntk &lt;- tokens_remove(tk, \n                    stopwords(\"es\"))\n\n# Convierte en bi-gramas\ntb &lt;- tokens_ngrams(tk, 2)\n\n# Convierte en raíces\ntr &lt;- tokens_wordstem(tk, language = \"es\")\n\n\n\nCrea dos matrices de frecuencia: (a) una con las palabras y (b) otra con sus raíces. Selecciona las 30 palabras más frecuentes en cada una de ellas.\n\n\nSolución# Crea la matriz de frecuencia para las palabras\nfm &lt;- dfm(tk, tolower = T)\n\n# Crea la matriz de frecuencia para las raíces\nfr &lt;- dfm(tr, tolower = T)\n\n# Selecciona las 30 palabras más frecuentes\ntopfeatures(fm, 30)\n\n# Selecciona las 30 raíces más frecuentes\ntopfeatures(fr, 30)\n\n\n\nCrea una matriz de frecuencia para cada grupo demográfico (variable “Grupo.Demografico” en docvars) y selecciona las 10 palabras más comunes para cada uno de ellos.\n\n\nSolución# Agrupa la matriz por grupo demográfico\nfg &lt;- dfm_group(fm, groups = docvars(cx, \"Grupo.Demografico\"))\n\n# Selecciona los 10 más frecuentes\ntopfeatures(fg, groups = docvars(fg, \"Grupo.Demografico\"))\n\n\n\nAhora, utilice la matriz agrupada del ejercicio 5, calcule los TF-IDF de cada palabra y seleccione las 20 palabras más importantes para cada grupo.\n\n\nSolución# Calcula el TF-IDF\nfi &lt;- dfm_tfidf(fg)\n\n# Selecciona las 20 más frecuentes\ntopfeatures(fi,\n            n = 20,\n            groups = docvars(fg, \"Grupo.Demografico\"))\n\n\n\nUtilice la matriz de frecuencia de los grupos demográficos para calcular el keyness del corpus (con la función textstat_keyness) y crea el gráfico utilizando la función texplot_keyness. Utilice como referencia el grupo “Obreros”.\n\n\nSolución# Calcula el keyness\nkn &lt;- textstat_keyness(fg, target = \"Obreros\")\n\n# Carga el paquete\nlibrary(quanteda.textplots)\n\n# Genera el gráfico\ntextplot_keyness(kn, color = c(\"red3\",\"blue\"))\n\n\n\nCree una matriz de co-ocurrencia y, luego, genere el sociograma de la asociación de las 25 co-ocurrencias más frecuentes.\n\n\nSolución# Crea una matriz de co-ocurrencia\nfc &lt;- fcm(tk)\n\n# Selecciona las 25 co-ocurrencias mas frecuentes\nfeat &lt;- names(topfeatures(fc, 25))\n\nfc &lt;- fcm_select(fc, pattern = feat) \n\n# carga el paquete\nlibrary(quanteda.textplots)\n\n# genera la red\nlibrary(ggplot2)\nset.seed(pi)\ntextplot_network(fc, \n                 edge_color = \"blue\", \n                 edge_alpha = 0.05)\n\n\n\nCrea el gráfico de correlaciones para el corpus a partir de 100 palabras con frecuencia superior a 50 en el corpus.\n\n\nSolución# Carga el paquete tenet\nlibrary(tenet)\n\n# Crea una lista de correlaciones\nll &lt;- corTerms(cx,\n               min.freq = 50, \n               n.terms = 100)\n\n# Genera el sociograma\ncorNet(ll)"
  },
  {
    "objectID": "01_explora_inductivo.html#lecturas-adicionales",
    "href": "01_explora_inductivo.html#lecturas-adicionales",
    "title": "\n2  Un primer acercamientoExploración inductiva de textos\n",
    "section": "\n2.7 Lecturas adicionales",
    "text": "2.7 Lecturas adicionales\n\n\nIgnatow G, Mihalcea R (2018). “The Philosophy and Logic of Text Mining.” In An Introduction to Text Mining: Research Design, Data Collection, and Analysis, chapter 4. SAGE, Los Angeles.\n\n\nEste breve capítulo del manual de Ignatow y Mihalcea trata de las distintas perspectivas de exploración de textos en las ciencias sociales. Trata de las principales corrientes epistemológicas, las dos culturas (“cuantitativa” y “cualitativa”) y describe las características y principales diferencias entre las lógicas deductiva, inductiva y abductiva.\n\n\nBenoit K (2020). “Text as data: An overview.” In The SAGE Handbook of Research Methods in Political Science and International Relations, chapter 26, 461. SAGE, Thousand Oaks. Publisher: SAGE Publishing Ltd Thousand Oaks.\n\n\nSe trata de otro excelente texto para clasificar las perspectivas (ahora desde la ciencia política) sobre el análisis de texto. Propone una aclaración conceptual importante: un texto considerado como un conjunto de datos no puede ser confundido con un texto en su integridad. La conversión a datos supone transformaciones, abstracción y cierto grado de reducción. Sobre todo, deja claro qué NO debemos esperar de esa perspectiva de análisis para evitar frustraciones o inferencias inadecuadas.\n\n\nGrimmer J, Roberts ME, Stewart BM (2022). “Bag of Words.” In Text as Data: A New Framework for Machine Learning and the Social Sciences, chapter 5, 94-111. Princeton University Press, New Haven.\n\n\nEl texto de Grimmer, Roberts y Stewart representa una buena introducción a los principales conceptos tratados en esta sección. Introducen el análisis de tipo “saco de palabras” (bag of words) y explican los procesos de su implementación como la tokenización, la remoción de palabras vacías (stop words), la reducción a lemas o raíces y la creación de matrices de frecuencias.\n\n\nSilge J, Robinson D (2017). “‘Analyzing Word & Document Frequency - tf-idf’ y ‘Relationships between words: n-grams and correlations’.” In Text Mining with R: A Tidy Approach, chapter 4 and 5. O’Reilly Media, London. https://www.tidytextmining.com/.\n\n\nFinalmente, los dos capítulos de Silge y Robinson explican más detenidamente y de forma práctica varios análisis realizados en esta sección. Frecuencia de palabras, TF-IDF, la ley de Zipf (muy importante para comprender las frecuencias de palabras), así como los N-gramas y las correlaciones entre palabras."
  },
  {
    "objectID": "01_explora_inductivo.html#referencias",
    "href": "01_explora_inductivo.html#referencias",
    "title": "\n2  Un primer acercamientoExploración inductiva de textos\n",
    "section": "\n2.8 Referencias",
    "text": "2.8 Referencias\n\n\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with r: A Tidy Approach. London: O’Reilly Media. https://www.tidytextmining.com/."
  },
  {
    "objectID": "02_explora_deductivo.html#introducción",
    "href": "02_explora_deductivo.html#introducción",
    "title": "\n3  Juego de palabras:Exploración deductiva y abductiva de textos\n",
    "section": "\n3.1 Introducción",
    "text": "3.1 Introducción\n¿Cuál es la posición de los partidos con relación a la reducción de impuestos? ¿Cuáles mencionan la pobreza o el aumento de la desigualdad en sus manifiestos? ¿En qué textos aparece el tema de la inmigración como amenaza a la integridad social o política del país? ¿Cuáles diputados intervienen durante los plenos en favor de políticas que reduzcan las emisiones y frenen el cambio climático? Cada una de esas preguntas encierra un conjunto de opciones claras en términos empíricos, teóricos y metodológicos. Primero, se basan en el contenido textual como fuente de información empírica. Segundo, establecen la comparación como método, buscando diferencias significativas según partido o ideología. Finalmente, los partidos o diputados se dividen en grupos que hipotéticamente se antagonizan ante ciertas políticas consideradas clave. Existe un comportamiento esperado que se puede o no confirmar a partir de análisis del material seleccionado.\nA diferencia de lo que se hizo en el análisis inductivo, ya no se trata de ver aquí qué palabras o términos aparecen en un conjunto de documentos, sino de buscar cómo temas concretos se manifiestan, por quiénes, dónde y en qué contexto. El análisis exploratorio deductivo nos permite evaluar la prevalencia de un tema en los textos, verificar si su distribución resulta uniforme o se concentra de acuerdo con la ideología, el partido político o un momento histórico concreto.\nEn esta parte del trabajo revisaremos los instrumentos y estrategias disponibles para el desarrollo de temas. Examinaremos diferentes herramientas para determinar el contexto en el que se inscriben, así como determinaremos la prevalencia de distintas categorías analíticas. Haremos especial hincapié en el concepto de diccionario o léxico como el resultado de un proceso de codificación temática y construcción teórica a partir del análisis abductivo resultante de la consulta e iteración constante entre texto (como material empírico fundamental), teoría e interpretación (Thompson 2022; B. L. Kennedy and Thornberg 2018).\nEl texto siguiente se divide en tres sesiones. El segundo apartado se dedica a las técnicas de búsqueda de palabras en los textos y que resultan muy útiles para situar las ideas en su contexto. La tercera parte trata de la codificación temática a partir de diccionarios. Cómo la construcción y refinamiento de léxicos puede resultar de un proceso iteractivo que da lugar al desarrollo de nuevas categorías analíticas. Finalmente, el cuarto apartado explora los temas para identificar patrones, el grado de asociación entre ellos y su distribución según distintas categorías analíticas (como partidos o ideología, por ejemplo)."
  },
  {
    "objectID": "02_explora_deductivo.html#búsqueda-de-palabras",
    "href": "02_explora_deductivo.html#búsqueda-de-palabras",
    "title": "\n3  Juego de palabras:Exploración deductiva y abductiva de textos\n",
    "section": "\n3.2 Búsqueda de palabras",
    "text": "3.2 Búsqueda de palabras\nEn el apartado sobre el método inductivo aprendimos a contar palabras. No obstante, en muchas ocasiones, el significado de un término puede variar según el contexto. Según la Real Academia Española (RAE), la palabra “estrella” puede significar tanto un cuerpo celeste como una persona que sobresale en su profesión, por ejemplo. En los estudios políticos palabras como pueblo, democracia o libertad exigen que el analista establezca siempre el contenido concreto asociado a tales expresiones abstractas. El propósito de esta sección consiste en introducir algunas herramientas que permitan determinar el significado de una expresión de forma clara y con menos ambigüedad.\n\n3.2.1 Keyword in Context (Kwic)\n\nUna forma sencilla de contextualizar términos consiste en visualizarlos directamente en los pasajes del texto en que aparecen. El método kwic (keyword in context) extrae de un texto o corpus todos los trechos en los que aparece una palabra y los muestra dentro de un contexto o ventana que puede ser compuesta por una o más expresiones antecedentes y posteriores. En el ejemplo abajo, buscamos la palabra “libertad” en los discursos de investidura de los presidentes españoles, con una ventana de 5 palabras alrededor del término.\n\nCódigo# Carga el paquete tenet\nlibrary(tenet)\n\n# Crea un corpus (discursos inaugurales Espana)\ncp &lt;- corpus(spa.inaugural)\n\n# Crea un data.frame a partir de\n# la funcion Keyword in Context de\n# Quanteda \nd &lt;- kwic(x = tokens(cp),\n          pattern= \"libertad\",\n          window = 5)\n\n# Visualiza los resultados\n reactable::reactable(d,\n                     resizable = T, \n                     wrap = F)\n\n\n\n\n\n\n\nComo podemos observar, el uso del término libertad varía significativamente según el momento y el presidente en cuestión. Adolfo Suárez lo utiliza en un contexto de transición hacia la democracia, como superación de una etapa anterior autoritaria. Por esa razón, la palabra aparece junto a derechos, instituciones y democracia. Felipe González la utiliza junto a las ideas de igualdad y solidaridad, mientras que Aznar las asocia a la expresión, enseñanza y seguridad. Zapatero introduce el concepto de libertad sexual. Rajoy la asocia a prosperidad e igualdad, mientras que Pedro Sánchez se centra en dos ejes: valores postmateriales (sexual, aborto, eutanasia) y territorial (autonomía de las comunidades autónomas).\nPor otra parte, si hacemos un ejercicio y utilizamos el término “empleo”, se puede averiguar que este se refiere casi exclusivamente al mundo laboral y políticas activas de acceso o creación de puestos de trabajo. Solo en dos ocasiones específicas se trata del verbo “emplear” con el significado “utilizar”, como las referencias “emplear una política monetaria” o el “empleo de los caudales públicos”, ambas en el primer discurso de investidura de Felipe González. Por lo tanto, al examinar los resultados, vemos que esos dos casos constituyen una excepción al significado principal de empleo a que hacen referencia todos los discursos.\n\n3.2.2 Árbol de palabras\nEl árbol de palabras (wordtree) nos brinda una visión semejante al kwic con una diferencia fundamental: cada palabra que compone la frase se dimensiona de acuerdo con la frecuencia con que aparecen en los textos. Este recurso resulta útil para discriminar los usos más comunes de los términos en sus contextos predominantes. De acuerdo con Wattenberg y Viégas (2008, 2–3), corresponde a una alternativa gráfica y exploratoria de visualización de los kwic. Se trata, además, de un recurso interactivo que permite al usuario jugar con los contextos, direccionando su mirada hacia frases concretas o subiendo a patrones más generales. Posee tres características distintivas. Primero, facilita la identificación de repeticiones de palabras. Segundo, tiene una estructura de árbol claramente identificable. Finalmente, facilita la exploración del contexto.\nPor lo tanto, representa una herramienta que sirve tanto para la exploración de patrones en los textos durante una primera fase exploratoria de un estudio como de instrumento de comunicación de patrones o temas recurrentes encontrados en los datos. Su interactividad invita tanto a la descubierta como a una mayor atención a los argumentos que se desean transmitir.\nEl código abajo utiliza la función wordtree del paquete tenet para crear el árbol de palabras alrededor del término libertad. Como podemos ver, el resultado es muy semejante al producido por el kwic. No obstante, ahora nuestra atención se ve atraída por las palabras de mayor tamaño. Las rutas más comunes se evidencian, como es el caso de “de la libertad de expresión”, por ejemplo.\n\nCódigo# Crea un arbol de palabras en tenet\nwordtree(corpus = cp,\n         keyword = \"libertad\",\n         height = 800)\n\n\n\n\n\n\n  \n  \n                   \n                   \n\n\nPor otra parte, si filtramos el corpus para que incluya solamente textos de un presidente o partido político, podemos identificar los usos específicos que hacen de los términos y, así, trazar variantes y revelar patrones útiles teóricamente. Seguramente veremos diferencias sustantivas entre Adolfo Suárez y Pedro Sánchez, como hemos podido contrastar en el apartado anterior. Además, será posible identificar de forma más sencilla las expresiones más recurrentes o típicas de cada uno. En el caso de José María Aznar, por ejemplo, la libertad se asocia de forma muy evidente al progreso económico.\n\n3.2.3 Prevalencia en el tiempo\nGoogle Libros, consiste en uno de los servicios más interesantes de Google. Se trata de un proyecto ambicioso que ha llevado a cabo la digitalización y el pre-procesamiento lingüístico masivo de un número enorme de libros en diferentes lenguas. Además de disponibilizar muchos documentos en línea (en su gran mayoría de dominio público), la aplicación realiza el cálculo de una serie de métricas entre las cuales se encuentra la densidad de palabras o expresiones (los n-gramas como ya hemos visto en el capítulo anterior). De ahí surgió el Google N-Gram Viewer, una aplicación que permite mapear la evolución de un término o palabra en una lengua en largos períodos de tiempo.\nEl paquete ngramr en R permite acceder a los datos de Google N-Gram Viewer y obtener las frecuencias relativas. La función ngrami, por ejemplo, busca una o más palabras sin considerar si está en mayúsculas o minúsculas y retorna un data.frame con los resultados. Estos datos se pueden emplear luego para generar un gráfico.\nLa gran ventaja de N-Gram Viewer está en su carácter histórico. Revela cómo una palabra o expresión evoluciona en una misma lengua a lo largo de amplios períodos de tiempo. El código abajo realiza la búsqueda de tres infraestructuras clave: los telégrafos, los ferrocarriles y el Internet. A partir del examen de su prevalencia en los libros en español de 1800 a 2019, podemos observar sus distintos ciclos temporales:\n\nCódigo# Carga el paquete ngramr\nlibrary(ngramr)\n\n# Busca los términos (case-insensitive)\nnm &lt;- ngrami(\n          c(\"telégrafo\",\n            \"ferrocarril\",\n            \"internet\"), \n          corpus = \"es-2019\")\n\n# Genera un gráfico con los resultados\nlibrary(ggplot2)\n\np &lt;- ggplot(nm, \n            aes(\n              x=Year, \n              y=Frequency, \n              color=Phrase))+\n  geom_line()+\n  theme_classic()+\n  scale_color_discrete(name=\"Término\")+\n  labs(\n    title=\"Google N-Gramas (1800-2019)\", \n    subtitle = \"Densidad de palabras en libros en español de 1800 a 2019.\")+\n  xlab(\"Año\")+\n  ylab(\"Frecuencia relativa\")\n\np\n\n\n\n\nLas curvas no pueden ser más ilustrativas. Los telégrafos presentan una ascensión entre 1850 y 1900 para, luego, entrar en declive. Los ferrocarriles se han comportado de forma similar, aunque aparezcan de forma mucho más frecuente en los libros del período. Internet, una infraestructura mucho más reciente, se ve reflejada a partir de una curva exponencial a partir de los 2000.\nNo resulta nada difícil replicar la misma lógica en textos que posean alguna secuencia temporal o lógica. Para reproducir el análisis realizado por el NGram Viewer con un corpus no previamente preparado necesitamos llevar a cabo dos tareas centrales. La primera consiste en crear una función que busque algunos términos o expresiones en el corpus y, luego, calcule su densidad. En el segundo paso se organiza la base de datos de modo que cada unidad textual refleje una unidad de tiempo. Por ejemplo, podemos aglutinar los documentos por día, mes, año o cualquier medida que convenga al investigador. Lo importante es que las unidades sean homogéneas y comparables entre sí.\nUna de las bases de datos de ejemplo incluidas en el paquete tenet se conforma por todas las intervenciones parlamentarias durante la XIV Legislatura del Congreso de Diputados de España, vigente entre diciembre de 2019 y junio de 2023. La base original contiene la intervención de cada diputado y la fecha en la que se ha realizado. Por esa razón, resulta relativamente sencillo agregar los textos por fecha. No se recomienda el uso de días, pues los intervalos entre sesiones no resulta uniforme. Algunas semanas contienen dos o más sesiones, mientras que otras apenas se reúnen. Por esa razón, emplearemos en nuestro ejemplo el mes como unidad de comparación de tiempo.\nEl panel abajo contiene la descripción detallada de cada paso:\n\n\nPaso 1: Función\nPaso 2: Tabla\nResultado: Gráfico\n\n\n\nEmpezaremos por crear una función llamada countNgram con cuatro parámetros o argumentos. El primero, keywords, informa cuáles son los términos que deseamos buscar. El segundo, corpus, se refiere al conjunto de textos que serán utilizados como base para el recuento. En tercer lugar, time, suministra la referencia secuencial de tiempo (u otra) para cada uno de los textos contenidos en corpus (en nuestro caso los meses). Por esa razón, el número de elementos de esos dos últimos argumentos debe ser siempre igual. Finalmente, rel.freq establece si se calculará la frecuencia absoluta o la relativa. Esta última se calcula por defecto a menos que se defina rel.freq=FALSE.\n\nCódigo# Función countNgram, que cuenta el \n# número de veces un conjunto de palabras\n# aparecen en un corpus.\ncountNgram &lt;- function(keywords, \n                       corpus=NULL, \n                       time=NULL, \n                       rel.freq=TRUE){\n  \n  # Cuenta todos los términos de la lista\n  tt &lt;- outer(corpus, keywords, stringi::stri_count_regex)\n  \n  # Nombre cada columna de la matriz de resultados \n  # con los términos\n  colnames(tt) &lt;- keywords\n  \n  # Transforma los resultados en data.frame\n  tt &lt;- data.frame(tt)\n  \n  # Si se desea la frecuencia relativa\n  if(rel.freq==TRUE){\n    \n    # Cuenta todas las palabras de todas\n    # las intervenciones de cada mes\n    count &lt;- stringi::stri_count_words(corpus)\n\n    # Calcula la frecuencia relativa\n    for(i in 1:ncol(tt)) tt[,i] &lt;- tt[,i]/count\n    \n  }\n\n  # Añade la identificación del tiempo\n  # (en nuestro caso el mes)\n  tt$Time &lt;- time\n  \n  # Cambia el formato de los datos\n  # (importante para generar el gráfico\n  # en un formato ggplot2)\n  tt &lt;- reshape2::melt(tt, id.vars = \"Time\")\n  \n  # Asigna los nombres de las columnas\n  names(tt) &lt;- c(\"Time\",\"Keyword\",\"Density\")\n\n  # Devuelve el resultado final\n  return(tt)\n      \n}\n\n\n\n\nEn el paso siguiente, creamos una copia de la base de datos de intervenciones legislativas, identificamos el mes en el cual se ha realizado y aglutinamos todas las intervenciones en una sola unidad de textos según la nueva unidad de tiempo. Tales “intervenciones del mes” serán empleadas en la recién creada countNgram para calcular la densidad de los términos “sexual” e “inmigra” que incluyen todo relativo a los derechos sexuales e inmigración. El código describe cómo hacerlo paso a paso y la tabla exhibe los resultados.\n\nCódigo# Crea una copia de los diarios de sesiones del \n# Congreso de Diputados de España\nss &lt;- spa.sessions\n\n# Genera una variable con el mes de cada\n# intervención de cada diario de sesiones\nss$mes &lt;- as.Date(\n            paste0(\n              substr(\n                ss$session.date, \n                1,\n                7),\n              \"-01\"))\n\n# Junta todas las intervenciones de un \n# mismo mes en un largo texto\nss &lt;- aggregate(list(texto=ss$speech.text), \n                by=list(mes=ss$mes), \n                paste0, \n                collapse=\"\\n\")\n\n# Ejecuta la función que cuenta los términos\n# por unidad de tiempo (en el caso cada mes)\nres &lt;- countNgram(\n            keywords = c(\"sexual\",\"inmigra\"), \n            corpus = ss$texto,\n            time = ss$mes)\n\n# Multiplica la frecuencia relativa por 10 mil \n# para facilitar la lectura de los datos y\n# reduce el número de dígitos decimales a 2.\nres$Density &lt;- round(res$Density*10000,2)\n\n# Enseña los resultados\nreactable(res,\n          resizable = T)\n\n\n\n\n\n\n\n\n\n\n\nFinalmente, el último paso para reproducir el ejemplo anterior con base en datos de Google NGram Viewer consiste en crear un gráfico. El código abajo hace justamente eso:\n\nCódigo# Carga el paquete ggplot2\nlibrary(ggplot2)\n\n# Renombra las variables para\n# que aparezcan mejor en el gráfico\nnames(res) &lt;- c(\"Mes\",\"Keyword\",\"Densidad\")\n\n# Genera un gráfico de línea con\n# la evolución por mes de cada \n# término\np &lt;- ggplot(res, \n            aes(\n              x=Mes, \n              y=Densidad,\n              color=Keyword))+\n  geom_line()+\n  theme_classic()+\n  theme(legend.position=\"bottom\")+\n  scale_color_discrete(name=\"Término\")+\n  labs(\n    title=\"Sesiones legislativas del Congreso de Diputados (2019-2023)\", \n    subtitle = 'Densidad de palabras conteniendo \"inmigra\" y \"sexual\" en los diarios de sesiones del Congreso\\nde Diputados de España entre diciembre de 2019 y junio de 2023.')+\n  xlab(\"Año\")+\n  ylab(\"Frecuencia (a cada 10 mil)\")\n\n# Visualiza los resultados\np\n\n\n\n\n\n\n\n\n\n\nEl examen de los resultados de la tabla y el gráfico revelan dos picos, uno para cada palabra. En el caso de expresiones relacionadas a “sexual” este se encuentra en octubre de 2021, cuando se ha llevado a cabo el debate sobre la propuesta de la Ley Orgánica Garantía de la Libertad Sexual (la ley del “sí es sí”). Para “inmigra”, se trata de agosto de 2020, mes en el cual tuvo lugar una sesión de la diputación permanente en la que se aborda explícitamente el tema de la inmigración irregular ocurrida en ese verano.\n\n3.2.4 Etiquetado automático de textos\nUna tarea central de cualquier análisis cualitativo de texto consiste en la lectura atenta y la selección de pasajes o términos para la codificación temática. Se trata de un proceso iterativo -con muchas idas y venidas, revisiones y cambios- en el cual el investigador marca o subraya trechos o palabras del documento y les asigna un código o categoría analítica. En ese sentido, la lectura orienta el proceso de codificación y clasificación, dando lugar al surgimiento de conceptos más abstractos. Aunque la investigación parta de algunas nociones previas, esta fase exploratoria consiste en la clave para la vinculación de elementos concretos presentes en el texto a temas teóricos más amplios. Existe cierto consenso en la literatura sobre el tema que considera la clasificación temática como el resultado (y no el punto de partida) del análisis cualitativo (Saldana 2015, 13; Krippendorff 2004; Neuendorf 2017; Guest, MacQueen, and Namey 2011; Miles, Huberman, and Saldana 2019; Auerbach and Silverstein 2003). Es el producto de la interacción entre el analista y el texto.\nPor esa razón, se necesitan instrumentos que faciliten tal interacción entre el investigador y el texto, que les permitan partir de algunas semillas y profundizar en las sucesivas capas de análisis hasta encontrar los temas centrales de su investigación. En este apartado introduciremos dos opciones de etiquetado y visualización de textos. Constituyen recursos que permiten lo que algunos autores (Saldana 2015, 13) denominan como pre-codificación, como el sublineado o coloreado de pasajes del texto con el propósito de identificar pasajes útiles para la generación de hipótesis o la creación de conceptos.\nEl etiquetado toma un texto único y examina la presencia de un conjunto de palabras-clave o expresiones definidas por el investigador. Si se busca, por ejemplo, saber cómo los presidentes mencionan las políticas sociales en sus discursos, no sería raro que se empezara buscando referencias a la seguridad social, sanidad, educación, pobreza o desigualdad. Estudiosos del populismo, por otra parte, buscarían referencias a patria, pueblo, nación, o élites.\nLa función tagText del paquete tenet permite realizar dicha tarea. Su utilidad consiste en subrayar un texto concreto según una lista de palabras-clave. Su propósito resulta muy sencillo: ayudar en la creación de códigos. El usuario puede leer el texto y al mismo tiempo, crear un diccionario o una lista de palabras que resultan particularmente expresivas de una idea o concepto que se desea capturar.\nEl código abajo selecciona el discurso de investidura de Adolfo Suárez y subraya todas las palabras que contengan las raíces “politic”, “acci”, “conflict”, “partid”, “defensa”, y la expresión “fuerzas armadas”. A cada palabra asocia un color para que sea más fácil su identificación en el texto. De ese modo, se puede ver dónde aparecen.\n\nCódigo# crea un objeto con el nombre de todas \n# las paletas de colores disponibles en tenet\npal &lt;- listPalettes()\n\n# Examina un conjunto de palabras en el\n# discurso de investidura de Adolfo Suárez\ntagText(as.character(spa.inaugural$text[1]), \n        keywords = c(\"politic\", \n                     \"acci\", \n                     \"conflict\", \n                     \"partid\", \n                     \"defensa\",\n                     \"fuerzas armadas\"), \n        palette = pal$wesanderson$Darjeeling1,\n        font.size = 18, \n        title = \"Adolfo Suarez (1979)\",\n        margin = 100)\n\n\n\nComo se puede observar, a cada raíz, palabra-clave o expresión le corresponde un color distinto. Este atributo favorece no solo la identificación de su posición en el texto, sino que también los distingue y subraya cuando coinciden en un mismo párrafo o sentencia. Sitúa, contextualiza, compara y, sobre todo, mantiene el lector anclado al texto.\nUna queja o desconfianza de ciertos investigadores cualitativos frente a métodos asistidos por ordenador tiene que ver con el distanciamiento que éstos últimos provocan con relación a las fuentes textuales. No obstante, el proceso no tiene que implicar una tal dicotomía si se emplean herramientas adecuadas para el examen de patrones textuales.\nLa función tagText también se puede utilizar en conjunción con diccionarios. Resulta útil para averiguar la consistencia de la codificación desarrollada, así como permite perfeccionar los códigos existentes. La estructura de un diccionario resulta sencilla. A cada código corresponde un conjunto de términos, raíces o expresiones. Al emplear un diccionario en tagText todos los elementos de un código se representan bajo un mismo color. De ese modo, resulta más fácil no solo ubicar las categorías, sino también ver hasta qué punto aparecen en un mismo pasaje del documento. Más adelante, cuando tratemos del proceso de codificación por medio de diccionarios emplearemos algunos ejemplos.\n\n3.2.5 Dispersión léxica\nEn ciertas ocasiones interesa saber no solo cuántas veces una palabra aparece en un texto o corpus, sino dónde. El lugar en que se manifiesta una idea puede ser muy significativo para determinar su importancia en un discurso. ¿Está por todo el texto o aparece solamente en algunas partes? ¿Qué documentos poseen mayor o menor concentración?\nEl gráfico de dispersión léxica (lexical dispersion plot) representa una excelente visualización para determinar la localización de un conjunto de palabras-clave en distintos pasajes o sentencias de documentos de componen un corpus y, por lo tanto, su grado de dispersión (o concentración). La función plotLexDiv del paquete tenet permite crear el gráfico a partir de un conjunto de palabras clave (o incluso de un diccionario). El paquete quanteda.textplots también dispone de una función semejante, pero limitada a una sola palabra de cada vez y que no permite la agregación de términos bajo una misma categoría de un diccionario.\nEl código abajo explora dos maneras en las que los presidentes de gobierno españoles se refieren a los ciudadanos del país. La primera, con énfasis más patriótico, se centra en las expresiones “españoles” y “españolas”. La segunda, con un carácter más republicano, enfoca la membresía a la comunidad política bajo los términos “ciudadanos” y “ciudadanas”. ¿Existen diferencias en el uso de esos dos modos de representar a los miembros de la sociedad española? ¿Su presencia resulta concentrada o dispersa, es decir, se trata de algo muy concreto en una parte de los discurso o aparece en todo el texto como una expresión recurrente y articuladora de las demás partes?\n\nCódigo# Palabras \np1 &lt;- plotLexDiv(cp,\n           title = \"Españoles\", \n           keywords = c(\"espanoles\",\"espanolas\"), \n           custom.colors = c(\"red3\"),\n            )\n\np2 &lt;- plotLexDiv(cp,\n           title = \"Ciudadanos\", \n           keywords = c(\"ciudadanos\",\"ciudadanas\"), \n           custom.colors = c(\"blue\"),\n            )\n\nlibrary(grid)\nlibrary(gridExtra)\n\ngrid.arrange(p1,p2, nrow=1)\n\n\n\n\nUna breve inspección visual revela que hay diferencias entre los presidentes en el uso de una u otra forma. Mariano Rajoy aparece como el más enfático usuario de “españoles”, mientras que Felipe González se decanta por “ciudadanos”. Otros se refieren a ambos términos, lo que exige un escrutinio más detenido para entender el contexto del uso de cada. En algunos documentos las expresiones aparecen a lo largo de toda la extensión, mientras que en otros, como españoles en Zapatero II o ciudadanos en Calvo Sotelo, solo en pasajes concretos.\n\n3.2.6 Filtrado de textos: la ratio Documento/Corpus\nCuando se trabaja con muchos textos, uno de los mayores retos consiste en separar el material útil del montón de información que no se desea emplear, al menos en un principio. Por eso, hacen falta estrategias de selección o filtrado de documentos que permitan arrojar luz sobre aquellos documentos que contienen material sustantivo para la investigación.\nEn muchos casos, basta con buscar aquellos documentos que contienen una expresión o palabra. No obstante, ¿qué pasa cuando esta palabra aparece en varios documentos, pero ni todos ellos son relevantes? En estos casos, hace falta algún criterio que permita discriminar de forma ponderada su peso relativo en un corpus.\nLa ratio Documento - Corpus de una palabra constituye una solución posible para este problema. Se trata de una medida que evalúa cuán marcada resulta la aparición de un término concreto en cada documento frente a su frecuencia relativa media en todo el corpus. Una cifra superior a uno en un documento indica una prevalencia más alta que la media y, a la inversa, un valor inferior a uno señala un aparecimiento menos frecuente.\n\\[fd/fc = \\frac{Fd_{i}}{F_i}\\]\nDonde:\nFdi corresponde a la frecuencia relativa del término i en del documento d.\nFi corresponde a la frecuencia relativa del término i en todo el corpus.\nPuesto que los textos que componen un mismo corpus pueden presentar distinto tamaño, se utiliza la frecuencia relativa para evitar distorsiones y garantizar la comparabilidad de los resultados.\nHemos creado la función tfRatio en el paquete tenet con el objetivo de calcular la ratio de una palabra-clave, raíz o expresión en todos los documentos de un corpus. Genera una lista con la ratio del término para cada documento o una lista de los documentos que superan un cierto umbral (por medio de los parámetros threshold y return.selected).\nEl código abajo calcula la ratio de la raíz “machis” (machismo, machista) para todos los discursos de investidura de los presidentes de gobierno españoles:\n\nCódigotfRatio(cp, \"machis\")\n\n [1] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 2.62 0.00 0.00 0.00 4.68 3.27\n\n\nComo podemos observar, solamente tres de los 15 textos contienen alguna referencia al machismo. Estos son los dos discursos de Pedro Sánchez y el primero de José Luis Rodríguez Zapatero.\nSi queremos seleccionar los textos, el parámetro threshold permite establecer un mínimo para que un documento se considere relevante y el argumento return.selected retorna el índice del documento en lugar el la ratio. El código abajo utiliza las ratios para seleccionar los textos en los que palabras como machismo o machista aparezcan más del doble de veces que en la media de todo el corpus (threshold=2):\n\nCódigo# Convierte los textos en un data.frame\ntx &lt;- convert(cp, \"data.frame\")\n\n# Selecciona con relacion a la ratio\nsx &lt;- tx[tfRatio(cp, \n                 \"machis\", \n                 threshold = 2, \n                 return.selected = T),]\n\n# Visualiza los resultados\nreactable(sx, \n          resizable = T,  \n          wrap = F)\n\n\n\n\n\n\n\nAhora el investigador puede utilizar otras herramientas como el tagText, los árboles de palabra, la dispersión léxica o el kwic para analizar los textos y entender cómo cada uno de ellos trata el tema del machismo. Al reducir de forma clara el tiempo de selección de los textos relevantes, esta medida posibilita concentrar esfuerzos en otras tareas de análisis."
  },
  {
    "objectID": "02_explora_deductivo.html#codificación-temática-con-diccionarios",
    "href": "02_explora_deductivo.html#codificación-temática-con-diccionarios",
    "title": "\n3  Juego de palabras:Exploración deductiva y abductiva de textos\n",
    "section": "\n3.3 Codificación temática con diccionarios",
    "text": "3.3 Codificación temática con diccionarios\nLa codificación temática consiste en una de las estrategias más comunes en las ciencias sociales para tratar datos en forma de texto. De forma muy resumida, representa un proceso fundamentado en la lectura profundizada de una cantidad limitada de textos y acompañada de anotaciones que permitan extraer temas o conceptos que ayuden en el desarrollo o teste de hipótesis.\nDe acuerdo con Krippendorff (2004, 18) el análisis de contenido constituye una técnica que permite la realización de inferencias a partir de textos en el contexto en que se emplean. Neuendorf (2017, 1), por otra parte, define el método como el “análisis sistemático, objetivo y cuantitativo de las características de un mensaje”. Otros autores (Saldana 2015; Miles, Huberman, and Saldana 2019) extienden el proceso de codificación más allá del análisis de contenido y mencionan un número más amplio de métodos que se basan en el examen detenido de información textual, como el análisis de discurso, la teoría fundamentada (grounded theory) o el análisis narrativo.\nLa codificación de temas y la extracción de insights teóricos es una constante en la literatura sobre los métodos cualitativos en las ciencias sociales. En algunos casos, se considera un proceso casi artesanal basado en la lectura profunda y un conocimiento detallado de los textos y sus contextos. En otros, la existencia de volúmenes masivos de documentos invita a la combinación entre: (a) el escrutinio atento de una muestra de los documentos y (b) métodos cuantitativos que permitan sintetizar y clasificar un masa de material imposible de examinar de forma tradicional.\n\n3.3.1 Expresiones, códigos, temas\nResulta útil iniciar este apartado con una breve distinción entre expresiones (términos, palabras o n-gramas), códigos (etiquetas que sintetizan el contenido) y temas (constructos teóricos derivados del análisis). De forma resumida, se puede argumentar que las primeras indexan los textos (conectan palabras a ideas) y ayudan a seleccionar atributos o pasajes que pueden ser particularmente significativas o teóricamente relevantes. Los siguientes (códigos) posibilitan aunar los términos en grupos o colecciones y llevar a cabo un esfuerzo de agregación o síntesis conceptual previo al desarrollo de conceptos más abstractos. Finalmente, un tema emerge del análisis de los distintos códigos, sus contextos y relaciones recíprocas.\nImaginemos que deseamos saber la posición de distintos partidos sobre las políticas sociales. Una forma de encontrar informaciones consiste en buscar palabras como pobreza, desigualdad, ayudas, sanidad, educación, seguridad social, asistencia social, transferencias, entre otros términos. En su conjunto, tales expresiones permiten ubicar en los diferentes documentos de un corpus cuánto y dónde se ha mencionado alguna de esas medidas o asuntos relacionados. Por lo tanto, actúan como instrumentos de búsqueda e indexación de categorías abarcadoras, que pueden estar reunidas en códigos que ayuden a identificar cada aspecto mencionado con la noción general de política social. Sin este paso, resulta muy complicado filtrar aquellos pasajes más destacados sobre ese tema en corpus con muchos documentos. Tampoco se podría calcular su peso relativo: ¿cuál política recibe un número mayor de menciones: la sanidad o la educación?\nLos códigos son, a su vez, “etiquetas que atribuyen significado simbólico a información descriptiva o inferencial compilada durante un estudio” (Miles, Huberman, and Saldana 2019, 78–79). Tales “etiquetas” pueden ser palabras o expresiones que permitan atribuir un sentido más amplio, sea descriptivo o conceptual, a la palabra o trecho al que se le asocia. También pueden considerarse como dispositivos de segmentación y aglomeración de significados. Al asociar un conjunto diverso de palabras a los términos izquierda y derecha, por ejemplo, establecemos qué partes se aúnan dentro de esas categorías y, al mismo tiempo, las separamos del resto del texto (diagrama 1).\n\n\n\nDiagrama 1. Ejemplos de términos y expresiones, así como de códigos de primer y segundo nivel en un diccionario con las categorías izquierda y derecha.\n\n\nAdemás, los códigos facilitan la búsqueda, clasificación y ordenamiento de la información según los intereses del investigador. De cierto modo, sintetizan la diversidad de términos y expresiones en un número reducido de categorías o marcadores. Corresponden a una heurística, un método de descubierta (Miles, Huberman, and Saldana 2019, 80). Al reestructurar los textos de acuerdo a categorías analíticamente significativas, permite identificar patrones, establecer la relación entre códigos y desarrollar interpretaciones analíticas de más alto nivel. En cierto sentido, constituyen una herramienta para estructurar el texto y permitir su recomposición bajo lógicas analíticas distintas.\nFinalmente, los temas constituyen construcciones teóricas derivadas del análisis de los códigos. Pueden surgir del examen de un, diez o cien mil documentos. Todo va a depender del diseño de la investigación y cómo cada investigador construye sus categorías analíticas, abstrayendo temas más generales a partir de códigos encontrados en los textos. El tipo y el grado de abstracción dependerá de los objetivos definidos para cada estudio. No obstante, el método suele ser siempre el mismo: la definición de códigos, su análisis y la posterior síntesis en un conjunto de temas clave y teóricamente significativos.\nAsociar códigos a los textos, por lo tanto, permite extraer información valiosa para la investigación. Como hemos señalado, un código puede estar asociado tanto a una palabra en concreto como a pasajes enteros. Por esa razón, cabe subrayar que el método propuesto en este trabajo resulta un poco distinto de la codificación tradicional y representa una alternativa híbrida entre métodos puramente cuantitativos y la construcción de un sistema cualitativo tradicional. Aquí, la búsqueda de raíces, palabras o expresiones se utiliza para identificar e indexar dónde una determinada idea aparece en el documento.\nTales elementos empíricos fundamentales pueden estar aislados y servir para un examen preliminar del corpus o agregados según categorías analíticas. Los códigos también pueden anidarse en una estructura jerárquica, como ramas e un árbol. Como vimos en el diagrama 1 arriba, los términos y expresiones “derecho aborto”, “cambio climático” y “transex” conforman la categoría “progresismo”. Ya “educ”, “pobreza” y “social” se aúnan en “igualdad social”. Finalmente, los dos códigos (progresismo e igualdad social) componen la categoría más abstracta “izquierda”.\nEn resumen, un código agrupa términos básicos en categorías analíticas más o menos homogéneas internamente y distintas entre sí. De ese modo, un código se constituye a partir de: (a) una recolección de esas unidades textuales básicas (más que de frases o trechos enteros) o (b) de otros códigos con menores niveles de abstracción.\nTal estrategia puede considerarse a primera vista como limitante. No obstante, presenta dos grandes puntos fuertes. En primer lugar, palabras o expresiones cortas sirven como elementos útiles para la selección de pasajes más amplios de forma automática y el filtrado de contenido relevante. Al emplearse en conjunto con las herramientas de visualización disponibles, como el kwic y el etiquetado de textos resulta más facil situar las ideas en su contexto sin tener que examinar de antemano largos pasajes de textos.\nSegunda ventaja: el uso de palabras, bigramas o trigramas es indudablemente más fácil de cuantificar y comparar en grandes corpus de texto. Resulta mucho más sencillo (y rápido) comparar “machis” en 10 mil documentos que frases concretas que solo indirectamente mencionan el tema del machismo. Para estos últimos casos, estrategias complementarias, como la inclusión de metáforas u otras alusiones indirectas relevantes puede ayudar en su identificación. Cuando el número de documentos es inabarcable desde una metodología tradicional, una alternativa útil consiste en combinar la lectura profundizada de una muestra cuidadosamente seleccionada de textos con algoritmos que permitan rastrear la presencia de códigos en todo el material disponible.\nEste método implica un cambio en la forma de construcción de los códigos. Aún más cuando empleamos diccionarios (o léxicos) temáticos como instrumento. La inclusión de una palabra o n-grama en los diccionarios hace con que nuevos pasajes salgan a la luz. La clave se encuentra en analizar las frases o párrafos que contienen tales expresiones y, a partir de ahí, refinar el mismo léxico, desarrollar códigos más abstractos y generar temas teóricamente relevantes para el estudio. Como se ha señalado más arriba, no se trata de abandonar un enfoque cualitativo, sino adaptarlo a situaciones en las que la lectura exhaustiva y en profundidad del corpus resulta imposible o poco práctica.\n\n3.3.2 Diccionarios como colecciones de códigos\nComo se ha mencionado más arriba, los diccionarios pueden considerarse como dispositivos de ensamblaje de expresiones de interés teórico o como instrumentos de organización de ideas. Permiten la aglutinación de términos en categorías analíticas más amplias y su posterior organización en estructuras conceptuales jerárquicas.\nEl diagrama 1 más arriba nos muestra como ciertas raíces, palabras o expresiones se agrupan en categorías como progresismo, conservadurismo, liberalismo económico o igualdad social y éstas en códigos más abstractos como izquierda o derecha. Este breve diccionario nos ayuda a asociar términos que se pueden encontrar en los textos a conceptos o ideas más abstractas, sin una manifestación empírica directa. De ese modo, la izquierda se compone por dos dimensiones -igualdad social y progresismo-, mientras que la derecha se caracteriza como la combinación entre liberalismo económico y conservadurismo social.\nAunque esté muy lejos de brindar una definición exhaustiva de los conceptos de derecha e izquierda, este pequeño ejemplo ilustra cómo un diccionario permite asociar elementos empíricos concretos encontrados en los textos a dimensiones teórico-conceptuales más abstractas.\nPor esa misma razón, se debe subrayar la naturaleza iterativa del proceso. Las categorías más abarcadoras (como progresismo, por ejemplo) se suelen construir a partir de un proceso reiterado de examen de los textos, la inclusión de nuevos términos y la revisión de los resultados. Incluso en los casos en que se dispone de un diccionario previo hecho por otros, resulta fundamental la adaptación a los propósitos de investigación para alcanzar los mejores resultados.\nLa creación de un diccionario representa una técnica de medición y búsqueda (Neuendorf 2017, 126–27) en la el uso de palabras y otros elementos textuales permiten identificar la presencia de ciertas ideas o conceptos en un corpus determinado. También se le podría considerar como un libro de códigos, una compilación de categorías y los elementos que le componen. La formalización explícita de los grupos y su documentación facilita el trabajo en grupo y aumenta la transparencia y reproducibilidad de al menos parte del análisis realizado (Saldana 2015, 21).\nEl ejemplo abajo crea un diccionario que clasifica 39 términos según los códigos “economía”, “fiscal”, “educación”, “sanidad” y “medioambiente”. Luego, lo emplea en conjunción con la función tagText para resaltar las categorías en el discurso de investidura de Adolfo Suárez. Al analizar los resultados, vemos que a cada categoría (o código) corresponde un color cuyo nombre se revela al mover el cursos sobre una palabra subrayada.\n\nCódigo# Crea un diccionario de algunos términos políticos\ndic &lt;- dictionary(\n    list(\n    economica=c(\"econom\",\n               \"inversion\",\n               \"empresa\",\n               \"desarroll\",\n               \"monetari\",\n               \"industri\",\n               \"agric\",\n               \"agrari\"),\n    fiscal=c(\"hacienda\",\n               \"gasto\",\n               \"impuest\",\n               \"presupuest\",\n               \"tribut\",\n               \"tasa\",\n               \"fiscal\"),\n    educacion=c(\"educa\",\n             \"profesor\",\n             \"docent\",\n             \"escuel\",\n             \"colegio\",\n             \"universi\",\n             \"formación\"),\n    sanidad=c(\"sanidad\",\n               \"salud\",\n               \"hospital\",\n               \"sanitari\",\n               \"médic\",\n               \"enfermer\",\n               \"salud\"),\n    medioambiente=c(\"sostenible\",\n                 \"cambio clima\",\n                 \"medioambient\",\n                 \"reciclaje\",\n                 \"ecológico\",\n                 \"límpia\",\n                 \"invernadero\",\n                 \"emisiones\",\n                 \"carbono\",\n                 \"plástico\",\n                 \"fósiles\")))\n\n# genera un texto para ser leído en el panel\n# Viewer de RStudio\ntagText(spa.inaugural$text[1], \n        keywords = dic, \n        palette = pal$MetBrewer$Austria,\n        font.size = 18, \n        title = \"Adolfo Suarez (1979)\",\n        margin = 100)\n\n\n\nEl resultado nos sitúa en un espacio entre una búsqueda automatizada de términos y la codificación manual. Al aplicar el diccionario a un texto específico, se puede observar no solo dónde las categorías aparecen más o menos, sino también cómo estas se asocian entre ellas en un mismo párrafo, por ejemplo. Algunos pasajes son monotemáticos, inciden sobre una idea clave. Otros interesan por la asociación entre conceptos distintos. En muchas ocasiones es justamente la asociación entre temas lo que permite el surgimiento de nuevas hipótesis. Además, el análisis de la incidencia de los códigos en el texto invita a la revisión del diccionario para incorporar nuevos términos o categorías y, de ese modo, completar el análisis.\nMiles et al. (2019, 86) sugieren que un método para utilizar diccionarios como herramientas para la codificación consiste en crear una lista provisional de códigos por medio de un proceso deductivo a partir de las referencias teóricas que sirven de marco para el estudio. Una vez elaborada, puede servir de semilla para el examen de los textos y pasar por procesos sucesivos de adaptación, refinamiento y elaboración con el empleo de una codificación inductiva complementaria.\nEste proceso de revisión constante requiere instrumentos que permitan explorar, ordenar, filtrar y sintetizar la información. La función tagCorpus de tenet emplea una tabla interactiva que permite a los usuarios llevar a cabo una serie de tareas de exploración de los términos y códigos de un diccionario en todo un corpus. Por lo tanto, se centra en posibilitar la identificación tanto de aspectos compartidos como de señas distintivas entre documentos, categorías o actores. Además, permite examinar de forma sumaria la coocurrencia de códigos en frases o párrafos.\nEl paquete tenet también incluye un diccionario de ejemplo llamado dic.pol.es. Se trata de un conjunto de códigos que analizan diferentes dimensiones de los discursos políticos. Contiene tres niveles: (1) palabras o expresiones, (2) códigos de primer nivel y (3) códigos de segundo nivel. Por ejemplo, “izquierda unida” pertenece al código nivel-1 “partidos” y al código nivel-2 “actores”. Por su parte, “ilustres” pertence a “retórica” (nivel-1) y a “discurso” (nivel-2).\nEl código abajo utiliza la función tagCorpus y el diccionario dic.pol.es para identificar la incidencia de las categorías en cada sentencia del corpus de los discursos de investidura españoles. Como se podrá ver, abajo, el resultado es una tabla con siete columnas. La primera Order corresponde al orden de la frase en el documento X (columna Doc.). De ese modo, Order igual a 1 y Doc. igual a Suárez corresponde a la primera frase del discurso de investidura de Adolfo Suárez. Paragraph corresponde a la unidad textural, que puede ser el documento completo (documents), párrafos (paragraphs) o oraciones (sentences). Las categorías más frecuentes aparecen en la columna siguiente (Main Category) y todas las categorías encontradas en la columna Paragraph aparecen en All Categories, incluídas, por supuestos, las más frecuentes. Matches informa el número de veces una palabra o término del diccionario se ha encontrado en el texto. Finalmente, Cat. No. informa el número total de categorías encontradas.\nAdicionalmente, debajo del nombre de cada columna, se pueden encontrar campos de filtro. Basta digitar cualquier valor o texto para seleccionar los resultados. Por ejemplo, si uno desea saber cómo Calvo Sotelo trataba temas sociales, puede seleccionar solo los documentos que se inicien por “Calvo” y que, en All Categories, incluya el código “social”. Al hacer clic sobre el nombre de cada columna también se pueden ordenar los valores de forma ascendente o descendente.\n\nCódigo# tagCorpus, hace algo parecido para un corpus\ntagCorpus(cp,\n          defaultPageSize = 4,\n          dic.pol.es, \n          palette = pal$lisa$PaulGauguin, \n          reshape.to = \"sentences\", \n          show.details = T)\n\n\n\n\n\n\n\nConsideremos otro ejemplo. Si queremos identificar cuáles actores sociales y políticos mencionados en los discursos de investidura de los presidentes españoles que estén vinculados al tema tecnológico, podemos filtrar las sentencias del corpus en las que el tema principal (Main Category) son los “actores” y en que también aparezcan (All Categories) “tecnologia”.\nVemos que la concepción tecnológica de los presidentes pasa por una actuación clave de empresas y del mercado. Poco se menciona sobre el rol de la inversión en ciencia. Mucho inversor y poco investigador. La innovación, por lo tanto, se da por la atracción de capital y de tecnologías desarrolladas por otros más que por un proceso autónomo de construcción tecnológica a partir de la inversión en ciencia. No resulta para nada casual que casi la mitad del presupesto asignado a investigación suela estar constituida por créditos destinados al I+D+I de empresas (en colaboración con la universidad)."
  },
  {
    "objectID": "02_explora_deductivo.html#análisis-temático",
    "href": "02_explora_deductivo.html#análisis-temático",
    "title": "\n3  Juego de palabras:Exploración deductiva y abductiva de textos\n",
    "section": "\n3.4 Análisis temático",
    "text": "3.4 Análisis temático\n¿Cuáles códigos tienen más peso? ¿Qué categorías se asocian de forma más estrecha? Una vez creados los códigos y los diccionarios, cabe dar un paso adelante y buscar patrones, identificar las características de los conjuntos de términos y sus relaciones con otros atributos en los textos.\nEl análisis de la incidencia de los códigos en un corpus y su interrelación permiten explicitar patrones y definir el peso relativo de cada idea en los textos. Este apartado emplea tres estrategias para explorar la importancia de los temas. La primera consiste en averiguar el peso de las categorías, es decir, emplear estadísticas sumarias, como la frecuencia relativa de códigos o expresiones, para establecer su prevalencia. La segunda se basa en la desagregación y el filtro para comparar grupos o atributos o para seleccionar aspectos concretos que se desean examinar con más detenimiento. La tercera investiga su asociación por medio de las redes de coocurrencia.\nCombinadas, tales estrategias permiten identificar los temas centrales presentes en un texto o corpus y cómo se relacionan entre sí. También revelan su variación de acuerdo con variables contextuales, como puede ser un partido, el presidente o un período de tiempo determinado. Se tratan de herramientas sencillas, pero muy útiles, a la hora de contextualizar ideas e identificar variaciones importantes en el uso de conceptos o términos.\n\n3.4.1 Estadísticas temáticas\nDenominamos estadísticas temáticas el conjunto de técnicas cuantitativas que permiten representar la importancia de categorías o expresiones en un corpus. ¿Cuántas veces los presidentes de gobierno han mencionado la ciencia en sus discursos de investidura? ¿Cuántas han mencionado al terrorismo? ¿Quiénes han sido los que más uso han hecho de la expresión “género” o “fuerzas de seguridad”?\nPor lo tanto, aunque sencillas, tales herramientas permiten delinear diferencias programáticas e ideológicas entre distintos actores políticos. Sobre todo, señala aquellas categorías más frecuentes, tanto por el número de veces que aparecen como por la cantidad de documentos en los que aparecen. Por ejemplo, solo algunos de los presidentes mencionan el tema de género en sus discursos (en especial José Luis Rodríguez Zapatero y Pedro Sánchez). No obstante, temas como el mercado laboral o la fiscalidad del Estado, como esperado, aparecen en todos ellos (aunque acompañados de distintos calificativos).\nEl panel abajo contiene un conjunto de recursos para el análisis de los códigos del diccionario dic.pol.es aplicado a los discursos de investidura de los presidentes de gobierno de España. La primera pestaña (tabla) contiene la frecuencia relativa de los códigos y términos del diccionario en el corpus. Las demás corresponden a visualizaciones que permiten hacer una síntesis de los pesos relativos de palabras-clave y categorías en los textos. Force Directed Tree genera un diagrama de árbol que representa la jerarquía de los términos como una red. Las dos alternativas siguientes (voronoi tree) generan una imagen parecida, pero con otros instrumentos de interacción y niveles de zoom. Esto permite mirar hacia los resultados de una forma ligeramente distinta.\n\n\nTabla\nForce-Directed Tree\nVoronoi Tree (Códigos)\nVoronoi Tree (keywords)\n\n\n\nLa función countKeywords de tenet produce un data.frame con un conjunto de campos que auxilian en el análisis del peso relativo de cada categoría en un corpus determinado. El primero es groups, que indica el grupo (como partido o presidente, por ejemplo) que detalla los resultados. Si no se ha informado ninguna variable de grupo, aparecerá “All” (todos). El segundo, level1, señala el código de más alto nivel en un diccionario (la función admite hasta dos niveles de jerarquía, en esos casos aparecería también level2). El tercero, keyword, indica la palabra-clave que conforma el diccionario y frequency muestra la frecuencia (absoluta o relativa del término en el corpus).\nLa tabla abajo muestra la frecuencia relativa (por cada mil palabras) de cada término del diccionario dic.pol.es en el corpus de discursos de investidura de los presidentes españoles.\n\nCódigo# Calcula la frecuencia relativa en que cada\n# palabra ha sido encontrada para cada nivel\n# del diccionario\nxy &lt;- countKeywords(cp, \n                    dic.pol.es, \n                    rel.freq = T, \n                    quietly = TRUE)\n\n# Elimina los términos no encontrados\nxy &lt;- xy[xy$frequency&gt;0,]\n\n# Puesto que es una frecuencia relativa\n# multiplicamos por 10 mil para tener la\n# ratio de ocurrencia a cada 10 mil palabras\nxy$frequency &lt;- round(xy$frequency*10000, 1)\n\n# Visualiza los resultados\nreactable::reactable(xy, \n                     resizable = T)\n\n\n\n\n\n\n\n\n\nSi ordenamos por frecuencia, vemos que la categoría más general de discurso y, dentro de esta, España, se destaca. A ella le sigue la figura retórica de “Señor”, que incluye todas las formas derivadas: señor, señora, señoría, señores, y demás. Se trata también de una forma de tratamiento común en este tipo de texto en los que los candidatos a presidente de gobierno se refieren de forma respetuosa a los demás representantes parlamentarios.\n\n\nAunque la tabla nos brinde los detalles de la frecuencia de cada término, una visualización de todos los códigos a la vez posibilita entender su peso relativo de forma instantánea y comparada. Un diagrama de árbol (force directed tree) representa cada categoría en el diccionario como un árbol en el que cada código es una rama y cada elemento una hoja que se atraen o repulsan de acuerdo con su peso relativo (Holten 2006). El tamaño de cada círculo (rama o hoja) se define de acuerdo con su frecuencia y el color según el nivel más alto en el diccionario. A mayor peso, más centralidad en el gráfico. De acuerdo con el ejemplo que se emplea aquí, discurso tendrá un color, social otro, exterior el suyo y así sucesivamente. Cada una de esas categorías abarcadoras mantendrá la misma estructura de códigos secundarios y expresiones (o keywords) como figuran en el diccionario.\n\nCódigojs &lt;- jsonTree(data = xy, groups=c(\"level1\",\"level2\"), elements = \"keyword\", value=\"frequency\")\n\nforceDirectedTree(js,attraction = -8,\n                  palette = pal$lisa$JackBush_1, \n                  max.radius = 18, \n                  height = 500)\n\n\n\n\n\n\n\n\nComo se puede ver, la categoría discurso es la que más peso tiene en el corpus. Está conformada por expresiones de tratamiento como “señorías” o “investidura” y relacionadas a España, como “españoles”, “patria” o “pueblo”. Viene seguida de temas sociales, de política exterior y fiscales.\n\n\nUn treemap corresponde a otra forma de visualización de datos jerárquicos. En este caso, todas las categorías y sus subcategorías se dividen en un círculo fragmentado en partes que se dimensionan según la frecuencia de cada código (Balzer and Deussen 2005).\n\nCódigoxx &lt;- aggregate(list(frequency=xy$frequency),\n                by=list(level1=xy$level1,\n                        level2=xy$level2),\n                sum, na.rm=TRUE)\n\nplotVoronoiTree(data = xx,\n                groups = \"level1\",\n                elements = \"level2\",\n                value = \"frequency\")\n\n\n\n\n\nAl hacer clic sobre una categoría, el gráfico hace un zoom y redistribuye el espacio solo con las subcategorías del código principal seleccionado.\n\n\nEl ejemplo abajo repite el gráfico anterior, pero ahora con las palabras-clave como unidades de división de las áreas del círculo. Aquí se pueden observar el peso relativo de cada expresión en la configuración de cada código.\n\nCódigoxx &lt;- aggregate(list(frequency=xy$frequency),\n                by=list(level1=xy$level1,\n                        keyword=xy$keyword),\n                sum, na.rm=TRUE)\n\nplotVoronoiTree(data = xx,\n                groups = \"level1\",\n                elements = \"keyword\",\n                value = \"frequency\")\n\n\n\n\n\n\n\n\n\n\n\nUn análisis rápido de los resultados subraya la importancia de formas retóricas y puramente discursivas en el corpus. El uso de expresiones de tratamiento, como señorías, o relativos a España o los españoles predomina por su reiterada aparición. El tamaño de la categoría “discurso” en todas las visualizaciones manifiesta claramente su predominio sobre los demás temas.\nLa segunda categoría de mayor importancia se encuentra relacionada con temas sociales. No sorprende que las cuestiones laborales, y en particular el empleo, constituyan elementos centrales de los discursos de todos los presidentes. Con relación a las demás áreas de política social, se percibe un destaque muy particular a la educación. Se trata de un tema alrededor del que los distintos partidos siempre han marcado sus diferencias programáticas. Tal protagonismo se ve reflejado en los discursos de investidura.\nEn política exterior, pesa mucho más Europa frente a otros temas y al resto del mundo. Se observa una clara orientación hacia el contexto regional frente a otros vínculos políticos más tradicionales. Este patrón se puede verificar claramente en la falta casi absoluta de protagonismo de América Latina en los textos.\nLa mención a distintos actores sociales también resulta útil para entender la relación de los presidentes con diferentes sectores de la sociedad civil. En el corpus analizado, queda claro el destaque atribuido a las empresas y empresarios, vistos como promotores de crecimiento económico. En segundo lugar, hay muchas referencias al propio partido o a aquellos que forman parte de la coalición de gobierno. Los trabajadores ocupan el último lugar.\nEl tema territorial aparece, principalmente, bajo la forma de acción administrativa del Estado hacia comunidades autónomas y la administración local. No obstante, otros temas vinculados con la dimensión territorial de la organización del Estado español, como el regionalismo y el terrorismo, también presentan cierto destaque.\nLa tecnología es vista como un motor de desarrollo. No obstante, la ciencia ocupa un rol marginal. En varios discursos, se trata de dar incentivos a empresas y atraer tecnologías desarrolladas en otros países más que crear un sistema de investigación robusto que permita la innovación desde España.\nEn relación a las categorías postmaterialistas, se observan dos patrones. El medioambiente conforma el tema con más peso y con un carácter más transversal. De una forma o de otra, todos los presidentes lo consideran un problema a atajar. No obstante, la diversidad sexual constituye un divisor de aguas. Aparecen con una frecuencia significativamente mayor en los discursos de los dos últimos presidentes socialistas y constituyen, de cierto modo, una marca de sus programas de gobierno.\n\n3.4.2 Temas por atributo\n¿Cómo distintos partidos mencionan un tema? ¿Y los presidentes? En muchas ocasiones, el punto central del análisis consiste en comparar cómo los temas varían según un atributo cualquiera como, por ejemplo, la ideología, el tiempo, o distintas regiones o países.\nEn algunos casos, interesa desagregar los datos generales por atributo y examinar cómo los patrones varían según cada valor o grupo. En otros, el objetivo consiste el filtrar o seleccionar algunos valores para explorarlos en profundidad. La capacidad de manipulación de datos representa uno de los puntos fuertes de R. Resulta muy sencillo realizar búsquedas y selecciones de datos a partir de criterios lógicos. Por esa razón, emplear tales capacidades en favor de un análisis de datos más detallado consiste en algo sencillo.\nEl panel abajo desagrega los datos presentados anteriormente por partidos y por presidente, así como filtra los resultados solo para el código “postmaterialismo”. En las pestañas tabla, se presentan las frecuencias desagregadas por ambas variables y, en las pestañas Sankey, se presentan los datos en un diagrama aluvial conocido como diagrama de Sankey (A. B. W. Kennedy and Sankey 1898; Riehmann, Hanfler, and Froehlich 2005).\n\n\nTabla: Partidos\nSankey: Partidos\nTabla: Presid.\nSankey: Presid.\nFiltro\n\n\n\nLa tabla abajo presenta las frecuencias relativas desagregadas por partido de cada código y expresión contenida en el diccionario dic.pol.es. Como en los ejemplos anteriores, se ha empleado el corpus de los discursos de investidura de los presidentes de gobierno de España. La única diferencia con el ejemplo anterior está en el uso del parámetro group.var=“Partido” en la función countKeywords, que establece que los resultados ahora deben ser desagrupados por el partido político del presidente.\n\nCódigolibrary(quanteda)\ndocvars(cp, \"Partido\") &lt;- c(\"UCD\", \"UCD\", \"PSOE\",\n                            \"PSOE\", \"PSOE\", \"PSOE\",\n                            \"PP\", \"PP\", \"PSOE\",\n                            \"PSOE\", \"PP\", \"PP\",\n                            \"PP\", \"PSOE\", \"PSOE\")\n\nxp &lt;- countKeywords(cp, \n                    dic.pol.es, \n                    rel.freq = T, \n                    group.var = \"Partido\",\n                    quietly = TRUE)\n\n\nxx &lt;- aggregate(list(frequency=xp$frequency), \n                 by=list(groups=xp$groups, \n                         level1=xp$level1,\n                         level2=xp$level2), \n                 sum, na.rm=T)\n \nxx &lt;- xx[xx$frequency&gt;0,]\n\nxx$frequency &lt;- round(xx$frequency*1000,2)\n\nreactable(xx, \n          filterable = T,\n          resizable = T)\n\n\n\n\n\n\n\n\n\n\n\nEl diagrama de Sankey abajo representa los resultados de la tabla anterior. Cada barra de la izquierda corresponde a un partido y de la derecha a un código del diccionario. Los vínculos de cada partido a cada categoría se hacen visible cuando se pasa el cursor sobre una de las barras. Si el cursor está sobre un partido, se muestran sus vínculos con todas las categorías. Si, por otra parte, se pone sobre una categoría, se señalan todos los partidos y la intensidad con la que se vinculan.\n\nCódigoxx &lt;- aggregate(list(frequency=xp$frequency), \n                 by=list(groups=xp$groups,\n                         level2=xp$level2), \n                 sum, na.rm=T)\n\nxx &lt;- xx[xx$frequency&gt;0,]\n\nxx$frequency &lt;- round(xx$frequency*1000,2)\n\nplotSankey(xx, from = \"groups\", to=\"level2\", value = \"frequency\", opacity = 0.05)\n\n\n\n\n\n\n\n\nSi pasamos el cursor sobre el código “democracia”, por ejemplo, vemos que la UCD contiene el mayor número de menciones. Se trata de algo absolutamente esperado, puesto que los presidentes de este partido (Adolfo Suárez y Leopoldo Calvo-Sotelo) han sido los primeros a ocupar el cargo durante la transición a la democracia. Si consideramos los términos “España” y “empresas” vemos que el PP, a su vez, contiene un mayor protagonismo, aunque en el último caso, su preponderancia resulta modesta frente a los demás grupos políticos.\n\n\nEn este caso, los datos se desagregan por presidente.\n\nCódigoxz &lt;- countKeywords(cp, \n                    dic.pol.es, \n                    rel.freq = T, \n                    group.var = \"President\",\n                    quietly = TRUE)\n\nxx &lt;- aggregate(list(frequency=xz$frequency), \n                 by=list(groups=xz$groups, \n                         level1=xz$level1,\n                         level2=xz$level2), \n                 sum, na.rm=T)\n \nxx &lt;- xx[xx$frequency&gt;0,]\n\nxx$frequency &lt;- round(xx$frequency*1000,2)\n\nreactable(xx, \n          filterable = T,\n          resizable = T)\n\n\n\n\n\n\n\n\n\n\n\nEl mismo diagrama, pero ahora desagregado por presidente.\n\nCódigoxx &lt;- aggregate(list(frequency=xz$frequency), \n                 by=list(groups=xz$groups,\n                         level2=xz$level2), \n                 sum, na.rm=T)\n \nxx &lt;- xx[xx$frequency&gt;0,]\n\nxx$frequency &lt;- round(xx$frequency*1000,2)\n\nplotSankey(xx, from = \"groups\", to=\"level2\", value = \"frequency\", opacity = 0.05)\n\n\n\n\n\n\n\n\nAquí vemos cómo cada presidente utiliza los términos. Resulta muy llamativo el uso de expresiones relacionadas a “España” por Mariano Rajoy, “genero” por Pedro Sánchez o “fiscal” por Aznar. Tales códigos les destacan frente a los demás y nos permiten identificar las características de sus discursos que les singularizan.\n\n\nFiltrado de valores\nTambién podemos filtrar los valores para centrar la atención a una categoría o código específico. En algunos casos, como género o medioambiente, por ejemplo, resulta difícil ver las diferencias en un gráfico dada su pequeño peso frente a otras categorías más frecuentes. En el ejemplo abajo, seleccionamos solamente los códigos de segundo nivel relacionados al “postmaterialismo”, es decir, cuestiones de género, medioambiente y memoria histórica.\n\nCódigox1 &lt;- xz[xz$level1==\"postmaterialismo\",]\n\nxx &lt;- aggregate(list(frequency=x1$frequency), \n                 by=list(groups=x1$groups,\n                         level2=x1$level2), \n                 sum, na.rm=T)\n \nxx &lt;- xx[xx$frequency&gt;0,]\n\nxx$frequency &lt;- round(xx$frequency*1000,2)\n\nplotSankey(xx, from = \"groups\", to=\"level2\", value = \"frequency\", opacity = 0.05)\n\n\n\n\n\n\n\n\nComo vemos, en los temas postmateriales hay un predominio de presidentes de gobierno del PSOE, en especial Zapatero y Sánchez. No obstante, en algunos temas como la memoria histórica y el medioambiente, presidentes de otros partidos también aparecen con menciones, aunque en menor grado.\n\n\n\n\n3.4.3 Redes temáticas\n¿Qué códigos siempre se mencionan juntos? ¿Qué otros nunca aparecen en una misma frase, párrafo o documento? El análisis de la asociación entre categorías constituye otro recurso muy útil para identificar patrones en los textos y facilitar el análisis del contenido de los mismos. Dicha tarea constituye el núcleo del desarrollo de redes temáticas, construidas a partir de la abstracción de códigos hacia conjuntos interrelacionados de temas (Attride-Stirling 2001). En los discursos políticos términos como democracia o libertad tienden a estar asociados a otras expresiones que les califican y permiten asignar una posición ideológica concreta. Por ese motivo, el análisis de las redes de asociación temática permiten avanzar aún más en la comprensión de los patrones existentes en el contenido de los documentos que componen un corpus.\nEl panel abajo trabaja con dos niveles. El primero examina la relación entre códigos de más alto nivel como actores, instituciones, política exterior o fiscal. El segundo baja un escalón y trata de los códigos menos abstractos como laboral, europa, retorica, genero. Para cada nivel los datos se muestran tanto bajo la forma de una tabla con los términos y el número de veces que aparecen juntos como en un diagrama de cuerdas (chord diagram) que permite la visualización de redes cuyos nodos se encuentran densamente asociados entre sí (Bremer and Wu 2012).\n\n\nTabla (nivel 1)\nCuerdas (nivel 1)\nTabla (nivel 2)\nCuerdas (nivel 2)\n\n\n\nLa tabla abajo ha sido producida a partir de la función matchCodes que examina la coocurrencia de los códigos de un diccionario determinado en un corpus. Aquí se emplean los discursos de investidura organizados según sentencias y se busca mapear la asociación entre los códigos de más alto nivel del diccionario dic.pol.es. El resultado es un data.frame con tres columnas: term1, correspondiente al primer término, term2, representando el segundo código, y value, que contiene el número de veces en que esas dos categorías aparecen en una misma unidad textual del corpus (sentencia, párrafo o documento entero).\n\nCódigocs &lt;- corpus_reshape(cp, \"sentences\")\n\nd1 &lt;- matchCodes(cs, \n                dic.pol.es, \n                level = 1, \n                quietly=TRUE)\n\nd1 &lt;- d1[order(d1$value, decreasing = T),]\n\nlibrary(htmltools)\n\nbar_chart &lt;- function(label, \n                      width = \"100%\", \n                      height = \"1rem\", \n                      fill = \"purple\", \n                      background = NULL) {\n  \n  bar &lt;- div(\n          style = list(\n                    background = fill, \n                    width = width, \n                    height = height)\n          )\n  \n  chart &lt;- div(\n          style = list(\n                    flexGrow = 1, \n                    marginLeft = \"0.5rem\", \n                    background = background), \n          bar)\n  \n  div(\n    style = list(\n            display = \"flex\", \n            alignItems = \"center\"), \n    label, \n    chart)\n}\n\nreactable::reactable(\n            d1, \n            resizable = T,\n            filterable = T,            \n            columns = list(\n                        value = colDef(\n                        name = \"value\", \n                        align = \"left\", \n                        cell = function(value) {\n                            width &lt;- paste0(\n                                      value / max(d1$value) * 100, \n                                      \"%\")\n                            bar_chart(value, width = width)\n                            }\n                          )\n                        )\n            )\n\n\n\n\n\n\n\n\n\nAl examinar los resultados, la díada más frecuente corresponde a discurso-instituciones, con 746 ocurrencias, seguida de discurso-exterior, con 489, y discurso-social, con 358. Las menos frecuentes son instituciones-postmaterialismo, con 4 ocurrencias, y defensa-postmaterialismo, con 5.\n\n\nEl diagrama de cuerdas abajo revela el patrón en su conjunto, algo más difícil de observar solo por el examen de la tabla anterior. Además de discurso, temas sociales, e instituciones son los que más se asocian entre sí y con las demás categorías. Postmaterialismo, defensa y tecnología los que menos.\n\nCódigoplotChord(d1, from = \"term1\", to =\"term2\", value= \"value\")\n\n\n\n\n\n\n\nLa tabla abajo repite la operación, pero ahora para las categorías de segundo nivel. Ahora, la díada parlamento-retorica predomina, con 344 apariciones. España-retorica viene en segundo lugar, con 299 ocurrencias. Se tratan claramente de referencias al mismo Congreso de los Diputados y a los españoles y a España. Tales asociaciones corresponden a lo que ya hemos visto en análisis anteriores.\n\nCódigocs &lt;- corpus_reshape(cp, \"sentences\")\n\nd2 &lt;- matchCodes(cs, \n                dic.pol.es, \n                level = 2, \n                quietly=TRUE)\n\nd2 &lt;- d2[order(d2$value, decreasing = T),]\n\nlibrary(htmltools)\n\nbar_chart &lt;- function(label, \n                      width = \"100%\", \n                      height = \"1rem\", \n                      fill = \"purple\", \n                      background = NULL) {\n  \n  bar &lt;- div(\n          style = list(\n                    background = fill, \n                    width = width, \n                    height = height)\n          )\n  \n  chart &lt;- div(\n          style = list(\n                    flexGrow = 1, \n                    marginLeft = \"0.5rem\", \n                    background = background), \n          bar)\n  \n  div(\n    style = list(\n            display = \"flex\", \n            alignItems = \"center\"), \n    label, \n    chart)\n}\n\nreactable::reactable(\n            d2, \n            resizable = T,\n            filterable = T,\n            columns = list(\n                        value = colDef(\n                        name = \"value\", \n                        align = \"left\", \n                        cell = function(value) {\n                            width &lt;- paste0(\n                                      value / max(d2$value) * 100, \n                                      \"%\")\n                            bar_chart(value, width = width)\n                            }\n                          )\n                        )\n            )\n\n\n\n\n\n\n\n\n\n\n\nEl diagrama de cuerdas abajo señala las relaciones entre las categorías de segundo nivel. Las categorías que más se vinculan a otras son retórica, España, administración y otros temas de carácter social. Las menos asociadas son género, policía, memoria histórica y medioambiente.\n\nCódigoplotChord(d2, \n          from = \"term1\", \n          to =\"term2\", \n          value= \"value\")"
  },
  {
    "objectID": "02_explora_deductivo.html#consideraciones-finales",
    "href": "02_explora_deductivo.html#consideraciones-finales",
    "title": "\n3  Juego de palabras:Exploración deductiva y abductiva de textos\n",
    "section": "\n3.5 Consideraciones finales",
    "text": "3.5 Consideraciones finales\nEn esta parte hemos explorado el uso de estrategias de análisis que facilitan la selección, la codificación y el análisis temático de textos. Como ya ha sido mencionado de forma extensiva, no se trata de reemplazar métodos cualitativos tradicionales, sino de ofrecer entradas alternativas para el análisis de contenido. La mayoría de las técnicas presentadas aquí tienen en mente un contexto híbrido de investigación. De un lado, se desea mantener al máximo el rigor de un examen en profundidad del corpus. De otro, se quiere expandir el abanico de opciones disponibles para la identificación de patrones, su comunicación y el desarrollo de nuevas ideas.\nCabe subrayar que, aunque las estrategias desarrolladas aquí tengan en mente corpus con un número elevado de textos, las herramientas que se han presentado pueden emplearse también para una cantidad pequeña de documentos. Entrevistas, libros, cuadernos de campo, todos pueden ser objeto de la aplicación de los métodos discutidos aquí. Para un investigador acostumbrado a estudios cuantitativos, representa una oportunidad para examinar nuevas fuentes de información con mayor profundidad. Para los que se basan en métodos cualitativos, se abre la posibilidad de expandir sus instrumentos de trabajo con nuevas formas de visualización y exploración del material empírico con el que trabajan."
  },
  {
    "objectID": "02_explora_deductivo.html#ejercicios",
    "href": "02_explora_deductivo.html#ejercicios",
    "title": "\n3  Juego de palabras:Exploración deductiva y abductiva de textos\n",
    "section": "\n3.6 Ejercicios",
    "text": "3.6 Ejercicios\nEjercicio 1. Keyword in Context (kwic). Utilice la función kwic para buscar el término “igualdad” en el corpus de los discursos de investidura de los presidentes de gobierno españoles. Utilice una ventana de 7 palabras antes y después.\n\nSolución# Carga los paquetes tenet y quanteda\nlibrary(tenet)\nlibrary(quanteda)\n\n# Crea un corpus (discursos inaugurales Espana)\ncp &lt;- corpus(spa.inaugural)\n\n# Crea un data.frame a partir de\n# la funcion Keyword in Context de\n# Quanteda \nd &lt;- kwic(x = tokens(cp),\n          pattern= \"igualdad\",\n          window = 7)\n\n# Visualiza los resultados\nView(d)\n\n\nEjercicio 2. Árbol de palabras. Crea un árbol de palabras empleando el mismo corpus y la misma palabra del ejercicio anterior. Puedes intentarlo también con otras palabras de tu interés.\n\nSolución# Crea un arbol de palabras en tenet\nwordtree(corpus = cp,\n         keyword = \"igualdad\",\n         height = 800)\n\n# Palabra alternativa: empleo\n# (puede ser cualquiera que elijas)\nwordtree(corpus = cp,\n         keyword = \"empleo\",\n         height = 800)\n\n\nEjercicio 3. Etiquetado de textos. Seleccione el último discurso inaugural de Pedro Sánchez (texto 15) de la base de datos spa.inaugural y emplee las palabras clave “tecnol”, “ciencia”, “científ”, y “digital”.\n\nSolución# Examina un conjunto de palabras en el\n# discurso de investidura de Adolfo Suárez\ntagText(as.character(spa.inaugural$text[15]), \n        keywords = c(\"tecnol\", \n                     \"digital\", \n                     \"ciencia\", \n                     \"científ\"),\n        title = \"Pedro Sanchez (2019)\")\n\n\nEjercicio 4. Etiqueta do textos con diccionario. Repita el ejercicio anterior. No obstante, ahora, utilice el diccionario de la sección “Diccionarios como colecciones de códigos” para etiquetar el texto de Pedro Sánchez. Utilice la paleta de colors “FantasticFox1” para colorear las categorías, un tamaño de fuente de 20 y unos márgines de 120 pixeles.\n\nSolución# Crea el diccionario\ndic &lt;- dictionary(\n  list(\n    economica=c(\"econom\",\n               \"inversion\",\n               \"empresa\",\n               \"desarroll\",\n               \"monetari\",\n               \"industri\",\n               \"agric\",\n               \"agrari\"),\n    fiscal=c(\"hacienda\",\n               \"gasto\",\n               \"impuest\",\n               \"presupuest\",\n               \"tribut\",\n               \"tasa\",\n               \"fiscal\"),\n    educacion=c(\"educa\",\n             \"profesor\",\n             \"docent\",\n             \"escuel\",\n             \"colegio\",\n             \"universi\",\n             \"formación\"),\n    sanidad=c(\"sanidad\",\n               \"salud\",\n               \"hospital\",\n               \"sanitari\",\n               \"médic\",\n               \"enfermer\",\n               \"salud\"),\n    medioambiente=c(\"sostenible\",\n                 \"cambio clima\",\n                 \"medioambient\",\n                 \"reciclaje\",\n                 \"ecológico\",\n                 \"límpia\",\n                 \"invernadero\",\n                 \"emisiones\",\n                 \"carbono\",\n                 \"plástico\",\n                 \"fósiles\")))\n\n# genera un texto para ser leído en el panel\n# Viewer de RStudio\ntagText(spa.inaugural$text[15], \n        keywords = dic, \n        palette = \"FantasticFox1\",\n        font.size = 20, \n        title = \"Pedro Sanchez (2019)\",\n        margin = 120)\n\n\nEjercicio 5. Etiquetado de corpus. Ahora emplee el mismo diccionario para etiquetar todo el corpus de discursos de investidura. Reorganice los documentos en párrafos (“paragraph”) en lugar de emplear documentos enteros.\n\nSolución# La función tagCorpus etiqueta\n# todo un corpus de acuerdo con un\n# diccionario\ntagCorpus(cp,\n          dic, \n          reshape.to = \"paragraph\")\n\n\nEjercicio 6. Etiquetado de corpus. Ahora repita el último ejercicio. Pero ahora crea un nuevo diccionario con los términos que más te interesen.\n\nSolución# Crea un nuevo diccionario, he elegido\n# algunas políticas públicas concretas\ndic.nuevo &lt;- dictionary(\n  list(\n    gobierno=c(\"gobierno\",\n               \"instituciones\",\n               \"comunidades\",\n               \"autonom\",\n               \"administracion\",\n               \"entidades\"),\n    politica=c(\"politica\",\n               \"partido\",\n               \"accion\",\n               \"cortes\"),\n    patria=c(\"patria\",\n             \"nacion\",\n             \"espanoles\",\n             \"pueblo\"),\n    justicia=c(\"constituci\",\n               \"justicia\",\n               \"judic\",\n               \"ley\",\n               \"legal\"),\n    financiero=c(\"financ\",\n                 \"hacienda\",\n                 \"presupuesto\",\n                 \"tributa\",\n                 \"fiscal\"),\n    equidad=c(\"desigual\", \n              \"igualdad\")))\n\n# Etiqueta los textos según el nuevo\n# diccionario\ntagCorpus(cp,\n          dic.nuevo, \n          reshape.to = \"paragraph\")\n\n\nEjercicio 7. Estadísticas temáticas. Utilice el diccionario empleado en el ejercicio 4 para contar la frecuencia de cada código en el corpus de discursos de investidura. Recuerda que debes utilizar la función countKeywords. No elimine los códigos con frecuencia igual a cero y utilice la frecuencia absoluta.\n\nSolución# Calcula la frecuencia ABSOLUTA en que cada\n# palabra ha sido encontrada para cada nivel\n# del diccionario dic\nxy &lt;- countKeywords(cp, \n                    dic, \n                    quietly = TRUE)\n\n# Visualiza los resultados\nreactable::reactable(xy, \n                     resizable = T,\n                     filterable = T)\n\n\nEjercicio 8. Diagrama de árbol. Utilice los resultados del ejercicio anterior para crear un diagrama de árbol (Force Directed Tree). No obstante, ahora elimine las palabras clave con frecuencia igual a cero. Utilice “level1” y “keywords” como grupos y “keywords” como elementos.\n\nSolución# Elimina los valores con frecuencia\n# igual a cero\nxy &lt;- xy[xy$frequency&gt;0,]\n\n# Formatea los datos para que puedan \n# ser representados en el gráfico\njs &lt;- jsonTree(data = xy, \n               groups=c(\"level1\",\"keyword\"), \n               elements = \"keyword\", \n               value=\"frequency\")\n\n# Genera el gráfico\nforceDirectedTree(js)\n\n\nEjercicio 9. Árbol de Voronoi. Utilice los resultados del ejercicio 7 para crear ahora un diagrama de Voronoi (Voronoi Treemap). No obstante, ahora elimine las palabras clave con frecuencia igual a cero. Utilice “level1” y “keywords” como grupos y “keywords” como elementos.\n\nSolución# Genera el gráfico\nplotVoronoiTree(data = xy,\n                groups = \"level1\",\n                elements = \"keyword\",\n                value = \"frequency\")\n\n\nEjercicio 10. Estadísticas temáticas: filtro. Repita la operación del ejercicio 7, pero ahora desagrega los resultados por presidente y emplea la frecuencia relativa.\n\nSolución# Calcula la frecuencia ABSOLUTA en que cada\n# palabra ha sido encontrada para cada nivel\n# del diccionario dic\nxy &lt;- countKeywords(cp, \n                    dic,\n                    group.var = \"President\",\n                    quietly = TRUE,\n                    rel.freq = T)\n\n# Visualiza los resultados\nreactable::reactable(xy, \n                     resizable = T,\n                     filterable = T)\n\n\nEjercicio 11. Diagrama aluvial de Sankey. Genere un diagrama de Sankey con los resultados del ejercicio anterior. No olvides de agregar los valores por grupo (“groups”) y por código (“level1”) y eliminar los códigos con frecuencia igual a cero.\n\nSolución# Agrega los resultados por presidente y\n# por código del diccionario\nxx &lt;- aggregate(list(frequency=xy$frequency), \n                 by=list(groups=xy$groups,\n                         level1=xy$level1), \n                 sum, na.rm=T)\n\n# Elimina los términos sin correspondencia\n# en el corpus\nxx &lt;- xx[xx$frequency&gt;0,]\n\n# General el gráfico \nplotSankey(xx, \n           from = \"groups\", \n           to=\"level1\", \n           value = \"frequency\")\n\n\nEjercicio 12. Coocurrencia de códigos. Reorganice el corpus en el nivel de párrafos. Calcule las coocurrencias de los códigos del diccionario dic.\n\nSolución# Reorganiza el corpus según párrafos\ncs &lt;- corpus_reshape(cp, \"paragraph\")\n\n# Calcula las coocurrencias\nd1 &lt;- matchCodes(cs, \n                dic, \n                level = 1, \n                quietly=TRUE)\n\n# Ordena las frecuencias de mayor a menor\nd1 &lt;- d1[order(d1$value, decreasing = T),]\n\n# Visualiza los resultados\nView(d1)\n\n\nEjercicio 13. Diagrama de cuerdas. Utilice los resultados del ejercicio anterior para crear un diagrama de cuerdas que permita visualiza las conexiones entre los distintos términos del diccionario dic aplicado al corpus de discursos de investidura.\n\nSolución# Visualiza los resultados\nplotChord(d1, \n          from = \"term1\", \n          to =\"term2\", \n          value= \"value\")"
  },
  {
    "objectID": "02_explora_deductivo.html#lecturas-adicionales",
    "href": "02_explora_deductivo.html#lecturas-adicionales",
    "title": "\n3  Juego de palabras:Exploración deductiva y abductiva de textos\n",
    "section": "\n3.7 Lecturas adicionales",
    "text": "3.7 Lecturas adicionales\n\n\nAttride-Stirling J (2001). “Thematic networks: an analytic tool for qualitative research.” Qualitative research, 1(3), 385-405.\n\n\nCon un número de 8.178 citas en Google Académico, el artículo de Attride-Stirling se ha convertido en referencia casi obligatoria en el proceso de codificación temática. Se destaca por presentar los temas no como ideas sueltas creadas a partir de códigos, sino como redes jerárquicas que van desde los referentes empíricos (en nuestro caso los textos) hasta el tema central del estudio.\n\n\nKennedy BL, Thornberg R (2018). “Deduction, induction, and abduction.” In The SAGE handbook of qualitative data collection, chapter 4, 49-64. SAGE Publications, London.\n\n\nEl texto de Kennedy y Tornberg representa una excelente introducción a las lógicas inductiva, deductiva y abductiva. Su consulta resulta muy recomendable, si deseas conocer más sobre las principales características y diferencias entre esos tipos de razonamiento o métodos de construcción de conocimiento.\n\n\nSaldana J (2015). The Coding Manual for Qualitative Researchers Third Edition. SAGE, Los Angeles ; London.\n\n\nEl libro de Saldaña corresponde a una guía detallada del proceso de codificación temática. No solo detalla los distintos tipos de código como sugiere algunas heurísticas para aplicarlos a los textos. Se trata de una referencia básica para el proceso de creación de códigos.\n\n\nThompson J (2022). “A guide to abductive thematic analysis.” The Qualitative Report, 5(27), 1410-1421.\n\n\nEl breve artículo de Thompson suministra una guía paso a paso de cómo emplear un razonamiento abductivo en el análisis temático en las ciencias sociales. Constituye una lectura interesante para aquellos que desean una orientación más detallada de cómo llevar a cabo el proceso de análisis de texto.\n\n\n\n\n\nAttride-Stirling, Jennifer. 2001. “Thematic Networks: An Analytic Tool for Qualitative Research.” Qualitative Research 1 (3): 385–405.\n\n\nAuerbach, Carl, and Louise B. Silverstein. 2003. Qualitative Data: An Introduction to Coding and Analysis. New York: NYU press.\n\n\nBalzer, Michael, and Oliver Deussen. 2005. “Voronoi Treemaps.” In IEEE Symposium on Information Visualization, 2005. INFOVIS 2005, 49–56. IEEE. https://doi.org/10.1109/INFVIS.2005.1532128.\n\n\nBremer, Nadieh, and Shirley Wu. 2012. Data Sketches: A Journey of Imagination, Exploration, and Beautiful Data Visualizations. New York: CRC Press.\n\n\nGuest, Greg, Kathleen M. MacQueen, and Emily E. Namey. 2011. Applied Thematic Analysis. Thousand Oaks, CA: SAGE Publications.\n\n\nHolten, Danny. 2006. “Hierarchical Edge Bundles: Visualization of Adjacency Relations in Hierarchical Data.” IEEE Transactions on Visualization and Computer Graphics 12 (5): 741–48.\n\n\nKennedy, A B W, and H R Sankey. 1898. “The Thermal Efficiency of Steam Engines. Report of the Committee Appointed to the Council Upon the Subject of the Definition of a Standard or Standards of Thermal Efficiency for Steam Engines: With an Introductory Note. (Including Appendixes and Plate at Back of Volume).” Minutes of the Proceedings of the Institution of Civil Engineers 134 (1898): 278–312. https://doi.org/10.1680/imotp.1898.19100.\n\n\nKennedy, Brianna L., and Robert Thornberg. 2018. “Deduction, Induction, and Abduction.” In The SAGE Handbook of Qualitative Data Collection, 49–64. London: SAGE Publications.\n\n\nKrippendorff, Klaus. 2004. Content Analysis: An Introduction to Its Methodology. Thousand Oaks, CA: Sage publications.\n\n\nMiles, Matthew B., A. Michael Huberman, and Johnny Saldana. 2019. Qualitative Data Analysis: A Methods Sourcebook. Thousand Oaks, CA: SAGE Publications.\n\n\nNeuendorf, Kimberly A. 2017. The Content Analysis Guidebook. Thousand Oaks, CA: SAGE Publications.\n\n\nRiehmann, Patrick, Manfred Hanfler, and Bernd Froehlich. 2005. “Interactive Sankey Diagrams.” In IEEE Symposium on Information Visualization, 2005. INFOVIS 2005., 233240. IEEE.\n\n\nSaldana, Johnny. 2015. The Coding Manual for Qualitative Researchers Third Edition. Los Angeles ; London: SAGE.\n\n\nThompson, Jamie. 2022. “A Guide to Abductive Thematic Analysis.” The Qualitative Report 5 (27): 1410–21.\n\n\nWattenberg, Martin, and Fernanda B. Viégas. 2008. “The Word Tree, an Interactive Visual Concordance.” IEEE Transactions on Visualization and Computer Graphics 14 (6): 12211228."
  },
  {
    "objectID": "02_explora_deductivo.html#referencias",
    "href": "02_explora_deductivo.html#referencias",
    "title": "\n3  Juego de palabras:Exploración deductiva y abductiva de textos\n",
    "section": "\n3.8 8. Referencias",
    "text": "3.8 8. Referencias\n\n\n\n\nAttride-Stirling, Jennifer. 2001. “Thematic Networks: An Analytic Tool for Qualitative Research.” Qualitative Research 1 (3): 385–405.\n\n\nAuerbach, Carl, and Louise B. Silverstein. 2003. Qualitative Data: An Introduction to Coding and Analysis. New York: NYU press.\n\n\nBalzer, Michael, and Oliver Deussen. 2005. “Voronoi Treemaps.” In IEEE Symposium on Information Visualization, 2005. INFOVIS 2005, 49–56. IEEE. https://doi.org/10.1109/INFVIS.2005.1532128.\n\n\nBremer, Nadieh, and Shirley Wu. 2012. Data Sketches: A Journey of Imagination, Exploration, and Beautiful Data Visualizations. New York: CRC Press.\n\n\nGuest, Greg, Kathleen M. MacQueen, and Emily E. Namey. 2011. Applied Thematic Analysis. Thousand Oaks, CA: SAGE Publications.\n\n\nHolten, Danny. 2006. “Hierarchical Edge Bundles: Visualization of Adjacency Relations in Hierarchical Data.” IEEE Transactions on Visualization and Computer Graphics 12 (5): 741–48.\n\n\nKennedy, A B W, and H R Sankey. 1898. “The Thermal Efficiency of Steam Engines. Report of the Committee Appointed to the Council Upon the Subject of the Definition of a Standard or Standards of Thermal Efficiency for Steam Engines: With an Introductory Note. (Including Appendixes and Plate at Back of Volume).” Minutes of the Proceedings of the Institution of Civil Engineers 134 (1898): 278–312. https://doi.org/10.1680/imotp.1898.19100.\n\n\nKennedy, Brianna L., and Robert Thornberg. 2018. “Deduction, Induction, and Abduction.” In The SAGE Handbook of Qualitative Data Collection, 49–64. London: SAGE Publications.\n\n\nKrippendorff, Klaus. 2004. Content Analysis: An Introduction to Its Methodology. Thousand Oaks, CA: Sage publications.\n\n\nMiles, Matthew B., A. Michael Huberman, and Johnny Saldana. 2019. Qualitative Data Analysis: A Methods Sourcebook. Thousand Oaks, CA: SAGE Publications.\n\n\nNeuendorf, Kimberly A. 2017. The Content Analysis Guidebook. Thousand Oaks, CA: SAGE Publications.\n\n\nRiehmann, Patrick, Manfred Hanfler, and Bernd Froehlich. 2005. “Interactive Sankey Diagrams.” In IEEE Symposium on Information Visualization, 2005. INFOVIS 2005., 233240. IEEE.\n\n\nSaldana, Johnny. 2015. The Coding Manual for Qualitative Researchers Third Edition. Los Angeles ; London: SAGE.\n\n\nThompson, Jamie. 2022. “A Guide to Abductive Thematic Analysis.” The Qualitative Report 5 (27): 1410–21.\n\n\nWattenberg, Martin, and Fernanda B. Viégas. 2008. “The Word Tree, an Interactive Visual Concordance.” IEEE Transactions on Visualization and Computer Graphics 14 (6): 12211228."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referencias",
    "section": "",
    "text": "Attride-Stirling, Jennifer. 2001. “Thematic Networks: An Analytic\nTool for Qualitative Research.” Qualitative Research 1\n(3): 385–405.\n\n\nAuerbach, Carl, and Louise B. Silverstein. 2003. Qualitative Data:\nAn Introduction to Coding and Analysis. New York: NYU press.\n\n\nBalzer, Michael, and Oliver Deussen. 2005. “Voronoi\nTreemaps.” In IEEE Symposium on Information Visualization,\n2005. INFOVIS 2005, 49–56. IEEE. https://doi.org/10.1109/INFVIS.2005.1532128.\n\n\nBremer, Nadieh, and Shirley Wu. 2012. Data Sketches: A Journey of\nImagination, Exploration, and Beautiful Data Visualizations. New\nYork: CRC Press.\n\n\nGuest, Greg, Kathleen M. MacQueen, and Emily E. Namey. 2011. Applied\nThematic Analysis. Thousand Oaks, CA: SAGE Publications.\n\n\nHolten, Danny. 2006. “Hierarchical Edge Bundles: Visualization of\nAdjacency Relations in Hierarchical Data.” IEEE Transactions\non Visualization and Computer Graphics 12 (5): 741–48.\n\n\nKennedy, A B W, and H R Sankey. 1898. “The Thermal Efficiency of\nSteam Engines. Report of the Committee Appointed to the Council Upon the\nSubject of the Definition of a Standard or Standards of Thermal\nEfficiency for Steam Engines: With an Introductory Note. (Including\nAppendixes and Plate at Back of Volume).” Minutes of the\nProceedings of the Institution of Civil Engineers 134 (1898):\n278–312. https://doi.org/10.1680/imotp.1898.19100.\n\n\nKennedy, Brianna L., and Robert Thornberg. 2018. “Deduction,\nInduction, and Abduction.” In The SAGE Handbook of\nQualitative Data Collection, 49–64. London: SAGE Publications.\n\n\nKrippendorff, Klaus. 2004. Content Analysis: An Introduction to Its\nMethodology. Thousand Oaks, CA: Sage publications.\n\n\nMiles, Matthew B., A. Michael Huberman, and Johnny Saldana. 2019.\nQualitative Data Analysis: A Methods Sourcebook. Thousand Oaks,\nCA: SAGE Publications.\n\n\nNeuendorf, Kimberly A. 2017. The Content Analysis Guidebook.\nThousand Oaks, CA: SAGE Publications.\n\n\nRiehmann, Patrick, Manfred Hanfler, and Bernd Froehlich. 2005.\n“Interactive Sankey Diagrams.” In IEEE Symposium on\nInformation Visualization, 2005. INFOVIS 2005., 233240. IEEE.\n\n\nSaldana, Johnny. 2015. The Coding Manual for Qualitative Researchers\nThird Edition. Los Angeles ; London: SAGE.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with r: A Tidy\nApproach. London: O’Reilly Media. https://www.tidytextmining.com/.\n\n\nThompson, Jamie. 2022. “A Guide to Abductive Thematic\nAnalysis.” The Qualitative Report 5 (27): 1410–21.\n\n\nWattenberg, Martin, and Fernanda B. Viégas. 2008. “The Word Tree,\nan Interactive Visual Concordance.” IEEE Transactions on\nVisualization and Computer Graphics 14 (6): 12211228."
  }
]