[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Análisis de textos políticos con R",
    "section": "",
    "text": "Prefacio\nAquí el prefacio del libro. Se trata de una breve introducción con el propósito, el público a que se destina y la descripción de la estructura."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introducción",
    "section": "",
    "text": "Aquí la introducción. Un capítulo introductorio sobre el análisis de textos políticos. Como el prefacio, escribirlo por último. De ese modo, se podrá adaptar al contenido final del libro."
  },
  {
    "objectID": "01_explora_inductivo.html#introducción",
    "href": "01_explora_inductivo.html#introducción",
    "title": "\n2  Primeros pasos\n",
    "section": "\n2.1 Introducción",
    "text": "2.1 Introducción\nLa Teogonía de Hesíodo describe el paso del Caos -el estado de desorden originario- hacia un orden divino por medio de un catálogo de nacimientos y creación de diferentes dioses y seres mitológicos. Los filósofos presocráticos, por otra parte, intentaban explicar el origen del mundo a partir de un principio ordenador fundamental (ἀρχή), una idea clave a partir de la cuál se derivaba todo lo demás. En la primera visión, el mundo se delinea a partir de una serie de eventos sucesivos y caprichos de los dioses en su lucha por protagonismo. No existe un principio claro, sino que tal empresa depende de actos de voluntad, poder y negociación entre los participantes. En la segunda, existe una ley inexorable que estructura nuestra realidad y cabe al pensador desvelarla por medio de la razón.\nEn lo que tange al análisis de textos también podemos encontrar dos acercamientos análogos. Uno puede adentrar en sus “misterios” desproveído de nociones anteriores y explorar los patrones y estructuras que puedan emerger sin el auxilio de guías previos que orienten tal aventura. El otro modo consiste en tener un norte claro desde el principio y emplear nociones e hipótesis como lentes que orientan el examen de los documentos. Al primero lo llamamos inductivo y al segundo deductivo.\nCada una de esas maneras de mirar hacia los textos subraya una forma alternativa de aprender. El método inductivo conduce a un viaje errante, sin destino cierto, pero plagado de sorpresas y nuevos descubrimientos. El deductivo, por su parte, supone un destino y un rumbo a la vista, permite pocos desvíos. Aunque esté abierto a la serendipia, los principios de partida y los objetivos suelen ser definidos de antemano.\nSe consideran antagónicos solamente bajo sus formas típicas o ideales. En el trabajo de análisis, sin embargo, resulta muy poco frecuente examinar cualquier documento sin nociones previas o al menos cierta intuición de qué se podría encontrar. Tampoco abundan trabajos que germinan provistos de un catálogo meticuloso de los instrumentos adecuados y rutas correctas hacia los objetivos. La ciencia es una labor errante. Conocer a partir de textos es un proceso iterativo, que supone múltiples acercamientos sucesivos, idas y venidas constantes. La combinación de ambos acercamientos, aunque predomine uno de ellos, resulta inevitable.\nEsta parte del trabajo se centra en algunas estrategias inductivas para el análisis de textos. Se ha decidido solamente emplear aquí métodos que no requieran la lectura previa de los textos por dos razones. La primera es didáctica y evitar confundir tales métodos con otros de carácter deductivo. La segunda, igualmente importante, consiste en señalar la utilidad del método inductivo para la generación de hipótesis, desarrollo de diccionarios e identificación de temas en corpus formados por un elevado número de documentos. Se trata de una combinación entre una introducción metodológica y su aplicación práctica inmediata. Empezaremos con la apertura de los textos, la creación de un corpus para luego aplicar distintas técnicas preparatorias que ayudarán a viabilizar el análisis."
  },
  {
    "objectID": "01_explora_inductivo.html#primeros-pasos",
    "href": "01_explora_inductivo.html#primeros-pasos",
    "title": "\n2  Primeros pasos\n",
    "section": "\n2.2 Primeros pasos",
    "text": "2.2 Primeros pasos\n\n2.2.1 Creaccion de un corpus\nUna vez los documentos han sido preparados y pre-procesados, pueden ser abiertos en R. La función readtext del paquete con el mismo nombre permite importar (o “abrir”) textos individuales o carpetas enteras. Los documentos pueden ser de diferentes formatos: txt, doc(x), pdf, html, csv, tab, tsv, xml, xls(x), json, odt, o rtf. Se trata de una función muy útil para importar volúmenes grandes de texto.\n\nCódigo# Obtiene una lista de archivos en\n# una carpeta online de Github\nlibrary(jsonlite)\n\nurl &lt;- \"https://api.github.com/repos/rodrodr/tenet_texts/contents/spa.inaugural\"\n\nnm &lt;- read_json(url)\nnm &lt;- list2DF(nm)\nnm &lt;- sort(as.character(unlist(nm[8,])))\n\n# Carga el paquete\nlibrary(readtext)\n\n# Importa los textos\ntx &lt;- readtext(nm)\n\n# Ordena por nombre de archivo\ntx &lt;- tx[order(tx$doc_id),]\n\n# Visualiza los resultados\nreactable::reactable(tx,\n                     resizable = T, \n                     wrap = F)\n\n\n\n\n\n\n\nComo se puede observar, se cargan 15 discursos de investidura de los Presidentes de gobierno de España desde 1979 hasta la actualidad. Se trata de un objeto de tipo data.frame con dos columnas: doc_id, en general el nombre del archivo, y text, que contiene el texto integral. Este formato servirá de base y resulta obligatorio para la transformación de esos textos en un objeto de tipo corpus perteneciente al paquete quanteda, base o infraestructura de la mayor parte de los análisis realizados durante todo el curso.\nAdemás de doc_id y text, el data.frame, uno puede añadir más variables que ayuden a contextualizar los documentos y suministren información útil para el posterior análisis. No obstante, hay que tener claro que la función readtext solamente genera las dos primeras variables. Los metadatos adicionales deben ser añadidos a posteriori, sea justo después de la importación o, luego, como documentación del corpus, como veremos más adelante.\nEl hecho de que utilicemos textos guardados en una carpeta en la nube hace con que el código arriba sea un poco más complejo del que sería necesario. En el caso de que los archivos estén en el disco duro bastaría con informar el camino hacia la carpeta:\n\nCódigo# Carga el paquete\nlibrary(readtext)\n\n# Importa los textos\ntx &lt;- readtext(\"/Escritorio/Carpeta/\")\n\n# Ordena por nombre de archivo\ntx &lt;- tx[order(tx$doc_id),]\n\n# Visualiza los resultados\nreactable::reactable(tx,\n                     resizable = T, \n                     wrap = F)\n\n\nUna vez abiertos los datos, existen dos opciones. La primera es tratar los datos para extraer metadatos o agregar/fragmentar los textos en otras unidades de observación (como los tweets de un mismo partido, o fragmentar un libro por capítulos). La segunda consiste en transformar el data.frame en un objeto corpus y seguir con el análisis:\n\nCódigo# Carga el paquete quanteda\nlibrary(quanteda)\n\n# Transforma los textos en corpus\ncp &lt;- corpus(tx)\n\n# Visualiza los resultados\nreactable::reactable(summary(cp),\n                     resizable = T, \n                     wrap = F)\n\n\n\n\n\n\n\nAl explorar el objeto corpus por medio de la función summary(cp), vemos un conjunto de variables descriptivas: Text, nombre del documento; Types, señala el número de palabras y símbolos únicos en el documento; Tokens, número total de palabras y símbolos; y Sentences, cantidad de frases en el texto.\n\n2.2.2 Adicionar metadatos\nEl siguiente paso consiste en adicionar más información contextual (metadatos) sobre los textos. Tales informaciones resultarán de mucha utilidad en las siguientes etapas de análisis, puesto que permitirán agregar las informaciones según distintas características. Por ejemplo, podemos decidir agrupar los textos según presidente (y no gestión o legislatura). También podríamos organizar el análisis según partido del presidente o por su ideología.\nCuanto mayor la documentación de los textos, mayores las posibilidades de reagrupar, fragmentar o reordenar los textos según distintas categorías analíticas. Además, se posibilitan distintas comparaciones entre grupos y entre éstos con el patrón general.\nLa función docvars posibilita crear nuevas variables contextuales o de metadatos en un corpus. Su sintaxe resulta muy sencilla:\ndocvars(corpus,“variable name” ) &lt;- variable con el contenido.\n\nCódigodocvars(cp, \"Presidente\") &lt;- c(\"Adolfo Suárez\",\n                               \"Leopoldo Calvo Sotelo\",\n                               \"Felipe González\",\n                               \"Felipe González\",\n                               \"Felipe González\",\n                               \"Felipe González\",\n                               \"José María Aznar\",\n                               \"José María Aznar\",\n                               \"José Luis Zapatero\",\n                               \"José Luis Zapatero\",\n                               \"Mariano Rajoy\",\n                               \"Mariano Rajoy\",\n                               \"Mariano Rajoy\",\n                               \"Pedro Sánchez\",\n                               \"Pedro Sánchez\")\n\ndocvars(cp, \"Nombramiento\") &lt;- c(\"1979-03-31\",\n                                 \"1981-02-26\",\n                                 \"1982-12-02\",\n                                 \"1986-06-23\",\n                                 \"1989-12-05\",\n                                 \"1993-07-09\",\n                                 \"1996-05-04\",\n                                 \"2000-04-26\",\n                                 \"2004-04-17\",\n                                 \"2008-04-11\",\n                                 \"2011-12-20\",\n                                 \"2015-12-21\",\n                                 \"2016-10-30\",\n                                 \"2018-06-01\",\n                                 \"2020-01-07\")\n\n\ndocvars(cp, \"Cese\") &lt;- c(\"1981-02-26\",\n                         \"1982-12-02\",\n                         \"1986-06-23\",\n                         \"1989-10-30\",\n                         \"1993-06-07\",\n                         \"1996-03-04\",\n                         \"2000-03-13\",\n                         \"2004-03-15\",\n                         \"2008-03-10\",\n                         \"2011-11-21\",\n                         \"2015-12-21\",\n                         \"2016-10-29\",\n                         \"2018-06-01\",\n                         \"2019-04-29\",\n                         NA)\n\n\ndocvars(cp, \"Partido\") &lt;- c(\"UCD\", \"UCD\", \"PSOE\", \n                            \"PSOE\", \"PSOE\",\"PSOE\",\"PP\",\n                            \"PP\",\"PSOE\",\"PSOE\",\n                            \"PP\",\"PP\",\"PP\",\n                            \"PSOE\",\"PSOE\")\n\n\n\ndocvars(cp, \"Ideología\") &lt;- c(\"Derecha\", \"Derecha\", \"Izquierda\", \n                            \"Izquierda\", \"Izquierda\",\"Izquierda\",\n                            \"Derecha\", \"Derecha\", \"Izquierda\",\n                            \"Izquierda\", \"Derecha\", \"Derecha\", \n                            \"Derecha\", \"Izquierda\", \"Izquierda\")\n\n\n# Visualiza los resultados\nreactable::reactable(summary(cp),\n                     resizable = T, \n                     wrap = F)\n\n\n\n\n\n\nComo se puede observar en la tabla arriba, se han añadido las variables con el nombre del presidente, fecha de nombramiento, cese, el partido político al que pertenecía y la ideología de la mayor parte de los miembros de los partidos. Estas categorías permitirán separar en la fase de análisis diferentes perfiles de grupo como, por ejemplo, los conceptos o expresiones más utilizados por líderes de derecha e izquierda o por cada partido.\n\n2.2.3 Transformar un corpus\nOtra opción consiste en reorganizar el texto según nuevas unidades de análisis. A veces, algunos aspectos del discurso se desvelan de modo más claro cuando las información se organiza desde una perspectiva distinta. Esa pseudo-alteridad se puede alcanzar a veces por mirar a un mismo texto desde otro ángulo. ¿Qué cambios se pueden observar en la importancia de los conceptos cuando organizamos los textos según partido o ideología y no más de acuerdo con cada una de las legislaturas? ¿Aparece algo nuevo? ¿Existen contradicciones o patrones distintos frente a lo que habíamos percibido en el análisis anterior?\nSe pueden adoptar dos estrategias fundamentales. La primera consiste en fragmentar los textos en unidades menores como párrafos o sentencias, por ejemplo. La segunda trata de agregar los textos a partir de características comunes, como juntar todos los documentos de una misma ideología. Además, se pueden combinar entre sí. Podemos juntar todos los textos por partido y luego fragmentarlos por frase. De cualquier forma, el cambio en la unidad de observación debe tener un propósito analítico claro. ¿Qué se quiere aprender al estructurar los textos de una manera determinada?\nEmpecemos con la fragmentación. Utilicemos el corpus de discursos de inauguración de los presidentes de gobierno españoles y dividamos el corpus por párrafo. Esto se puede hacer con la función corpus_reshape de quanteda.\n\nCódigo# Reorganiza el corpus segun frases\ncs &lt;- corpus_reshape(x = cp, to = \"sentences\")\n\n# Visualiza los 100 primeros resultados\nreactable::reactable(summary(cs),\n                     resizable = T, \n                     wrap = F)\n\n\n\n\n\n\n\nPara agregar los textos, hay que dar un paso atrás y aunar los textos de un mismo grupo en un único documento. Para ello, podemos convertir el corpus documentado en un objeto de tipo data.frame y, luego, agregar los textos y volver a crear un corpus con la nueva unidad de observación. Utilizaremos ahora los presidentes como unidad.\n\nCódigo# Convierte el corpus documentado en un data.frame\ntd &lt;- convert(cp, to=\"data.frame\")\n\n# Unifica los textos en un solo documento a partir\n# de las funciones aggregate (que agrega por grupos)\n# y paste0, que colapsa textos.\n# Hemos decidido utilizar dos separadores de linea (\\n\\n)\n# para indicar la separacion entre un texto y otro\ntd &lt;- aggregate(list(text=td$text), by=list(Presidente=td$Presidente,\n                                      Partido=td$Partido,\n                                      Ideologia=td$Ideología),\n                                paste0,\n                                collapse=\"\\n\\n\")\n\n# vuelve a crear un corpus con el nuevo\n# objeto agregado\ncx &lt;- corpus(td)\n\n# Visualiza los 100 primeros resultados\nreactable::reactable(summary(cx),\n                     resizable = T, \n                     wrap = F)\n\n\n\n\n\n\n\nAhora mismo tenemos solamente siete documentos en el corpus. Corresponden a la nueva unidad de agregación: Presidente. Con dichas informaciones, la comparación se hace entre estilos discursivos de los líderes, más que un período sobre otro. Haríamos lo mismo para los partidos o la ideología. Incluso podríamos utilizar un corpus para cada unidad y comparar los resultados."
  },
  {
    "objectID": "01_explora_inductivo.html#el-arte-de-contar-palabras",
    "href": "01_explora_inductivo.html#el-arte-de-contar-palabras",
    "title": "\n2  Primeros pasos\n",
    "section": "\n2.3 El arte de contar palabras",
    "text": "2.3 El arte de contar palabras\nUna vez terminada la preparación del corpus, toca empezar el análisis. El modo más sencillo consiste en identificar qué palabras, conceptos o términos aparecen con mayor frecuencia y averiguar si hay diferencias sustantivas en su uso entre los documentos del corpus. Se trata de un método de análisis aplicable tanto a conjuntos pequeños de textos, que pueden ser leídos con antelación por el investigador, como a grandes repositorios imposibles de leer sin un proceso previo de análisis, clasificación y muestreo.\nEn esta parte del trabajo trataremos de cuatro temas relacionados con el recuento directo de palabras. El primero abarca las técnicas de preparación y cálculo de frecuencias de palabras, tanto para el corpus como un todo como para cada documento o grupo en particular. El segundo repite las operaciones, pero en lugar de palabras completas, se emplearán sus raíces (stemming). El tercero describe la ponderación de las frecuencias por su ocurrencia en todos los documentos. El cuarto se dedica a visualizaciones, como las nubes de palabras.\n\n2.3.1 Frecuencia de palabras\nEl primer paso para contar palabras o expresiones consiste en tokenizar el corpus y, a continuación, crear una matriz documento-atributo (dfm, en su acrónimo en inglés). Se trata de un procedimiento sencillo que fragmenta cada texto en palabras, n-gramas (conjunto de n-palabras que aparecen juntas como en el bigrama “economía política”, por ejemplo) o incluso frases.\n\nCódigo# Crea un objeto de tipo tokens por palabra\ntk &lt;- tokens(cp)\n\n# Crea un objeto dfm\nfm &lt;- dfm(tk)\n\n# Buscamos las 10 palabras más frecuentes\ntopfeatures(fm)\n\n   de     ,    la     .     y   que    en    el     a   los \n11069 10122  7332  5247  5168  5022  4535  4095  3218  2974 \n\n\nComo podemos ver, no aprendemos nada de la política española mirando hacia los 10 términos más frecuentes. Todos son conectores o puntuación que se repiten sistemáticamente en cualquier texto. Probablemente, “de”, “que”, “y”, “la”, así como la coma o el punto y aparte serán las palabras y los símbolos más comunes en cualquier texto escrito en español. Tales palablas se conocen como stop words o “palabras vacías” de contenido que suelen ser muy frecuentes en cualquier idioma. Para evitar que ellas supongan un problema, lo mejor es quitarlas del medio y recrear la matriz de frecuencias.\n\nCódigo# Crea un objeto de tipo tokens por palabra eliminando la punctuacion\ntk &lt;- tokens(cp, remove_punct = T)\n\n# Elimina las stopwords\ntk &lt;- tokens_remove(tk, stopwords(language = \"es\"))\n\n# Crea un objeto dfm\nfm &lt;- dfm(tk)\n\n# Buscamos las 10 palabras más frecuentes\ntopfeatures(fm)\n\n gobierno    españa  política  señorías    social      país  sociedad españoles \n      720       589       483       463       316       289       288       263 \n   empleo   sistema \n      255       253 \n\n\nAhora el panorama ha cambiado. Aparecen nuevos términos, como “gobierno”, “España”, “españoles”, “política”, “social”, “sociedad”, “empleo” o “sistema”. También palabras específicas de tratamiento formal en los discursos inaugurales o en intervenciones parlamentarias, como es el caso de “señorías”.\nUno puede visualizar la frecuencia de palabras en un corpus por medio de una nube de palabras. Aunque sea un recurso más estético que informativo, sirve para tener una idea somera e inicial del peso relativo de los términos en un corpus o documento específico. El código abajo utiliza la función wordcloud del paquete homónimo para generar el gráfico:\n\nCódigolibrary(quanteda.textplots)\nlibrary(tenet)\nlibrary(wordcloud)\n\nft &lt;-topfeatures(fm, 50)\n\npar(mar=rep(0,4))\nwordcloud(names(ft), \n          freq = ft, \n          colors = pal$cat.cartocolor.antique.11)\n\n\n\n\nAbajo, buscamos las 25 palabras más comunes, calculamos su frecuencia relativa y, además, creamos dos gráficos para representarlas. Utilizamos la función dfm_weight para obtener el peso relativo de los términos en el corpus. Esta última medida ponderada resulta especialmente importante: (a) cuando comparamos su peso en cada uno de los textos y (b) cuando la extensión de los documentos resulta muy distinta.\n\nCódigo# Buscamos las 25 palabras más frecuentes\nft &lt;- topfeatures(fm, n = 25)\n\n# Añadimos la frecuencia relativa \nfp &lt;- dfm_weight(fm, \"prop\")\n\n# Repite la búsqueda para la frecuencia relativa\nfr &lt;- topfeatures(fp, n = 25)\n\n# Convierte los resultados en un data.frame\nxx &lt;- data.frame(Palabra=names(ft), Frec.Abs=ft, Frec.Rel=fr)\n\n# Carga el paquete ggplot2\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(grid)\n\n# Genera un gráfico de barras para visualizar la frecuencia de las palabras\np1 &lt;- ggplot(xx, aes(x=Frec.Abs, y=reorder(Palabra, Frec.Abs)))+\n  geom_bar(stat=\"identity\", fill=\"darkgreen\")+\n  theme_classic()+\n  labs(title=\"Frecuencia ABSOLUTA\")+\n  ylab(\"\")+\n  xlab(\"Frecuencia Absoluta\")\n\np2 &lt;- ggplot(xx, aes(x=Frec.Rel, y=reorder(Palabra, Frec.Rel)))+\n  geom_bar(stat=\"identity\", fill=\"orange\")+\n  theme_classic()+\n  labs(title=\"Frecuencia RELATIVA\")+\n  ylab(\"\")+\n  xlab(\"Frecuencia Relativa\")\n\n# La función grid.arrange permite posicionar varios gráficos lado a lado o uno en cima del otro\ngrid.arrange(p1,p2, ncol=2)\n\n\n\n\nEl próximo paso sería calcular las frecuencias según un grupo o atributo del corpus, como el presidente, por ejemplo. El código abajo utiliza las funciones dfm_group para generar una matriz de frecuencia y dfm_weight para ponderar las palabras y obtener los valores relativos.\n\nCódigo# Crea un objeto dfm\nfg &lt;- dfm_group(fm, groups = quanteda::docvars(cp, \"Presidente\"))\n\n\n# Buscamos las 25 palabras más frecuentes para cada presidente\nft &lt;- topfeatures(fg, n = 25, \n                  groups = quanteda::docvars(fg, \"Presidente\"))\n\n# Genera una frecuencia relativa\nfgw &lt;- dfm_weight(fg, scheme=\"prop\")\n\nftg &lt;- topfeatures(fgw, n = 25, \n                  groups = quanteda::docvars(fg, \"Presidente\"))\n\n# Crea una base de datos a partir de esas informaciones\nnm &lt;- names(ft)\nxx &lt;- data.frame()\nfor(i in 1:length(nm)){\n  xx &lt;- rbind(xx, data.frame(\n                          Presidente=nm[i], \n                          Palabras=names(ft[[i]]), \n                          Freq=as.numeric(ft[[i]]),\n                          Freq.Rel=round(as.numeric(ftg[[i]]),3)))\n}\n\n# Visualiza\nlibrary(htmltools)\n\n# Render a bar chart with a label on the left\nbar_chart &lt;- function(label, width = \"100%\", height = \"1rem\", fill = \"#00bfc4\", background = NULL) {\n  bar &lt;- div(style = list(background = fill, width = width, height = height))\n  chart &lt;- div(style = list(flexGrow = 1, marginLeft = \"0.5rem\", background = background), bar)\n  div(style = list(display = \"flex\", alignItems = \"center\"), label, chart)\n}\n\nlibrary(reactable)\n\nreactable(\n  xx,\n  filterable = T,\n  columns = list(\n    Presidente=colDef(name=\"Presidente\"),\n    Freq = colDef(name = \"Frecuencia\", align = \"left\", cell = function(value) {\n      width &lt;- paste0(value / max(xx$Freq) * 100, \"%\")\n      bar_chart(value, width = width)\n    }),\n    Freq.Rel = colDef(name = \"Frec. Relativa\", align = \"left\", cell = function(value) {\n      width &lt;- paste0(value / max(xx$Freq.Rel) * 100, \"%\")\n      bar_chart(value, width = width, fill=\"red\")\n    })\n  )\n)\n\n\n\n\n\n\nPodemos ver en la tabla resultante que una misma expresión puede tener pesos distintos en los discursos de diferentes presidentes. Por ejemplo, el término política tiene un peso de 0.012 en el discurso de Calvo-Sotelo, pero una incidencia seis veces menor en el de Pedro Sánchez. Algo parecido sucede con la palabra señorías, mucho más común en los documentos de Sánchez si comparados con los de Felipe González.\n\n2.3.2 Raíces (stemming)\nNo obstante, muchas de las palabras que aparecen en la tabla comparten una misma raíz como, por ejemplo, España, españoles, españolas o económico, económica o económicas. Contarlas de forma separada, en realidad, puede fragmentar o ocultar un patrón o un tema más relevante bajo un sinfín de pequeñas variantes de un mismo concepto.\nEn esos casos, una técnica muy útil es la conversión de las palabras a sus raíces (stemming). Este procedimiento sencillo permite justamente evitar que matices entre términos impidan la identificación de un patrón claro dentro del corpus o en algunos de sus textos componentes.\nLa función dfm_wordstem extrae la raiz de los términos de una matriz de frecuencia. Su empleo es muy sencillo, sin embargo, se debe establecer la lengua adecuada de los textos del corpus para que la transformación funcione. La función establece el inglés por defecto. En nuestro ejemplo, definiremos el parámetro language como “es” para definir que se trata de español.\n\nCódigo# Convierte las palabras a sus raices\nfw &lt;- dfm_wordstem(fm, language = \"es\")\n\n# Buscamos las 25 palabras más frecuentes\ntopfeatures(fw, n = 25)\n\n    polit   gobiern     señor     españ    econom    social   español       deb \n      769       724       636       590       565       497       491       444 \n     pais    public       hac     mejor      nuev     comun      part desarroll \n      426       402       401       356       355       344       339       308 \n     pued     emple    socied  ciudadan     mayor    reform   autonom    sistem \n      300       289       288       285       285       280       277       276 \n    objet \n      264 \n\n\nLos resultados reducen la variedad, político, política, políticas se transforman en polit. Gobierno, gobierna, gobiernan en gobiern. Sin embargo, los resultados pueden mejorar. España está de un lado como españ, mientras que español, españoles y españolas se reducen a español. Sin embargo, el investigador siempre puede utilizar esos resultados como punto de partida y, en seguida, agregar o corregir lo que crea necesario.\n\n2.3.3 N-gramas\nEn varias ocasiones conviene explorar la combinación de palabras en búsqueda de expresiones comunes o recurrente. Algunos ejemplos claros en la política son “seguridad social”, “fuerzas armadas”, “políticas públicas”, “seguridad ciudadana”, “partido político”, entre otras. Para ello, utilizamos n-gramas, que son secuencias de n-palabras seguidas. Se llaman así porque pueden ser dos (bigramas), tres (trigramas) o más términos sucesivos.\nEn R, se trata de fransformar los tokens en n-gramas utilizando la función tokens_ngrams y, a continuación, calcular las frecuencias:\n\nCódigo# Convierte los tokens en bigramas\ntn &lt;- tokens_ngrams(tk, n=2)\n\n# Crea una matriz de frecuencia\nfk &lt;- dfm(tn)\n\n# Extrae los 25 mas comunes\ntopfeatures(fk, 25)\n\n    comunidades_autónomas             unión_europea         política_exterior \n                       94                        67                        60 \n          señoras_señores               punto_vista         señores_diputados \n                       51                        51                        49 \n        sociedad_española        política_económica               cuatro_años \n                       48                        45                        43 \n         seguridad_social             próximos_años                  cada_vez \n                       41                        40                        40 \n         confianza_cámara           creación_empleo          señor_presidente \n                       40                        40                        39 \n          acción_gobierno         fuerzas_políticas administraciones_públicas \n                       38                        38                        36 \n             primer_lugar                  debe_ser     formación_profesional \n                       35                        34                        34 \n               si_obtengo              últimos_años              mismo_tiempo \n                       34                        34                        31 \n        economía_española \n                       30 \n\n\nVarios bigramas interesantes saltan a la vista: comunidades autónomas, unión europea, política exterior, política económica, seguridad social, creación empleo, entre otros. También aparecen fórmulas retóricas como señoras señores, si obtengo, confianza cámara, por ejemplo.\nPodemos repetir el mismo procedimiento, pero ahora utilizando tres palabras en lugar de dos para ver qué resultados obtenemos. Este juego de ir subiendo el número de palabras en la expresión puede seguir indefinidamente hasta que no aporte ningún dato nuevo o interesante.\n\nCódigo# Convierte los tokens en trigramas\ntn &lt;- tokens_ngrams(tk, n=3)\n\n# Crea una matriz de frecuencia\nfk &lt;- dfm(tn)\n\n# Extrae los 25 mas comunes\ntopfeatures(fk, 25)\n\n          señoras_señores_diputados                si_obtengo_confianza \n                                 48                                  23 \n           obtengo_confianza_cámara                próximos_cuatro_años \n                                 18                                  17 \n            producto_interior_bruto         comunidad_económica_europea \n                                 17                                  15 \n          sistema_público_pensiones            señor_presidente_señoras \n                                 13                                  12 \n         presidente_señoras_señores    legislatura_discurso_investidura \n                                 12                                  11 \n          solicito_confianza_cámara                 últimos_cuatro_años \n                                 11                                  11 \n                   idea_españa_país            artículo_99_constitución \n                                 11                                  10 \n            todas_fuerzas_políticas           fuerzas_cuerpos_seguridad \n                                  9                                   9 \n           creación_puestos_trabajo              si_obtengo_investidura \n                                  9                                   9 \n           españa_necesita_gobierno            sistema_seguridad_social \n                                  9                                   8 \n                gobierno_si_obtengo              sistema_nacional_salud \n                                  8                                   8 \n         señora_presidenta_señorías comunidades_autónomas_ayuntamientos \n                                  8                                   7 \n    todas_administraciones_públicas \n                                  7 \n\n\nLa obtención de la confianza del parlamento aparece en más de una vez. El artículo 99 de la Constitución española (que rije el proceso de voto de confianza en el Presidente) resulta la novedad más clara en ese apartado. No obstante, otros términos relacionados a las políticas públicas -como producto interior bruto, sistema público pensiones, fuerzas cuerpos seguridad- o a la organización administrativa del Estado -comunidades autónomas ayuntamientos o todas administraciones públicas- también se destacan.\nSi llegamos a aumentar el n a 5, por ejemplo, aparece el I+D+I. En resumen, se trata de un recurso exploratorio bastante interesante para determinar qué expresiones compuestas o frases aparecen de forma repetitiva en los textos y que puedan incitar nuevas perspectivas sobre el contenido de los mismos.\n\n2.3.4 TF-IDF\nOtro método de selección de términos relevantes es llamado Term Frequency-Inverse Document Frequency (TF-IDF). La fórmula es intuitiva y premia aquellos casos que aparecen con mucha frecuencia, pero en relativamente pocos documentos, al mismo tiempo que penaliza los que están por todas partes:\n\\[tf/idf = freq_{td} * log(\\frac{D}{d_t}) \\]\nDonde:\nfreqtd es la frecuencia absoluta (o relativa) del término t en el cada documento d.\nD corresponde al número total de documentos.\ndt representa el número de documentos que contienen el término t.\nImaginemos un corpus con 10 documentos y dos palabras “la” y “pobreza”, ambas con una frecuencia de 20. La única diferencia es que “la” aparece en todos los 10 documentos con una frecuencia de 2 en cada uno, mientras que “pobreza” se menciona en solamente dos textos, uno 14 veces y otro 6. Al calcular el tf-idf para cada una, tenemos los siguientes resultados:\n\nPara cada uno de los documentos de “la”: 2*log(10/10) = 0\nPara el primer documento de “pobreza”: 14*log(10/2) = 22,5\nPara el segundo documento de “pobreza”: 6*log(10/2) = 9,7\n\nAl final, se observa que el peso de “la” se anula completamente (tf-idf = 0) tanto por la dispersión de la frecuencia total como por su aparición en muchos documentos. Lo inverso ocurre con “pobreza”, que tiene su frecuencia concentrada en dos textos y con mayor preponderancia en uno en concreto (un tf-idf total de 22,5 + 9,7 = 32,2).\nEn líneas generales permite identificar aquellas palabras que aparecen mucho, pero que no tanto para reducir su poder informativo. Por ejemplo, “la”, “de”, “el” o “ser”, “hacer” aparecen un número elevado de veces. Esta medida permite ponderar su peso por un factor que penaliza el hecho de aparezcan mucho en todos los documentos. El resultado son indicadores más elevados para conceptos que se destacan sin ser preponderantes o muy comunes en todos los elementos del corpus.\n\nCódigo# Convierte las palabras a sus raices\nfw &lt;- dfm_tfidf(fm)\n\n# Buscamos las 25 palabras más frecuentes\ntopfeatures(fw, n = 25)\n\n     digital consiguiente  progresista    coalición        vista      ustedes \n    18.94303     18.70318     17.79497     16.69924     16.65308     15.91760 \n       vamos           ss        euros       género            `      ejemplo \n    15.89324     15.26788     14.72378     14.31364     14.11310     13.92790 \n  presidenta       señora   transición        pacto  lógicamente  comunitaria \n    13.77675     13.20272     12.73408     11.79811     11.48063     11.48063 \n   ecológica         acta      pobreza         idea   revolución         2030 \n    11.37580     11.18352     10.97379     10.92005     10.90659     10.50074 \n        digo \n    10.48455 \n\n\nVemos que otros términos aparecen: digital, progresista, coalición, euros, género, transición, pacto, comunitaria, ecológica, pobreza y 2030. Tales términos sugieren contenido programático de la política y despiertan mayor interés que el lenguaje más formal que hemos visto hasta ahora. Por otra parte, verbos y expresiones muy peculiares de cada presidente, como el vamos de Pedro Sánchez o el consiguiente de Felipe González, saltan a la vista. Tales ejemplos evidencían cómo la medida tf-idf puede ayudar a singularizar el discurso de un presidente o de un partido político tanto por el contenido político que por las fórmulas lingüísticas empleadas para dirigirse a los miembros del Poder Legislativo. Además, como en los ejemplos anteriores, se pueden detallar los resultados por grupo (Presidente, partido, ideología, entre otras variables de contexto disponibles).\n\n2.3.5 Keyness\nEl keyness es otro método que compara la distribución desigual de términos entre textos. A partir de un texto de referencia, utiliza métodos estadísticos como el chi-cuadrado o la likelihood ratio para determinar cuáles palabras se acercan más a un documento y las que menos. A partir de esas informaciones podemos encontrar elementos útiles para caracterizar un discurso concreto.\nla función textstat_keyness del paquete quanteda.textstats permite calcular el keyness de los términos de un corpus con relación a un documento de referencia concreto. Utilicemos, por ejemplo, los discursos de Pedro Sánchez como referencia:\n\nCódigo# Nueva matriz de fecuencia\npfm &lt;- dfm(tk)\n\n# Atribuimos el nombre del presidente como grupo\npfm &lt;- dfm_group(pfm, groups = quanteda::docvars(pfm, \"Presidente\"))\n\n# Calcula el keyness\nkn &lt;- textstat_keyness(pfm, target = \"Pedro Sánchez\")\n\n# Visualiza los resultados en una tabla\nreactable(kn, \n          columns = list(\n                    chi2=colDef(\n                            format=colFormat(\n                            digits=2)),\n                    p=colDef(\n                            format=colFormat(\n                            digits=2))))\n\n\n\n\n\n\nVemos que las palabras que más se asocian al discurso de Sánchez son vamos, señorías, progresista, digital, avanzar y género. Las que menos son política, económica, esfuerzo, ciudadanos, proceso, exterior y cooperación. Podemos también compararlas visualmente utilizando la función textplot_keyness del paquete quanteda.textplots. En el gráfico abajo, se seleccionan las 20 palabras que más y menos caracterizan los textos de Pedro Sánchez.\n\nCódigolibrary(quanteda.textplots)\n\ntextplot_keyness(kn, color = c(\"red3\",\"blue\"))\n\n\n\n\nComo se trata de un gráfico basado en la arquitectura ggplot2, se pueden añadir elementos como títulos, temas, nuevos colores y otros elementos visuales.\n\n2.3.6 Ratio de probabilidades\n\nCódigocp &lt;- corpus(spa.inaugural)\n\nci &lt;- corpus_group(cp, groups = President)\n\nplotLogOddsRatio(corpus = ci, \n                  ref.cat = \"Sánchez\")"
  },
  {
    "objectID": "01_explora_inductivo.html#asociación-entre-palabras",
    "href": "01_explora_inductivo.html#asociación-entre-palabras",
    "title": "\n2  Primeros pasos\n",
    "section": "\n2.4 Asociación entre palabras",
    "text": "2.4 Asociación entre palabras\n\n2.4.1 Co-ocurrencias\nEl primer método de análisis de la asociación entre palabras explora la cantidad de veces en que dos palabras aparecen juntas en un corpus. Este fenómeno se denomina co-ocurrencia. Se calcula a partir de la función fcm de quanteda que genera una matriz de co-ocurrencia. Básicamente, se trata de una matriz NxN, donde N corresponde al número de palabras en el corpus.\n\nCódigo# Crea una matriz de co-ocurrencia\nfc &lt;- fcm(tk)\n\n# Selecciona las 50 co-ocurrencias mas frecuentes\nfeat &lt;- names(topfeatures(fc, 50))\n\nfc &lt;- fcm_select(fc, pattern = feat) \n\n# carga el paquete\nlibrary(quanteda.textplots)\n\n# genera la red\nlibrary(ggplot2)\nset.seed(pi)\ntextplot_network(fc, \n                 edge_color = \"red\", \n                 edge_alpha = 0.05, omit_isolated = T)\n\n\n\n\nEl código más arriba calcula la matriz de co-ocurrencia para el corpus de los discursos de investidura de los presidentes de gobierno de España y selecciona los 50 pares de términos más frecuentes. Con esos datos, genera un sociograma que representa las asociaciones más comunes entre palabras. El grosor de los vínculos revela la intensidad de su asociación y la centralidad de los nodos su peso o importancia en el conjunto de elementos seleccionados.\nComo podemos observar, España aparece en el centro, seguida por señorías, ley, vamos, partido y sistema. La red posee un núcleo más denso de palabras interconectadas entre sí y otro conjunto de términos periféricos, con escasos vínculos con este centro de la red.\n\n2.4.2 Co-localizaciones\nUn método adicional para el análisis de los vínculos entre términos es la co-localización. A diferencia de las co-ocurrencias, que se basan exclusivamente en las frecuencias, la función textstat_collocations del paquete quanteda.textstats utiliza un modelo log-linear para comparar la incidencia de un grupo de palabras y definir su grado de asociación. El coeficiente lambda (\\(\\lambda\\)) representa dicha estimación.\nLa co-localización define el grado de asociación de otra palabra cerca. Así se puede precedir qué palabra viene después a partir del conjunto que viene antes.\n\nCódigo# Genera una lista de 2 palabras que aparecen en secuencia\ncc &lt;- textstat_collocations(tk, size = 2)\n\nreactable(cc, \n          resizable=T, \n          rownames = F, \n          columns = list(\n                        lambda=colDef(format=colFormat(digits=2)),\n                        z=colDef(format=colFormat(digits = 2))))\n\n\n\n\n\n\n\n2.4.3 Correlación\nLa correlación corresponde a un método clásico de medir la asociación entre dos variables. Cuando se trata de la correlación entre términos podemos utilizar diferentes algoritmos. Silge y Robinson (2017), por ejemplo, emplean el coeficiente phi (\\(\\phi\\)) de Yule, un método de asociación a partir de la coincidencia binaria (1/0, Sí/No) entre palabras en un mismo documento. Como el rho (\\(\\rho\\)) de Pearson, posee un intervalo entre -1 y 1 y se interpreta del mismo modo.\nEsta medida puede resultar útil para textos cortos, como tweets o un corpus organizado según sentencias. En esos casos, importa menos la cantidad de las palabras que el hecho de que ambas aparezcan en un mismo documento. Se valora la coincidencia de dos conceptos o ideas y tiene poco sentido evaluar su intensidad. La probabilidad de encontrar una palabra con frecuencia superior a 1 en una misma frase u oración es pequeña.\nNo obstante, ese razonamiento también revela la principal limitación del coeficiente phi. Al tratarse de un test binario, no lleva en cuenta diferencias cuantitativas que pueden observarse en documentos más extensos como libros, capítulos, entrevistas, leyes, manifiestos o discursos. En estos casos, se requieren métodos más precisos que, además de la presencia o ausencia de los términos, ponderen la intensidad de asociación según su frecuencia o rango.\nLa correlación de orden de rango (rank-order correlation) o rho (\\(\\rho\\)) de Spearman resulta más indicada para esos casos. Se trata de una medida que ordena los documentos según el rango de frecuencia de cada palabra en concreto y compara el grado de similitud o diferencia entre los rangos. La fórmula es la siguiente:\n\\[S\\rho = 1-\\frac{6 \\sum R(X_i)-R(Yi)}{n(n^2-1)}\\]\nDonde:\n\nR(Xi) indica el rango (ranking) de Xi en los valores de X.\nR(Yi) indica el rango (ranking) de Yi en los valores de Y.\nn corresponde al número de observaciones. En nuestro caso, indica el número de documentos en el corpus.\n\n\nImportantePuesto que la distribución de las frecuencias de palabras no es normal, no se recomienda el empleo del rho (\\(\\rho\\)) de Pearson para avaliar su asociación. Tampoco resulta indicable el cálculo de la correlación para un corpus con un número muy reducido de documentos (menos de 10, por ejemplo). En tales escenarios, quizás sería mejor reestructurar el corpus según unidades menores -como sentencias o párrafos, por ejemplo- y emplear el phi (\\(\\phi\\)) como alternativa.\n\n\nEl código abajo crea una lista con nodos correspondientes a las palabras del corpus y vínculos que expresan la intensidad de su asociación y un sociograma representando la asociación entre las palabras. Por defecto, el método empleado es el rho de Spearman.\n\nCódigo# Carga el paquete tenet\nlibrary(tenet)\n\n# Crea una lista de correlaciones\nll &lt;- corTerms(cp, \n               min.freq = 100, \n               n.terms = 100)\n\n# Genera el sociograma\ncorNet(ll)\n\n\n\n\nCuanto más grandes los puntos, mayor la frecuencia de la palabra en el corpus. El grosor del vínculo revela la intensidad de asociación y el color su dirección. En el ejemplo arriba, correlaciones negativas se representan en rojo y positivas en azul. De ese modo, vemos que países y compromiso se relacionan de forma negativa. Fuerzas y social presentan una correlación positiva."
  },
  {
    "objectID": "01_explora_inductivo.html#consideraciones-finales",
    "href": "01_explora_inductivo.html#consideraciones-finales",
    "title": "\n2  Primeros pasos\n",
    "section": "\n2.5 Consideraciones finales",
    "text": "2.5 Consideraciones finales\nEn este documento hemos visto cómo abrir los textos en R y trabajar con distintas técnicas de análisis exploratorio. Nos hemos concentrado en métodos inductivos, sin una lectura anterior y profunda que orientara el análisis. Los ejemplos se han concentrado fundamentalmente en entender qué términos ocurren con mayor frecuencia y cómo se asocian entre ellos.\nEste tipo de análisis dista mucho de ser mínimamente aceptable dentro de una perspectiva cualitativista pura. Contar palabras constituye una aproximación somera al análisis de textos. No obstante, no tiene el propósito de reemplazar nada. Su utilidad reside en suministrar recursos y una primera aproximación a técnicas más profundas y sofisticadas. Los términos encontrados aquí sirven para la creación de diccionarios y la codificación temática. También ayudan a desvelar patrones no completamente observables desde una perspectiva cualitativa. Además, su poder reside en permitir extraer patrones de amplios volúmenes de texto, imposibles de leer uno a uno.\nEn la próxima sesión utilizaremos técnicas deductivas y otros métodos exploratorios para profundizar en el conocimiento de los textos. Trataremos de la codificación temática, la selección de textos según términos o atributos para un análisis más detallado y la creación de diccionarios como base para tareas de clasificación y descubierta."
  },
  {
    "objectID": "01_explora_inductivo.html#ejercicios",
    "href": "01_explora_inductivo.html#ejercicios",
    "title": "\n2  Primeros pasos\n",
    "section": "\n2.6 Ejercicios",
    "text": "2.6 Ejercicios\nEjercicio 1. Utilice el data.frame cis.corrupt del paquete tenet para crear un nuevo corpus. Realice el análisis del nuevo corpus utilizando la función summary. Guarde el resultado de summary en un data.frame llamado d.\n\nSolución# Carga los paquetes tenet y quanteda\nlibrary(tenet)\nlibrary(quanteda)\n\n# Convierte cis.corrupt en un corpus\ncx &lt;- corpus(cis.corrupt)\n\n# Resume los resultados\nd &lt;- summary(cx, n = nrow(cis.corrupt))\n\n\nEjercicio 2. Añada una variable de documentación (docvars) al corpus recién creado con la densidad o la diversidad léxica de cada texto dividiendo el número de types por tokens y multiplicando por 100. Formalmente, este término se denomina Type-Token Ratio (TTR). También añada otras dos variables a la documentación del corpus: (a) el número de palabras por sentencia y (b) el número de types por sentencia.\n\nSolución# Calcula el TTR\nd$TTR &lt;- round((d$Types/d$Tokens)*100,1)\n\n# Calcula el número de palabras por frase\nd$tokens_sentence &lt;- round(d$Tokens/d$Sentences,1)\n\n# Calcula el número de tipos por frase\nd$types_sentence &lt;- round(d$Types/d$Sentences,1)\n\n# Añade las tres variables como documentación del corpus\ndocvars(cx,\"TTR\") &lt;- d$TTR\ndocvars(cx,\"tokens_sentence\") &lt;- d$tokens_sentence\ndocvars(cx,\"types_sentence\") &lt;- d$types_sentence\n\n\nEjercicio 3. Divida el corpus en tokens bajo la forma de palabras. Excluyas puntuación, símbolos y palabras vacías (stop words). Cree un nuevo objeto con bi-gramas en lugar de solo palabras como tokens. Finalmente, genere una nueva lista de términos, pero ahora con solamente las raíces.\n\nSolución# Crea los tokens removiento puntuación y símbolos\ntk &lt;- tokens(cx,\n             remove_punct = T,\n             remove_symbols = T)\n\n# Elimina los stop words\ntk &lt;- tokens_remove(tk, \n                    stopwords(\"es\"))\n\n# Convierte en bi-gramas\ntb &lt;- tokens_ngrams(tk, 2)\n\n# Convierte en raíces\ntr &lt;- tokens_wordstem(tk, language = \"es\")\n\n\nEjercicio 4. Crea dos matrices de frecuencia: (a) una con las palabras y (b) otra con sus raíces. Selecciona las 30 palabras más frecuentes en cada una de ellas.\n\nSolución# Crea la matriz de frecuencia para las palabras\nfm &lt;- dfm(tk, tolower = T)\n\n# Crea la matriz de frecuencia para las raíces\nfr &lt;- dfm(tr, tolower = T)\n\n# Selecciona las 30 palabras más frecuentes\ntopfeatures(fm, 30)\n\n# Selecciona las 30 raíces más frecuentes\ntopfeatures(fr, 30)\n\n\nEjercicio 5. Crea una matriz de frecuencia para cada grupo demográfico (variable “Grupo.Demografico” en docvars) y selecciona las 10 palabras más comunes para cada uno de ellos.\n\nSolución# Agrupa la matriz por grupo demográfico\nfg &lt;- dfm_group(fm, groups = docvars(cx, \"Grupo.Demografico\"))\n\n# Selecciona los 10 más frecuentes\ntopfeatures(fg, groups = docvars(fg, \"Grupo.Demografico\"))\n\n\nEjercicio 6. Ahora, utilice la matriz agrupada del ejercicio 5, calcule los TF-IDF de cada palabra y seleccione las 20 palabras más importantes para cada grupo.\n\nSolución# Calcula el TF-IDF\nfi &lt;- dfm_tfidf(fg)\n\n# Selecciona las 20 más frecuentes\ntopfeatures(fi,\n            n = 20,\n            groups = docvars(fg, \"Grupo.Demografico\"))\n\n\nEjercicio 7. Utilice la matriz de frecuencia de los grupos demográficos para calcular el keyness del corpus (con la función textstat_keyness) y crea el gráfico utilizando la función texplot_keyness. Utilice como referencia el grupo “Obreros”.\n\nSolución# Calcula el keyness\nkn &lt;- textstat_keyness(fg, target = \"Obreros\")\n\n# Carga el paquete\nlibrary(quanteda.textplots)\n\n# Genera el gráfico\ntextplot_keyness(kn, color = c(\"red3\",\"blue\"))\n\n\nEjercicio 8. Cree una matriz de co-ocurrencia y, luego, genere el sociograma de la asociación de las 25 co-ocurrencias más frecuentes.\n\nSolución# Crea una matriz de co-ocurrencia\nfc &lt;- fcm(tk)\n\n# Selecciona las 25 co-ocurrencias mas frecuentes\nfeat &lt;- names(topfeatures(fc, 25))\n\nfc &lt;- fcm_select(fc, pattern = feat) \n\n# carga el paquete\nlibrary(quanteda.textplots)\n\n# genera la red\nlibrary(ggplot2)\nset.seed(pi)\ntextplot_network(fc, \n                 edge_color = \"blue\", \n                 edge_alpha = 0.05)\n\n\nEjercicio 9. Crea el gráfico de correlaciones para el corpus a partir de 100 palabras con frecuencia superior a 50 en el corpus.\n\nSolución# Carga el paquete tenet\nlibrary(tenet)\n\n# Crea una lista de correlaciones\nll &lt;- corTerms(cx,\n               min.freq = 50, \n               n.terms = 100)\n\n# Genera el sociograma\ncorNet(ll)"
  },
  {
    "objectID": "01_explora_inductivo.html#lecturas-adicionales",
    "href": "01_explora_inductivo.html#lecturas-adicionales",
    "title": "\n2  Primeros pasos\n",
    "section": "\n2.7 Lecturas adicionales",
    "text": "2.7 Lecturas adicionales\n\n\nIgnatow G, Mihalcea R (2018). “The Philosophy and Logic of Text Mining.” In An Introduction to Text Mining: Research Design, Data Collection, and Analysis, chapter 4. SAGE, Los Angeles.\n\n\nEste breve capítulo del manual de Ignatow y Mihalcea trata de las distintas perspectivas de exploración de textos en las ciencias sociales. Trata de las principales corrientes epistemológicas, las dos culturas (“cuantitativa” y “cualitativa”) y describe las características y principales diferencias entre las lógicas deductiva, inductiva y abductiva.\n\n\nBenoit K (2020). “Text as data: An overview.” In The SAGE Handbook of Research Methods in Political Science and International Relations, chapter 26, 461. SAGE, Thousand Oaks. Publisher: SAGE Publishing Ltd Thousand Oaks.\n\n\nSe trata de otro excelente texto para clasificar las perspectivas (ahora desde la ciencia política) sobre el análisis de texto. Propone una aclaración conceptual importante: un texto considerado como un conjunto de datos no puede ser confundido con un texto en su integridad. La conversión a datos supone transformaciones, abstracción y cierto grado de reducción. Sobre todo, deja claro qué NO debemos esperar de esa perspectiva de análisis para evitar frustraciones o inferencias inadecuadas.\n\n\nGrimmer J, Roberts ME, Stewart BM (2022). “Bag of Words.” In Text as Data: A New Framework for Machine Learning and the Social Sciences, chapter 5, 94-111. Princeton University Press, New Haven.\n\n\nEl texto de Grimmer, Roberts y Stewart representa una buena introducción a los principales conceptos tratados en esta sección. Introducen el análisis de tipo “saco de palabras” (bag of words) y explican los procesos de su implementación como la tokenización, la remoción de palabras vacías (stop words), la reducción a lemas o raíces y la creación de matrices de frecuencias.\n\n\nSilge J, Robinson D (2017). “‘Analyzing Word & Document Frequency - tf-idf’ y ‘Relationships between words: n-grams and correlations’.” In Text Mining with R: A Tidy Approach, chapter 4 and 5. O’Reilly Media, London. https://www.tidytextmining.com/.\n\n\nFinalmente, los dos capítulos de Silge y Robinson explican más detenidamente y de forma práctica varios análisis realizados en esta sección. Frecuencia de palabras, TF-IDF, la ley de Zipf (muy importante para comprender las frecuencias de palabras), así como los N-gramas y las correlaciones entre palabras.\n\n\n\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with r: A Tidy Approach. London: O’Reilly Media. https://www.tidytextmining.com/."
  },
  {
    "objectID": "02_explora_deductivo.html#introducción",
    "href": "02_explora_deductivo.html#introducción",
    "title": "\n3  Codificación temática\n",
    "section": "\n3.1 Introducción",
    "text": "3.1 Introducción\n¿Cuál es la posición de los partidos con relación a la reducción de impuestos? ¿Cuáles mencionan la pobreza o el aumento de la desigualdad en sus manifiestos? ¿En qué textos aparece el tema de la inmigración como amenaza a la integridad social o política del país? ¿Cuáles diputados intervienen durante los plenos en favor de políticas que reduzcan las emisiones y frenen el cambio climático? Cada una de esas preguntas encierra un conjunto de opciones claras en términos empíricos, teóricos y metodológicos. Primero, se basan en el contenido textual como fuente de información empírica. Segundo, establecen la comparación como método, buscando diferencias significativas según partido o ideología. Finalmente, los partidos o diputados se dividen en grupos que hipotéticamente se antagonizan ante ciertas políticas consideradas clave. Existe un comportamiento esperado que se puede o no confirmar a partir de análisis del material seleccionado.\nA diferencia de lo que se hizo en el análisis inductivo, ya no se trata de ver aquí qué palabras o términos aparecen en un conjunto de documentos, sino de buscar cómo temas concretos se manifiestan, por quiénes, dónde y en qué contexto. El análisis exploratorio deductivo nos permite evaluar la prevalencia de un tema en los textos, verificar si su distribución resulta uniforme o se concentra de acuerdo con la ideología, el partido político o un momento histórico concreto.\nEn esta parte del trabajo revisaremos los instrumentos y estrategias disponibles para el desarrollo de temas. Examinaremos diferentes herramientas para determinar el contexto en el que se inscriben, así como determinaremos la prevalencia de distintas categorías analíticas. Haremos especial hincapié en el concepto de diccionario o léxico como el resultado de un proceso de codificación temática y construcción teórica a partir del análisis abductivo resultante de la consulta e iteración constante entre texto (como material empírico fundamental), teoría e interpretación (Thompson 2022; B. L. Kennedy and Thornberg 2018).\nEl texto siguiente se divide en tres sesiones. El segundo apartado se dedica a las técnicas de búsqueda de palabras en los textos y que resultan muy útiles para situar las ideas en su contexto. La tercera parte trata de la codificación temática a partir de diccionarios. Cómo la construcción y refinamiento de léxicos puede resultar de un proceso iteractivo que da lugar al desarrollo de nuevas categorías analíticas. Finalmente, el cuarto apartado explora los temas para identificar patrones, el grado de asociación entre ellos y su distribución según distintas categorías analíticas (como partidos o ideología, por ejemplo)."
  },
  {
    "objectID": "02_explora_deductivo.html#búsqueda-de-palabras",
    "href": "02_explora_deductivo.html#búsqueda-de-palabras",
    "title": "\n3  Codificación temática\n",
    "section": "\n3.2 Búsqueda de palabras",
    "text": "3.2 Búsqueda de palabras\nEn el apartado sobre el método inductivo aprendimos a contar palabras. No obstante, en muchas ocasiones, el significado de un término puede variar según el contexto. Según la Real Academia Española (RAE), la palabra “estrella” puede significar tanto un cuerpo celeste como una persona que sobresale en su profesión, por ejemplo. En los estudios políticos palabras como pueblo, democracia o libertad exigen que el analista establezca siempre el contenido concreto asociado a tales expresiones abstractas. El propósito de esta sección consiste en introducir algunas herramientas que permitan determinar el significado de una expresión de forma clara y con menos ambigüedad.\n\n3.2.1 Keyword in Context (Kwic)\n\nUna forma sencilla de contextualizar términos consiste en visualizarlos directamente en los pasajes del texto en que aparecen. El método kwic (keyword in context) extrae de un texto o corpus todos los trechos en los que aparece una palabra y los muestra dentro de un contexto o ventana que puede ser compuesta por una o más expresiones antecedentes y posteriores. En el ejemplo abajo, buscamos la palabra “libertad” en los discursos de investidura de los presidentes españoles, con una ventana de 5 palabras alrededor del término.\n\nCódigo# Carga el paquete tenet\nlibrary(tenet)\n\n# Crea un corpus (discursos inaugurales Espana)\ncp &lt;- corpus(spa.inaugural)\n\n# Crea un data.frame a partir de\n# la funcion Keyword in Context de\n# Quanteda \nd &lt;- kwic(x = tokens(cp),\n          pattern= \"libertad\",\n          window = 5)\n\n# Visualiza los resultados\n reactable::reactable(d,\n                     resizable = T, \n                     wrap = F)\n\n\n\n\n\n\n\nComo podemos observar, el uso del término libertad varía significativamente según el momento y el presidente en cuestión. Adolfo Suárez lo utiliza en un contexto de transición hacia la democracia, como superación de una etapa anterior autoritaria. Por esa razón, la palabra aparece junto a derechos, instituciones y democracia. Felipe González la utiliza junto a las ideas de igualdad y solidaridad, mientras que Aznar las asocia a la expresión, enseñanza y seguridad. Zapatero introduce el concepto de libertad sexual. Rajoy la asocia a prosperidad e igualdad, mientras que Pedro Sánchez se centra en dos ejes: valores postmateriales (sexual, aborto, eutanasia) y territorial (autonomía de las comunidades autónomas).\nPor otra parte, si hacemos un ejercicio y utilizamos el término “empleo”, se puede averiguar que este se refiere casi exclusivamente al mundo laboral y políticas activas de acceso o creación de puestos de trabajo. Solo en dos ocasiones específicas se trata del verbo “emplear” con el significado “utilizar”, como las referencias “emplear una política monetaria” o el “empleo de los caudales públicos”, ambas en el primer discurso de investidura de Felipe González. Por lo tanto, al examinar los resultados, vemos que esos dos casos constituyen una excepción al significado principal de empleo a que hacen referencia todos los discursos.\n\n3.2.2 Árbol de palabras\nEl árbol de palabras (wordtree) nos brinda una visión semejante al kwic con una diferencia fundamental: cada palabra que compone la frase se dimensiona de acuerdo con la frecuencia con que aparecen en los textos. Este recurso resulta útil para discriminar los usos más comunes de los términos en sus contextos predominantes. De acuerdo con Wattenberg y Viégas (2008, 2–3), corresponde a una alternativa gráfica y exploratoria de visualización de los kwic. Se trata, además, de un recurso interactivo que permite al usuario jugar con los contextos, direccionando su mirada hacia frases concretas o subiendo a patrones más generales. Posee tres características distintivas. Primero, facilita la identificación de repeticiones de palabras. Segundo, tiene una estructura de árbol claramente identificable. Finalmente, facilita la exploración del contexto.\nPor lo tanto, representa una herramienta que sirve tanto para la exploración de patrones en los textos durante una primera fase exploratoria de un estudio como de instrumento de comunicación de patrones o temas recurrentes encontrados en los datos. Su interactividad invita tanto a la descubierta como a una mayor atención a los argumentos que se desean transmitir.\nEl código abajo utiliza la función wordtree del paquete tenet para crear el árbol de palabras alrededor del término libertad. Como podemos ver, el resultado es muy semejante al producido por el kwic. No obstante, ahora nuestra atención se ve atraída por las palabras de mayor tamaño. Las rutas más comunes se evidencian, como es el caso de “de la libertad de expresión”, por ejemplo.\n\nCódigo# Crea un arbol de palabras en tenet\nwordtree(corpus = cp,\n         keyword = \"libertad\",\n         height = 800)\n\n\n\n\n\n\n  \n  \n                   \n                   \n\n\nPor otra parte, si filtramos el corpus para que incluya solamente textos de un presidente o partido político, podemos identificar los usos específicos que hacen de los términos y, así, trazar variantes y revelar patrones útiles teóricamente. Seguramente veremos diferencias sustantivas entre Adolfo Suárez y Pedro Sánchez, como hemos podido contrastar en el apartado anterior. Además, será posible identificar de forma más sencilla las expresiones más recurrentes o típicas de cada uno. En el caso de José María Aznar, por ejemplo, la libertad se asocia de forma muy evidente al progreso económico.\n\n3.2.3 Prevalencia en el tiempo\nGoogle Libros, consiste en uno de los servicios más interesantes de Google. Se trata de un proyecto ambicioso que ha llevado a cabo la digitalización y el pre-procesamiento lingüístico masivo de un número enorme de libros en diferentes lenguas. Además de disponibilizar muchos documentos en línea (en su gran mayoría de dominio público), la aplicación realiza el cálculo de una serie de métricas entre las cuales se encuentra la densidad de palabras o expresiones (los n-gramas como ya hemos visto en el capítulo anterior). De ahí surgió el Google N-Gram Viewer, una aplicación que permite mapear la evolución de un término o palabra en una lengua en largos períodos de tiempo.\nEl paquete ngramr en R permite acceder a los datos de Google N-Gram Viewer y obtener las frecuencias relativas. La función ngrami, por ejemplo, busca una o más palabras sin considerar si está en mayúsculas o minúsculas y retorna un data.frame con los resultados. Estos datos se pueden emplear luego para generar un gráfico.\nLa gran ventaja de N-Gram Viewer está en su carácter histórico. Revela cómo una palabra o expresión evoluciona en una misma lengua a lo largo de amplios períodos de tiempo. El código abajo realiza la búsqueda de tres infraestructuras clave: los telégrafos, los ferrocarriles y el Internet. A partir del examen de su prevalencia en los libros en español de 1800 a 2019, podemos observar sus distintos ciclos temporales:\n\nCódigo# Carga el paquete ngramr\nlibrary(ngramr)\n\n# Busca los términos (case-insensitive)\nnm &lt;- ngrami(\n          c(\"telégrafo\",\n            \"ferrocarril\",\n            \"internet\"), \n          corpus = \"es-2019\")\n\n# Genera un gráfico con los resultados\nlibrary(ggplot2)\n\np &lt;- ggplot(nm, \n            aes(\n              x=Year, \n              y=Frequency, \n              color=Phrase))+\n  geom_line()+\n  theme_classic()+\n  scale_color_discrete(name=\"Término\")+\n  labs(\n    title=\"Google N-Gramas (1800-2019)\", \n    subtitle = \"Densidad de palabras en libros \n                en español de 1800 a 2019.\")+\n  xlab(\"Año\")+\n  ylab(\"Frecuencia relativa\")\n\np\n\n\n\n\nLas curvas no pueden ser más ilustrativas. Los telégrafos presentan una ascensión entre 1850 y 1900 para, luego, entrar en declive. Los ferrocarriles se han comportado de forma similar, aunque aparezcan de forma mucho más frecuente en los libros del período. Internet, una infraestructura mucho más reciente, se ve reflejada a partir de una curva exponencial a partir de los 2000.\nNo resulta nada difícil replicar la misma lógica en textos que posean alguna secuencia temporal o lógica. Para reproducir el análisis realizado por el NGram Viewer con un corpus no previamente preparado necesitamos llevar a cabo dos tareas centrales. La primera consiste en crear una función que busque algunos términos o expresiones en el corpus y, luego, calcule su densidad. En el segundo paso se organiza la base de datos de modo que cada unidad textual refleje una unidad de tiempo. Por ejemplo, podemos aglutinar los documentos por día, mes, año o cualquier medida que convenga al investigador. Lo importante es que las unidades sean homogéneas y comparables entre sí.\nUna de las bases de datos de ejemplo incluidas en el paquete tenet se conforma por todas las intervenciones parlamentarias durante la XIV Legislatura del Congreso de Diputados de España, vigente entre diciembre de 2019 y junio de 2023. La base original contiene la intervención de cada diputado y la fecha en la que se ha realizado. Por esa razón, resulta relativamente sencillo agregar los textos por fecha. No se recomienda el uso de días, pues los intervalos entre sesiones no resulta uniforme. Algunas semanas contienen dos o más sesiones, mientras que otras apenas se reúnen. Por esa razón, emplearemos en nuestro ejemplo el mes como unidad de comparación de tiempo.\nEl panel abajo contiene la descripción detallada de cada paso:\n\n\nPaso 1: Función\nPaso 2: Tabla\nResultado: Gráfico\n\n\n\nEmpezaremos por crear una función llamada countNgram con cuatro parámetros o argumentos. El primero, keywords, informa cuáles son los términos que deseamos buscar. El segundo, corpus, se refiere al conjunto de textos que serán utilizados como base para el recuento. En tercer lugar, time, suministra la referencia secuencial de tiempo (u otra) para cada uno de los textos contenidos en corpus (en nuestro caso los meses). Por esa razón, el número de elementos de esos dos últimos argumentos debe ser siempre igual. Finalmente, rel.freq establece si se calculará la frecuencia absoluta o la relativa. Esta última se calcula por defecto a menos que se defina rel.freq=FALSE.\n\nCódigo# Función countNgram, que cuenta el \n# número de veces un conjunto de palabras\n# aparece en un corpus.\ncountNgram &lt;- function(keywords, \n                       corpus=NULL, \n                       time=NULL, \n                       rel.freq=TRUE){\n  \n  # Cuenta todos los términos de la lista\n  tt &lt;- outer(corpus, \n              keywords, \n              stringi::stri_count_regex)\n  \n  # Nombre cada columna de la matriz de resultados \n  # con los términos\n  colnames(tt) &lt;- keywords\n  \n  # Transforma los resultados en data.frame\n  tt &lt;- data.frame(tt)\n  \n  # Si se desea la frecuencia relativa\n  if(rel.freq==TRUE){\n    \n    # Cuenta todas las palabras de todas\n    # las intervenciones de cada mes\n    count &lt;- stringi::stri_count_words(corpus)\n\n    # Calcula la frecuencia relativa\n    for(i in 1:ncol(tt)){\n      tt[,i] &lt;- tt[,i]/count\n    }\n    \n  }\n\n  # Añade la identificación del tiempo\n  # (en nuestro caso el mes)\n  tt$Time &lt;- time\n  \n  # Cambia el formato de los datos\n  # (importante para generar el gráfico\n  # en un formato ggplot2)\n  tt &lt;- reshape2::melt(tt, id.vars = \"Time\")\n  \n  # Asigna los nombres de las columnas\n  names(tt) &lt;- c(\"Time\",\"Keyword\",\"Density\")\n\n  # Devuelve el resultado final\n  return(tt)\n      \n}\n\n\n\n\nEn el paso siguiente, creamos una copia de la base de datos de intervenciones legislativas, identificamos el mes en el cual se ha realizado y aglutinamos todas las intervenciones en una sola unidad de textos según la nueva unidad de tiempo. Tales “intervenciones del mes” serán empleadas en la recién creada countNgram para calcular la densidad de los términos “sexual” e “inmigra” que incluyen todo relativo a los derechos sexuales e inmigración. El código describe cómo hacerlo paso a paso y la tabla exhibe los resultados.\n\nCódigo# Crea una copia de los diarios de sesiones del \n# Congreso de Diputados de España\nss &lt;- spa.sessions\n\n# Genera una variable con el mes de cada\n# intervención de cada diario de sesiones\nss$mes &lt;- as.Date(\n            paste0(\n              substr(\n                ss$session.date, \n                1,\n                7),\n              \"-01\"))\n\n# Junta todas las intervenciones de un \n# mismo mes en un largo texto\nss &lt;- aggregate(list(texto=ss$speech.text), \n                by=list(mes=ss$mes), \n                paste0, \n                collapse=\"\\n\")\n\n# Ejecuta la función que cuenta los términos\n# por unidad de tiempo (en el caso cada mes)\nres &lt;- countNgram(\n            keywords = c(\"sexual\",\"inmigra\"), \n            corpus = ss$texto,\n            time = ss$mes)\n\n# Multiplica la frecuencia relativa por 10 mil \n# para facilitar la lectura de los datos y\n# reduce el número de dígitos decimales a 2.\nres$Density &lt;- round(res$Density*10000,2)\n\n# Enseña los resultados\nreactable(res,\n          resizable = T)\n\n\n\n\n\n\n\n\n\n\n\nFinalmente, el último paso para reproducir el ejemplo anterior con base en datos de Google NGram Viewer consiste en crear un gráfico. El código abajo hace justamente eso:\n\nCódigo# Carga el paquete ggplot2\nlibrary(ggplot2)\n\n# Renombra las variables para\n# que aparezcan mejor en el gráfico\nnames(res) &lt;- c(\"Mes\",\"Keyword\",\"Densidad\")\n\n# Genera un gráfico de línea con\n# la evolución por mes de cada \n# término\np &lt;- ggplot(res, \n            aes(\n              x=Mes, \n              y=Densidad,\n              color=Keyword))+\n  geom_line()+\n  theme_classic()+\n  theme(legend.position=\"bottom\")+\n  scale_color_discrete(name=\"Término\")+\n  labs(\n    title=\"Sesiones legislativas del Congreso \n           de Diputados (2019-2023)\", \n    subtitle = 'Densidad de palabras conteniendo \n               \"inmigra\" y \"sexual\" en los diarios \n               de sesiones del Congreso\\nde Diputados \n               de España entre diciembre de 2019 y \n               junio de 2023.')+\n  xlab(\"Año\")+\n  ylab(\"Frecuencia (a cada 10 mil)\")\n\n# Visualiza los resultados\np\n\n\n\n\n\n\n\n\n\n\nEl examen de los resultados de la tabla y el gráfico revelan dos picos, uno para cada palabra. En el caso de expresiones relacionadas a “sexual” este se encuentra en octubre de 2021, cuando se ha llevado a cabo el debate sobre la propuesta de la Ley Orgánica Garantía de la Libertad Sexual (la ley del “sí es sí”). Para “inmigra”, se trata de agosto de 2020, mes en el cual tuvo lugar una sesión de la diputación permanente en la que se aborda explícitamente el tema de la inmigración irregular ocurrida en ese verano.\n\nCódigolibrary(tenet)\n\n# Selecciona los principales diputados de \n# Vox en la XIV legislatura\nag &lt;- spa.sessions[\n  spa.sessions$rep.name%in%\n    c(\"Abascal Conde, Santiago\",\n      \"Espinosa de los Monteros de Simón, Iván\",\n      \"Olona Choclán, Macarena\",                \n      \"Ortega Smith-Molina, Francisco Javier\"),]\n\n# Reduce los nombres\nag$rep.name[ag$rep.name==\"Abascal Conde, Santiago\"] &lt;- \"Abascal\"\nag$rep.name[ag$rep.name==\"Espinosa de los Monteros de Simón, Iván\"] &lt;- \"Espinosa \"\nag$rep.name[ag$rep.name==\"Olona Choclán, Macarena\"] &lt;- \"Olona\"\nag$rep.name[ag$rep.name==\"Ortega Smith-Molina, Francisco Javier\"] &lt;- \"Ortega Smith\"\n\n# Create a variable of month for smoothing the data\nag$month &lt;- substr(ag$session.date,3,7)\n\n\n# Aggregate words by representative and month\nag &lt;- aggregate(\n  list(words=ag$speech.tokens), \n  by=list(\n    month=ag$month, \n    rep=ag$rep.name, \n    party=ag$rep.party), \n  sum, \n  na.rm=T)\n\n# Order the data by month\nag &lt;- ag[order(ag$month),]\n\nplotStream(ag,\n           x=\"month\",\n           y=\"words\",\n           group=\"rep\", elementId = \"elem223\")\n\n\n\n\n\n\n3.2.4 Etiquetado automático de textos\nUna tarea central de cualquier análisis cualitativo de texto consiste en la lectura atenta y la selección de pasajes o términos para la codificación temática. Se trata de un proceso iterativo -con muchas idas y venidas, revisiones y cambios- en el cual el investigador marca o subraya trechos o palabras del documento y les asigna un código o categoría analítica. En ese sentido, la lectura orienta el proceso de codificación y clasificación, dando lugar al surgimiento de conceptos más abstractos. Aunque la investigación parta de algunas nociones previas, esta fase exploratoria consiste en la clave para la vinculación de elementos concretos presentes en el texto a temas teóricos más amplios. Existe cierto consenso en la literatura sobre el tema que considera la clasificación temática como el resultado (y no el punto de partida) del análisis cualitativo (Saldana 2015, 13; Krippendorff 2004; Neuendorf 2017; Guest, MacQueen, and Namey 2011; Miles, Huberman, and Saldana 2019; Auerbach and Silverstein 2003). Es el producto de la interacción entre el analista y el texto.\nPor esa razón, se necesitan instrumentos que faciliten tal interacción entre el investigador y el texto, que les permitan partir de algunas semillas y profundizar en las sucesivas capas de análisis hasta encontrar los temas centrales de su investigación. En este apartado introduciremos dos opciones de etiquetado y visualización de textos. Constituyen recursos que permiten lo que algunos autores (Saldana 2015, 13) denominan como pre-codificación, como el sublineado o coloreado de pasajes del texto con el propósito de identificar pasajes útiles para la generación de hipótesis o la creación de conceptos.\nEl etiquetado toma un texto único y examina la presencia de un conjunto de palabras-clave o expresiones definidas por el investigador. Si se busca, por ejemplo, saber cómo los presidentes mencionan las políticas sociales en sus discursos, no sería raro que se empezara buscando referencias a la seguridad social, sanidad, educación, pobreza o desigualdad. Estudiosos del populismo, por otra parte, buscarían referencias a patria, pueblo, nación, o élites.\nLa función tagText del paquete tenet permite realizar dicha tarea. Su utilidad consiste en subrayar un texto concreto según una lista de palabras-clave. Su propósito resulta muy sencillo: ayudar en la creación de códigos. El usuario puede leer el texto y al mismo tiempo, crear un diccionario o una lista de palabras que resultan particularmente expresivas de una idea o concepto que se desea capturar.\nEl código abajo selecciona el discurso de investidura de Adolfo Suárez y subraya todas las palabras que contengan las raíces “politic”, “acci”, “conflict”, “partid”, “defensa”, y la expresión “fuerzas armadas”. A cada palabra asocia un color para que sea más fácil su identificación en el texto. De ese modo, se puede ver dónde aparecen.\n\nCódigo# crea un objeto con el nombre de todas \n# las paletas de colores disponibles en tenet\npal &lt;- listPalettes()\n\n# Examina un conjunto de palabras en el\n# discurso de investidura de Adolfo Suárez\ntagText(as.character(spa.inaugural$text[1]), \n        keywords = c(\"politic\", \n                     \"acci\", \n                     \"conflict\", \n                     \"partid\", \n                     \"defensa\",\n                     \"fuerzas armadas\"), \n        palette = pal$cat.wesanderson.Darjeeling1.5,\n        font.size = 18, \n        title = \"Adolfo Suarez (1979)\",\n        margin = 100)\n\n\n\nComo se puede observar, a cada raíz, palabra-clave o expresión le corresponde un color distinto. Este atributo favorece no solo la identificación de su posición en el texto, sino que también los distingue y subraya cuando coinciden en un mismo párrafo o sentencia. Sitúa, contextualiza, compara y, sobre todo, mantiene el lector anclado al texto.\nUna queja o desconfianza de ciertos investigadores cualitativos frente a métodos asistidos por ordenador tiene que ver con el distanciamiento que éstos últimos provocan con relación a las fuentes textuales. No obstante, el proceso no tiene que implicar una tal dicotomía si se emplean herramientas adecuadas para el examen de patrones textuales.\nLa función tagText también se puede utilizar en conjunción con diccionarios. Resulta útil para averiguar la consistencia de la codificación desarrollada, así como permite perfeccionar los códigos existentes. La estructura de un diccionario resulta sencilla. A cada código corresponde un conjunto de términos, raíces o expresiones. Al emplear un diccionario en tagText todos los elementos de un código se representan bajo un mismo color. De ese modo, resulta más fácil no solo ubicar las categorías, sino también ver hasta qué punto aparecen en un mismo pasaje del documento. Más adelante, cuando tratemos del proceso de codificación por medio de diccionarios emplearemos algunos ejemplos.\n\n3.2.5 Dispersión léxica\nEn ciertas ocasiones interesa saber no solo cuántas veces una palabra aparece en un texto o corpus, sino dónde. El lugar en que se manifiesta una idea puede ser muy significativo para determinar su importancia en un discurso. ¿Está por todo el texto o aparece solamente en algunas partes? ¿Qué documentos poseen mayor o menor concentración?\nEl gráfico de dispersión léxica (lexical dispersion plot) representa una excelente visualización para determinar la localización de un conjunto de palabras-clave en distintos pasajes o sentencias de documentos de componen un corpus y, por lo tanto, su grado de dispersión (o concentración). La función plotLexDiv del paquete tenet permite crear el gráfico a partir de un conjunto de palabras clave (o incluso de un diccionario). El paquete quanteda.textplots también dispone de una función semejante, pero limitada a una sola palabra de cada vez y que no permite la agregación de términos bajo una misma categoría de un diccionario.\nEl código abajo explora dos maneras en las que los presidentes de gobierno españoles se refieren a los ciudadanos del país. La primera, con énfasis más patriótico, se centra en las expresiones “españoles” y “españolas”. La segunda, con un carácter más republicano, enfoca la membresía a la comunidad política bajo los términos “ciudadanos” y “ciudadanas”. ¿Existen diferencias en el uso de esos dos modos de representar a los miembros de la sociedad española? ¿Su presencia resulta concentrada o dispersa, es decir, se trata de algo muy concreto en una parte de los discurso o aparece en todo el texto como una expresión recurrente y articuladora de las demás partes?\n\nCódigo# Genera el primer gráfico con las\n# palabras \"españoles\" y \"españolas\"\np1 &lt;- plotLexDiv(cp,\n           title = \"Españoles\", \n           keywords = c(\"espanoles\",\"espanolas\"), \n           custom.color = \"red3\")\n\n# Genera el segundo gráfico con las\n# palabras \"ciudadanos\" y \"ciudadanas\"\np2 &lt;- plotLexDiv(cp,\n           title = \"Ciudadanos\", \n           keywords = c(\"ciudadanos\",\"ciudadanas\"), \n           custom.color = \"blue\")\n            \n\n# Carga los paquetes necesarios para\n# pegar los dos gráficos como una única\n# imagen\nlibrary(grid)\nlibrary(gridExtra)\n\n# Genera la visualización final\ngrid.arrange(p1,p2, nrow=1)\n\n\n\n\nUna breve inspección visual revela que hay diferencias entre los presidentes en el uso de una u otra forma. Mariano Rajoy aparece como el más enfático usuario de “españoles”, mientras que Felipe González se decanta por “ciudadanos”. Otros se refieren a ambos términos, lo que exige un escrutinio más detenido para entender el contexto del uso de cada. En algunos documentos las expresiones aparecen a lo largo de toda la extensión, mientras que en otros, como españoles en Zapatero II o ciudadanos en Calvo Sotelo, solo en pasajes concretos.\n\n3.2.6 Filtrado de textos: la ratio Documento/Corpus\nCuando se trabaja con muchos textos, uno de los mayores retos consiste en separar el material útil del montón de información que no se desea emplear, al menos en un principio. Por eso, hacen falta estrategias de selección o filtrado de documentos que permitan arrojar luz sobre aquellos documentos que contienen material sustantivo para la investigación.\nEn muchos casos, basta con buscar aquellos documentos que contienen una expresión o palabra. No obstante, ¿qué pasa cuando esta palabra aparece en varios documentos, pero ni todos ellos son relevantes? En estos casos, hace falta algún criterio que permita discriminar de forma ponderada su peso relativo en un corpus.\nLa ratio Documento - Corpus de una palabra constituye una solución posible para este problema. Se trata de una medida que evalúa cuán marcada resulta la aparición de un término concreto en cada documento frente a su frecuencia relativa media en todo el corpus. Una cifra superior a uno en un documento indica una prevalencia más alta que la media y, a la inversa, un valor inferior a uno señala un aparecimiento menos frecuente.\n\\[fd/fc = \\frac{Fd_{i}}{F_i}\\]\nDonde:\nFdi corresponde a la frecuencia relativa del término i en del documento d.\nFi corresponde a la frecuencia relativa del término i en todo el corpus.\nPuesto que los textos que componen un mismo corpus pueden presentar distinto tamaño, se utiliza la frecuencia relativa para evitar distorsiones y garantizar la comparabilidad de los resultados.\nHemos creado la función tfRatio en el paquete tenet con el objetivo de calcular la ratio de una palabra-clave, raíz o expresión en todos los documentos de un corpus. Genera una lista con la ratio del término para cada documento o una lista de los documentos que superan un cierto umbral (por medio de los parámetros threshold y return.selected).\nEl código abajo calcula la ratio de la raíz “machis” (machismo, machista) para todos los discursos de investidura de los presidentes de gobierno españoles:\n\nCódigo# Calcula la ratio de las palabras que\n# contienen \"machis\" en el corpus de \n# los discursos de investidura\ntfRatio(cp, \"machis\")\n\n [1] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 2.62 0.00 0.00 0.00 4.68 3.27\n\n\nComo podemos observar, solamente tres de los 15 textos contienen alguna referencia al machismo. Estos son los dos discursos de Pedro Sánchez y el primero de José Luis Rodríguez Zapatero.\nSi queremos seleccionar los textos, el parámetro threshold permite establecer un mínimo para que un documento se considere relevante y el argumento return.selected retorna el índice del documento en lugar el la ratio. El código abajo utiliza las ratios para seleccionar los textos en los que palabras como machismo o machista aparezcan más del doble de veces que en la media de todo el corpus (threshold=2):\n\nCódigo# Convierte los textos en un data.frame\ntx &lt;- convert(cp, \"data.frame\")\n\n# Selecciona con relacion a la ratio\nsx &lt;- tx[tfRatio(cp, \n                 \"machis\", \n                 threshold = 2, \n                 return.selected = T),]\n\n# Visualiza los resultados\nreactable(sx, \n          resizable = T,  \n          wrap = F)\n\n\n\n\n\n\n\nAhora el investigador puede utilizar otras herramientas como el tagText, los árboles de palabra, la dispersión léxica o el kwic para analizar los textos y entender cómo cada uno de ellos trata el tema del machismo. Al reducir de forma clara el tiempo de selección de los textos relevantes, esta medida posibilita concentrar esfuerzos en otras tareas de análisis."
  },
  {
    "objectID": "02_explora_deductivo.html#codificación-temática-con-diccionarios",
    "href": "02_explora_deductivo.html#codificación-temática-con-diccionarios",
    "title": "\n3  Codificación temática\n",
    "section": "\n3.3 Codificación temática con diccionarios",
    "text": "3.3 Codificación temática con diccionarios\nLa codificación temática consiste en una de las estrategias más comunes en las ciencias sociales para tratar datos en forma de texto. De forma muy resumida, representa un proceso fundamentado en la lectura profundizada de una cantidad limitada de textos y acompañada de anotaciones que permitan extraer temas o conceptos que ayuden en el desarrollo o teste de hipótesis.\nDe acuerdo con Krippendorff (2004, 18) el análisis de contenido constituye una técnica que permite la realización de inferencias a partir de textos en el contexto en que se emplean. Neuendorf (2017, 1), por otra parte, define el método como el “análisis sistemático, objetivo y cuantitativo de las características de un mensaje”. Otros autores (Saldana 2015; Miles, Huberman, and Saldana 2019) extienden el proceso de codificación más allá del análisis de contenido y mencionan un número más amplio de métodos que se basan en el examen detenido de información textual, como el análisis de discurso, la teoría fundamentada (grounded theory) o el análisis narrativo.\nLa codificación de temas y la extracción de insights teóricos es una constante en la literatura sobre los métodos cualitativos en las ciencias sociales. En algunos casos, se considera un proceso casi artesanal basado en la lectura profunda y un conocimiento detallado de los textos y sus contextos. En otros, la existencia de volúmenes masivos de documentos invita a la combinación entre: (a) el escrutinio atento de una muestra de los documentos y (b) métodos cuantitativos que permitan sintetizar y clasificar un masa de material imposible de examinar de forma tradicional.\n\n3.3.1 Expresiones, códigos, temas\nResulta útil iniciar este apartado con una breve distinción entre expresiones (términos, palabras o n-gramas), códigos (etiquetas que sintetizan el contenido) y temas (constructos teóricos derivados del análisis). De forma resumida, se puede argumentar que las primeras indexan los textos (conectan palabras a ideas) y ayudan a seleccionar atributos o pasajes que pueden ser particularmente significativas o teóricamente relevantes. Los siguientes (códigos) posibilitan aunar los términos en grupos o colecciones y llevar a cabo un esfuerzo de agregación o síntesis conceptual previo al desarrollo de conceptos más abstractos. Finalmente, un tema emerge del análisis de los distintos códigos, sus contextos y relaciones recíprocas.\nImaginemos que deseamos saber la posición de distintos partidos sobre las políticas sociales. Una forma de encontrar informaciones consiste en buscar palabras como pobreza, desigualdad, ayudas, sanidad, educación, seguridad social, asistencia social, transferencias, entre otros términos. En su conjunto, tales expresiones permiten ubicar en los diferentes documentos de un corpus cuánto y dónde se ha mencionado alguna de esas medidas o asuntos relacionados. Por lo tanto, actúan como instrumentos de búsqueda e indexación de categorías abarcadoras, que pueden estar reunidas en códigos que ayuden a identificar cada aspecto mencionado con la noción general de política social. Sin este paso, resulta muy complicado filtrar aquellos pasajes más destacados sobre ese tema en corpus con muchos documentos. Tampoco se podría calcular su peso relativo: ¿cuál política recibe un número mayor de menciones: la sanidad o la educación?\nLos códigos son, a su vez, “etiquetas que atribuyen significado simbólico a información descriptiva o inferencial compilada durante un estudio” (Miles, Huberman, and Saldana 2019, 78–79). Tales “etiquetas” pueden ser palabras o expresiones que permitan atribuir un sentido más amplio, sea descriptivo o conceptual, a la palabra o trecho al que se le asocia. También pueden considerarse como dispositivos de segmentación y aglomeración de significados. Al asociar un conjunto diverso de palabras a los términos izquierda y derecha, por ejemplo, establecemos qué partes se aúnan dentro de esas categorías y, al mismo tiempo, las separamos del resto del texto (diagrama 1).\n\n\n\nDiagrama 1. Ejemplos de términos y expresiones, así como de códigos de primer y segundo nivel en un diccionario con las categorías izquierda y derecha.\n\n\nAdemás, los códigos facilitan la búsqueda, clasificación y ordenamiento de la información según los intereses del investigador. De cierto modo, sintetizan la diversidad de términos y expresiones en un número reducido de categorías o marcadores. Corresponden a una heurística, un método de descubierta (Miles, Huberman, and Saldana 2019, 80). Al reestructurar los textos de acuerdo a categorías analíticamente significativas, permite identificar patrones, establecer la relación entre códigos y desarrollar interpretaciones analíticas de más alto nivel. En cierto sentido, constituyen una herramienta para estructurar el texto y permitir su recomposición bajo lógicas analíticas distintas.\nFinalmente, los temas constituyen construcciones teóricas derivadas del análisis de los códigos. Pueden surgir del examen de un, diez o cien mil documentos. Todo va a depender del diseño de la investigación y cómo cada investigador construye sus categorías analíticas, abstrayendo temas más generales a partir de códigos encontrados en los textos. El tipo y el grado de abstracción dependerá de los objetivos definidos para cada estudio. No obstante, el método suele ser siempre el mismo: la definición de códigos, su análisis y la posterior síntesis en un conjunto de temas clave y teóricamente significativos.\nAsociar códigos a los textos, por lo tanto, permite extraer información valiosa para la investigación. Como hemos señalado, un código puede estar asociado tanto a una palabra en concreto como a pasajes enteros. Por esa razón, cabe subrayar que el método propuesto en este trabajo resulta un poco distinto de la codificación tradicional y representa una alternativa híbrida entre métodos puramente cuantitativos y la construcción de un sistema cualitativo tradicional. Aquí, la búsqueda de raíces, palabras o expresiones se utiliza para identificar e indexar dónde una determinada idea aparece en el documento.\nTales elementos empíricos fundamentales pueden estar aislados y servir para un examen preliminar del corpus o agregados según categorías analíticas. Los códigos también pueden anidarse en una estructura jerárquica, como ramas e un árbol. Como vimos en el diagrama 1 arriba, los términos y expresiones “derecho aborto”, “cambio climático” y “transex” conforman la categoría “progresismo”. Ya “educ”, “pobreza” y “social” se aúnan en “igualdad social”. Finalmente, los dos códigos (progresismo e igualdad social) componen la categoría más abstracta “izquierda”.\nEn resumen, un código agrupa términos básicos en categorías analíticas más o menos homogéneas internamente y distintas entre sí. De ese modo, un código se constituye a partir de: (a) una recolección de esas unidades textuales básicas (más que de frases o trechos enteros) o (b) de otros códigos con menores niveles de abstracción.\nTal estrategia puede considerarse a primera vista como limitante. No obstante, presenta dos grandes puntos fuertes. En primer lugar, palabras o expresiones cortas sirven como elementos útiles para la selección de pasajes más amplios de forma automática y el filtrado de contenido relevante. Al emplearse en conjunto con las herramientas de visualización disponibles, como el kwic y el etiquetado de textos resulta más facil situar las ideas en su contexto sin tener que examinar de antemano largos pasajes de textos.\nSegunda ventaja: el uso de palabras, bigramas o trigramas es indudablemente más fácil de cuantificar y comparar en grandes corpus de texto. Resulta mucho más sencillo (y rápido) comparar “machis” en 10 mil documentos que frases concretas que solo indirectamente mencionan el tema del machismo. Para estos últimos casos, estrategias complementarias, como la inclusión de metáforas u otras alusiones indirectas relevantes puede ayudar en su identificación. Cuando el número de documentos es inabarcable desde una metodología tradicional, una alternativa útil consiste en combinar la lectura profundizada de una muestra cuidadosamente seleccionada de textos con algoritmos que permitan rastrear la presencia de códigos en todo el material disponible.\nEste método implica un cambio en la forma de construcción de los códigos. Aún más cuando empleamos diccionarios (o léxicos) temáticos como instrumento. La inclusión de una palabra o n-grama en los diccionarios hace con que nuevos pasajes salgan a la luz. La clave se encuentra en analizar las frases o párrafos que contienen tales expresiones y, a partir de ahí, refinar el mismo léxico, desarrollar códigos más abstractos y generar temas teóricamente relevantes para el estudio. Como se ha señalado más arriba, no se trata de abandonar un enfoque cualitativo, sino adaptarlo a situaciones en las que la lectura exhaustiva y en profundidad del corpus resulta imposible o poco práctica.\n\n3.3.2 Diccionarios como colecciones de códigos\nComo se ha mencionado más arriba, los diccionarios pueden considerarse como dispositivos de ensamblaje de expresiones de interés teórico o como instrumentos de organización de ideas. Permiten la aglutinación de términos en categorías analíticas más amplias y su posterior organización en estructuras conceptuales jerárquicas.\nEl diagrama 1 más arriba nos muestra como ciertas raíces, palabras o expresiones se agrupan en categorías como progresismo, conservadurismo, liberalismo económico o igualdad social y éstas en códigos más abstractos como izquierda o derecha. Este breve diccionario nos ayuda a asociar términos que se pueden encontrar en los textos a conceptos o ideas más abstractas, sin una manifestación empírica directa. De ese modo, la izquierda se compone por dos dimensiones -igualdad social y progresismo-, mientras que la derecha se caracteriza como la combinación entre liberalismo económico y conservadurismo social.\nAunque esté muy lejos de brindar una definición exhaustiva de los conceptos de derecha e izquierda, este pequeño ejemplo ilustra cómo un diccionario permite asociar elementos empíricos concretos encontrados en los textos a dimensiones teórico-conceptuales más abstractas.\nPor esa misma razón, se debe subrayar la naturaleza iterativa del proceso. Las categorías más abarcadoras (como progresismo, por ejemplo) se suelen construir a partir de un proceso reiterado de examen de los textos, la inclusión de nuevos términos y la revisión de los resultados. Incluso en los casos en que se dispone de un diccionario previo hecho por otros, resulta fundamental la adaptación a los propósitos de investigación para alcanzar los mejores resultados.\nLa creación de un diccionario representa una técnica de medición y búsqueda (Neuendorf 2017, 126–27) en la el uso de palabras y otros elementos textuales permiten identificar la presencia de ciertas ideas o conceptos en un corpus determinado. También se le podría considerar como un libro de códigos, una compilación de categorías y los elementos que le componen. La formalización explícita de los grupos y su documentación facilita el trabajo en grupo y aumenta la transparencia y reproducibilidad de al menos parte del análisis realizado (Saldana 2015, 21).\nEl ejemplo abajo crea un diccionario que clasifica 39 términos según los códigos “economía”, “fiscal”, “educación”, “sanidad” y “medioambiente”. Luego, lo emplea en conjunción con la función tagText para resaltar las categorías en el discurso de investidura de Adolfo Suárez. Al analizar los resultados, vemos que a cada categoría (o código) corresponde un color cuyo nombre se revela al mover el cursos sobre una palabra subrayada.\n\nCódigo# Crea un diccionario de algunos términos políticos\ndic &lt;- dictionary(\n    list(\n    economica=c(\"econom\",\n               \"inversion\",\n               \"empresa\",\n               \"desarroll\",\n               \"monetari\",\n               \"industri\",\n               \"agric\",\n               \"agrari\"),\n    fiscal=c(\"hacienda\",\n               \"gasto\",\n               \"impuest\",\n               \"presupuest\",\n               \"tribut\",\n               \"tasa\",\n               \"fiscal\"),\n    educacion=c(\"educa\",\n             \"profesor\",\n             \"docent\",\n             \"escuel\",\n             \"colegio\",\n             \"universi\",\n             \"formación\"),\n    sanidad=c(\"sanidad\",\n               \"salud\",\n               \"hospital\",\n               \"sanitari\",\n               \"médic\",\n               \"enfermer\",\n               \"salud\"),\n    medioambiente=c(\"sostenible\",\n                 \"cambio clima\",\n                 \"medioambient\",\n                 \"reciclaje\",\n                 \"ecológico\",\n                 \"límpia\",\n                 \"invernadero\",\n                 \"emisiones\",\n                 \"carbono\",\n                 \"plástico\",\n                 \"fósiles\")))\n\n# genera un texto para ser leído en el panel\n# Viewer de RStudio\ntagText(spa.inaugural$text[1], \n        keywords = dic, \n        palette = pal$cat.cartocolor.prism.11,\n        font.size = 18, \n        title = \"Adolfo Suarez (1979)\",\n        margin = 100)\n\n\n\nEl resultado nos sitúa en un espacio entre una búsqueda automatizada de términos y la codificación manual. Al aplicar el diccionario a un texto específico, se puede observar no solo dónde las categorías aparecen más o menos, sino también cómo estas se asocian entre ellas en un mismo párrafo, por ejemplo. Algunos pasajes son monotemáticos, inciden sobre una idea clave. Otros interesan por la asociación entre conceptos distintos. En muchas ocasiones es justamente la asociación entre temas lo que permite el surgimiento de nuevas hipótesis. Además, el análisis de la incidencia de los códigos en el texto invita a la revisión del diccionario para incorporar nuevos términos o categorías y, de ese modo, completar el análisis.\nMiles et al. (2019, 86) sugieren que un método para utilizar diccionarios como herramientas para la codificación consiste en crear una lista provisional de códigos por medio de un proceso deductivo a partir de las referencias teóricas que sirven de marco para el estudio. Una vez elaborada, puede servir de semilla para el examen de los textos y pasar por procesos sucesivos de adaptación, refinamiento y elaboración con el empleo de una codificación inductiva complementaria.\nEste proceso de revisión constante requiere instrumentos que permitan explorar, ordenar, filtrar y sintetizar la información. La función tagCorpus de tenet emplea una tabla interactiva que permite a los usuarios llevar a cabo una serie de tareas de exploración de los términos y códigos de un diccionario en todo un corpus. Por lo tanto, se centra en posibilitar la identificación tanto de aspectos compartidos como de señas distintivas entre documentos, categorías o actores. Además, permite examinar de forma sumaria la coocurrencia de códigos en frases o párrafos.\nEl paquete tenet también incluye un diccionario de ejemplo llamado dic.pol.es. Se trata de un conjunto de códigos que analizan diferentes dimensiones de los discursos políticos. Contiene tres niveles: (1) palabras o expresiones, (2) códigos de primer nivel y (3) códigos de segundo nivel. Por ejemplo, “izquierda unida” pertenece al código nivel-1 “partidos” y al código nivel-2 “actores”. Por su parte, “ilustres” pertence a “retórica” (nivel-1) y a “discurso” (nivel-2).\nEl código abajo utiliza la función tagCorpus y el diccionario dic.pol.es para identificar la incidencia de las categorías en cada sentencia del corpus de los discursos de investidura españoles. Como se podrá ver, abajo, el resultado es una tabla con siete columnas. La primera Order corresponde al orden de la frase en el documento X (columna Doc.). De ese modo, Order igual a 1 y Doc. igual a Suárez corresponde a la primera frase del discurso de investidura de Adolfo Suárez. Paragraph corresponde a la unidad textural, que puede ser el documento completo (documents), párrafos (paragraphs) o oraciones (sentences). Las categorías más frecuentes aparecen en la columna siguiente (Main Category) y todas las categorías encontradas en la columna Paragraph aparecen en All Categories, incluídas, por supuestos, las más frecuentes. Matches informa el número de veces una palabra o término del diccionario se ha encontrado en el texto. Finalmente, Cat. No. informa el número total de categorías encontradas.\nAdicionalmente, debajo del nombre de cada columna, se pueden encontrar campos de filtro. Basta digitar cualquier valor o texto para seleccionar los resultados. Por ejemplo, si uno desea saber cómo Calvo Sotelo trataba temas sociales, puede seleccionar solo los documentos que se inicien por “Calvo” y que, en All Categories, incluya el código “social”. Al hacer clic sobre el nombre de cada columna también se pueden ordenar los valores de forma ascendente o descendente.\n\nCódigo# tagCorpus, hace algo parecido para un corpus\ntagCorpus(cp,\n          defaultPageSize = 4,\n          dic.pol.es, \n          palette = pal$cat.ggthemes.tableau.20, \n          reshape.to = \"sentences\", \n          show.details = T)\n\n\n\n\n\n\n\nConsideremos otro ejemplo. Si queremos identificar cuáles actores sociales y políticos mencionados en los discursos de investidura de los presidentes españoles que estén vinculados al tema tecnológico, podemos filtrar las sentencias del corpus en las que el tema principal (Main Category) son los “actores” y en que también aparezcan (All Categories) “tecnologia”.\nVemos que la concepción tecnológica de los presidentes pasa por una actuación clave de empresas y del mercado. Poco se menciona sobre el rol de la inversión en ciencia. Mucho inversor y poco investigador. La innovación, por lo tanto, se da por la atracción de capital y de tecnologías desarrolladas por otros más que por un proceso autónomo de construcción tecnológica a partir de la inversión en ciencia. No resulta para nada casual que casi la mitad del presupesto asignado a investigación suela estar constituida por créditos destinados al I+D+I de empresas (en colaboración con la universidad)."
  },
  {
    "objectID": "02_explora_deductivo.html#análisis-temático",
    "href": "02_explora_deductivo.html#análisis-temático",
    "title": "\n3  Codificación temática\n",
    "section": "\n3.4 Análisis temático",
    "text": "3.4 Análisis temático\n¿Cuáles códigos tienen más peso? ¿Qué categorías se asocian de forma más estrecha? Una vez creados los códigos y los diccionarios, cabe dar un paso adelante y buscar patrones, identificar las características de los conjuntos de términos y sus relaciones con otros atributos en los textos.\nEl análisis de la incidencia de los códigos en un corpus y su interrelación permiten explicitar patrones y definir el peso relativo de cada idea en los textos. Este apartado emplea tres estrategias para explorar la importancia de los temas. La primera consiste en averiguar el peso de las categorías, es decir, emplear estadísticas sumarias, como la frecuencia relativa de códigos o expresiones, para establecer su prevalencia. La segunda se basa en la desagregación y el filtro para comparar grupos o atributos o para seleccionar aspectos concretos que se desean examinar con más detenimiento. La tercera investiga su asociación por medio de las redes de coocurrencia.\nCombinadas, tales estrategias permiten identificar los temas centrales presentes en un texto o corpus y cómo se relacionan entre sí. También revelan su variación de acuerdo con variables contextuales, como puede ser un partido, el presidente o un período de tiempo determinado. Se tratan de herramientas sencillas, pero muy útiles, a la hora de contextualizar ideas e identificar variaciones importantes en el uso de conceptos o términos.\n\n3.4.1 Estadísticas temáticas\nDenominamos estadísticas temáticas el conjunto de técnicas cuantitativas que permiten representar la importancia de categorías o expresiones en un corpus. ¿Cuántas veces los presidentes de gobierno han mencionado la ciencia en sus discursos de investidura? ¿Cuántas han mencionado al terrorismo? ¿Quiénes han sido los que más uso han hecho de la expresión “género” o “fuerzas de seguridad”?\nPor lo tanto, aunque sencillas, tales herramientas permiten delinear diferencias programáticas e ideológicas entre distintos actores políticos. Sobre todo, señala aquellas categorías más frecuentes, tanto por el número de veces que aparecen como por la cantidad de documentos en los que aparecen. Por ejemplo, solo algunos de los presidentes mencionan el tema de género en sus discursos (en especial José Luis Rodríguez Zapatero y Pedro Sánchez). No obstante, temas como el mercado laboral o la fiscalidad del Estado, como esperado, aparecen en todos ellos (aunque acompañados de distintos calificativos).\nEl panel abajo contiene un conjunto de recursos para el análisis de los códigos del diccionario dic.pol.es aplicado a los discursos de investidura de los presidentes de gobierno de España. La primera pestaña (tabla) contiene la frecuencia relativa de los códigos y términos del diccionario en el corpus. Las demás corresponden a visualizaciones que permiten hacer una síntesis de los pesos relativos de palabras-clave y categorías en los textos. Force Directed Tree genera un diagrama de árbol que representa la jerarquía de los términos como una red. Las dos alternativas siguientes (voronoi tree) generan una imagen parecida, pero con otros instrumentos de interacción y niveles de zoom. Esto permite mirar hacia los resultados de una forma ligeramente distinta.\n\n\nTabla\nForce-Directed Tree\nVoronoi (Códigos)\nVoronoi (keywords)\n\n\n\nLa función countKeywords de tenet produce un data.frame con un conjunto de campos que auxilian en el análisis del peso relativo de cada categoría en un corpus determinado. El primero es groups, que indica el grupo (como partido o presidente, por ejemplo) que detalla los resultados. Si no se ha informado ninguna variable de grupo, aparecerá “All” (todos). El segundo, level1, señala el código de más alto nivel en un diccionario (la función admite hasta dos niveles de jerarquía, en esos casos aparecería también level2). El tercero, keyword, indica la palabra-clave que conforma el diccionario y frequency muestra la frecuencia (absoluta o relativa del término en el corpus).\nLa tabla abajo muestra la frecuencia relativa (por cada mil palabras) de cada término del diccionario dic.pol.es en el corpus de discursos de investidura de los presidentes españoles.\n\nCódigo# Calcula la frecuencia relativa en que cada\n# palabra ha sido encontrada para cada nivel\n# del diccionario\nxy &lt;- countKeywords(cp, \n                    dic.pol.es, \n                    rel.freq = T, \n                    quietly = TRUE)\n\n# Elimina los términos no encontrados\nxy &lt;- xy[xy$frequency&gt;0,]\n\n# Puesto que es una frecuencia relativa\n# multiplicamos por 10 mil para tener la\n# ratio de ocurrencia a cada 10 mil palabras\nxy$frequency &lt;- round(xy$frequency*10000, 1)\n\n# Visualiza los resultados\nreactable::reactable(xy, \n                     resizable = T)\n\n\n\n\n\n\n\n\n\nSi ordenamos por frecuencia, vemos que la categoría más general de discurso y, dentro de esta, España, se destaca. A ella le sigue la figura retórica de “Señor”, que incluye todas las formas derivadas: señor, señora, señoría, señores, y demás. Se trata también de una forma de tratamiento común en este tipo de texto en los que los candidatos a presidente de gobierno se refieren de forma respetuosa a los demás representantes parlamentarios.\n\n\nAunque la tabla nos brinde los detalles de la frecuencia de cada término, una visualización de todos los códigos a la vez posibilita entender su peso relativo de forma instantánea y comparada. Un diagrama de árbol (force directed tree) representa cada categoría en el diccionario como un árbol en el que cada código es una rama y cada elemento una hoja que se atraen o repulsan de acuerdo con su peso relativo (Holten 2006). El tamaño de cada círculo (rama o hoja) se define de acuerdo con su frecuencia y el color según el nivel más alto en el diccionario. A mayor peso, más centralidad en el gráfico. De acuerdo con el ejemplo que se emplea aquí, discurso tendrá un color, social otro, exterior el suyo y así sucesivamente. Cada una de esas categorías abarcadoras mantendrá la misma estructura de códigos secundarios y expresiones (o keywords) como figuran en el diccionario.\n\nCódigo# Prepara los datos en el formato\n# adecuado para el gráfico\njs &lt;- jsonTree(data = xy, \n               groups=c(\"level1\",\"level2\"), \n               elements = \"keyword\",\n               value=\"frequency\")\n\n# Genera el gráfico de árbol\nforceDirectedTree(js,attraction = -8,\n                  palette = pal$cat.awtools.bpalette.16, \n                  max.radius = 18, \n                  height = 500)\n\n\n\n\n\n\n\n\nComo se puede ver, la categoría discurso es la que más peso tiene en el corpus. Está conformada por expresiones de tratamiento como “señorías” o “investidura” y relacionadas a España, como “españoles”, “patria” o “pueblo”. Viene seguida de temas sociales, de política exterior y fiscales.\n\n\nUn treemap corresponde a otra forma de visualización de datos jerárquicos. En este caso, todas las categorías y sus subcategorías se dividen en un círculo fragmentado en partes que se dimensionan según la frecuencia de cada código (Balzer and Deussen 2005).\n\nCódigo# Agrega las frecuencias según los dos niveles\n# de código contenidos en el diccionario\nxx &lt;- aggregate(list(frequency=xy$frequency),\n                by=list(level1=xy$level1,\n                        level2=xy$level2),\n                sum, na.rm=TRUE)\n\n# Genera el gráfico\nplotVoronoiTree(data = xx,\n                groups = \"level1\",\n                elements = \"level2\",\n                value = \"frequency\")\n\n\n\n\n\nAl hacer clic sobre una categoría, el gráfico hace un zoom y redistribuye el espacio solo con las subcategorías del código principal seleccionado.\n\n\nEl ejemplo abajo repite el gráfico anterior, pero ahora con las palabras-clave como unidades de división de las áreas del círculo. Aquí se pueden observar el peso relativo de cada expresión en la configuración de cada código.\n\nCódigo# Agrega las frecuencias según el primer\n# nivel de código y las palabras clave \n# contenidas en el diccionario\nxx &lt;- aggregate(list(frequency=xy$frequency),\n                by=list(level1=xy$level1,\n                        keyword=xy$keyword),\n                sum, na.rm=TRUE)\n\n# Genera el gráfico\nplotVoronoiTree(data = xx,\n                groups = \"level1\",\n                elements = \"keyword\",\n                value = \"frequency\")\n\n\n\n\n\n\n\n\n\n\n\nUn análisis rápido de los resultados subraya la importancia de formas retóricas y puramente discursivas en el corpus. El uso de expresiones de tratamiento, como señorías, o relativos a España o los españoles predomina por su reiterada aparición. El tamaño de la categoría “discurso” en todas las visualizaciones manifiesta claramente su predominio sobre los demás temas.\nLa segunda categoría de mayor importancia se encuentra relacionada con temas sociales. No sorprende que las cuestiones laborales, y en particular el empleo, constituyan elementos centrales de los discursos de todos los presidentes. Con relación a las demás áreas de política social, se percibe un destaque muy particular a la educación. Se trata de un tema alrededor del que los distintos partidos siempre han marcado sus diferencias programáticas. Tal protagonismo se ve reflejado en los discursos de investidura.\nEn política exterior, pesa mucho más Europa frente a otros temas y al resto del mundo. Se observa una clara orientación hacia el contexto regional frente a otros vínculos políticos más tradicionales. Este patrón se puede verificar claramente en la falta casi absoluta de protagonismo de América Latina en los textos.\nLa mención a distintos actores sociales también resulta útil para entender la relación de los presidentes con diferentes sectores de la sociedad civil. En el corpus analizado, queda claro el destaque atribuido a las empresas y empresarios, vistos como promotores de crecimiento económico. En segundo lugar, hay muchas referencias al propio partido o a aquellos que forman parte de la coalición de gobierno. Los trabajadores ocupan el último lugar.\nEl tema territorial aparece, principalmente, bajo la forma de acción administrativa del Estado hacia comunidades autónomas y la administración local. No obstante, otros temas vinculados con la dimensión territorial de la organización del Estado español, como el regionalismo y el terrorismo, también presentan cierto destaque.\nLa tecnología es vista como un motor de desarrollo. No obstante, la ciencia ocupa un rol marginal. En varios discursos, se trata de dar incentivos a empresas y atraer tecnologías desarrolladas en otros países más que crear un sistema de investigación robusto que permita la innovación desde España.\nEn relación a las categorías postmaterialistas, se observan dos patrones. El medioambiente conforma el tema con más peso y con un carácter más transversal. De una forma o de otra, todos los presidentes lo consideran un problema a atajar. No obstante, la diversidad sexual constituye un divisor de aguas. Aparecen con una frecuencia significativamente mayor en los discursos de los dos últimos presidentes socialistas y constituyen, de cierto modo, una marca de sus programas de gobierno.\n\n3.4.2 Temas por atributo\n¿Cómo distintos partidos mencionan un tema? ¿Y los presidentes? En muchas ocasiones, el punto central del análisis consiste en comparar cómo los temas varían según un atributo cualquiera como, por ejemplo, la ideología, el tiempo, o distintas regiones o países.\nEn algunos casos, interesa desagregar los datos generales por atributo y examinar cómo los patrones varían según cada valor o grupo. En otros, el objetivo consiste el filtrar o seleccionar algunos valores para explorarlos en profundidad. La capacidad de manipulación de datos representa uno de los puntos fuertes de R. Resulta muy sencillo realizar búsquedas y selecciones de datos a partir de criterios lógicos. Por esa razón, emplear tales capacidades en favor de un análisis de datos más detallado consiste en algo sencillo.\nEl panel abajo desagrega los datos presentados anteriormente por partidos y por presidente, así como filtra los resultados solo para el código “postmaterialismo”. En las pestañas tabla, se presentan las frecuencias desagregadas por ambas variables y, en las pestañas Sankey, se presentan los datos en un diagrama aluvial conocido como diagrama de Sankey (A. B. W. Kennedy and Sankey 1898; Riehmann, Hanfler, and Froehlich 2005).\n\n\nTabla: Partidos\nSankey: Partidos\nTabla: Presid.\nSankey: Presid.\nFiltro\n\n\n\nLa tabla abajo presenta las frecuencias relativas desagregadas por partido de cada código y expresión contenida en el diccionario dic.pol.es. Como en los ejemplos anteriores, se ha empleado el corpus de los discursos de investidura de los presidentes de gobierno de España. La única diferencia con el ejemplo anterior está en el uso del parámetro group.var=“Partido” en la función countKeywords, que establece que los resultados ahora deben ser desagrupados por el partido político del presidente.\n\nCódigo# Carga el paquete quanteda\nlibrary(quanteda)\n\n# Añade la variable partido al corpus cp\n# que hemos creado anteriormente\ndocvars(cp, \"Partido\") &lt;- c(\"UCD\", \"UCD\", \"PSOE\",\n                            \"PSOE\", \"PSOE\", \"PSOE\",\n                            \"PP\", \"PP\", \"PSOE\",\n                            \"PSOE\", \"PP\", \"PP\",\n                            \"PP\", \"PSOE\", \"PSOE\")\n\n# Obtiene la frecuencia relativa de los\n# términos contenidos en el diccionario\n# dic.pol.es desagregados por la variable\n# partido.\nxp &lt;- countKeywords(cp, \n                    dic.pol.es, \n                    rel.freq = T, \n                    group.var = \"Partido\",\n                    quietly = TRUE)\n\n# Agrega los resultados por los dos niveles\n# de código del diccionario\nxx &lt;- aggregate(list(frequency=xp$frequency), \n                 by=list(groups=xp$groups, \n                         level1=xp$level1,\n                         level2=xp$level2), \n                 sum, na.rm=T)\n \n# Elimina los términos no encontrados\n# en el corpus\nxx &lt;- xx[xx$frequency&gt;0,]\n\n# Multiplica la frecuencia relativa por mil\n# para facilitar la visualización de los\n# valores.\nxx$frequency &lt;- round(xx$frequency*1000,2)\n\n# Visualiza los resultados\nreactable(xx, \n          filterable = T,\n          resizable = T)\n\n\n\n\n\n\n\n\n\n\n\nEl diagrama de Sankey abajo representa los resultados de la tabla anterior. Cada barra de la izquierda corresponde a un partido y de la derecha a un código del diccionario. Los vínculos de cada partido a cada categoría se hacen visible cuando se pasa el cursor sobre una de las barras. Si el cursor está sobre un partido, se muestran sus vínculos con todas las categorías. Si, por otra parte, se pone sobre una categoría, se señalan todos los partidos y la intensidad con la que se vinculan.\n\nCódigo# Agrega las frecuencias relativas según\n# el grupo (partido) y el segundo nivel \n# de códigos (más detallado)\nxx &lt;- aggregate(list(frequency=xp$frequency), \n                 by=list(groups=xp$groups,\n                         level2=xp$level2), \n                 sum, na.rm=T)\n\n# Elimina los términos no encontrados\n# en el corpus\nxx &lt;- xx[xx$frequency&gt;0,]\n\n# Multiplica la frecuencia relativa por mil\n# para facilitar la visualización de los\n# valores.\nxx$frequency &lt;- round(xx$frequency*1000,2)\n\n# Genera el gráfico\nplotSankey(xx, \n           from = \"groups\", \n           to=\"level2\", \n           value = \"frequency\", \n           opacity = 0.05)\n\n\n\n\n\n\n\n\nSi pasamos el cursor sobre el código “democracia”, por ejemplo, vemos que la UCD contiene el mayor número de menciones. Se trata de algo absolutamente esperado, puesto que los presidentes de este partido (Adolfo Suárez y Leopoldo Calvo-Sotelo) han sido los primeros a ocupar el cargo durante la transición a la democracia. Si consideramos los términos “España” y “empresas” vemos que el PP, a su vez, contiene un mayor protagonismo, aunque en el último caso, su preponderancia resulta modesta frente a los demás grupos políticos.\n\n\nEn este caso, los datos se desagregan por presidente.\n\nCódigo# Obtiene la frecuencia relativa de los\n# términos contenidos en el diccionario\n# dic.pol.es desagregados por la variable\n# President.\nxz &lt;- countKeywords(cp, \n                    dic.pol.es, \n                    rel.freq = T, \n                    group.var = \"President\",\n                    quietly = TRUE)\n\n# Agrega los resultados por: los grupos\n# (cada uno de los presidentes) y los dos \n# niveles de código del diccionario\nxx &lt;- aggregate(list(frequency=xz$frequency), \n                 by=list(groups=xz$groups, \n                         level1=xz$level1,\n                         level2=xz$level2), \n                 sum, na.rm=T)\n\n# Elimina los términos no encontrados\n# en el corpus\nxx &lt;- xx[xx$frequency&gt;0,]\n\n# Multiplica la frecuencia relativa por mil\n# para facilitar la visualización de los\n# valores.\nxx$frequency &lt;- round(xx$frequency*1000,2)\n\n# Visualiza los resultados\nreactable(xx, \n          filterable = T,\n          resizable = T)\n\n\n\n\n\n\n\n\n\n\n\nEl mismo diagrama, pero ahora desagregado por presidente.\n\nCódigo# Agrega las frecuencias relativas según\n# el grupo (presidentes) y el segundo nivel \n# de códigos (más detallado)\nxx &lt;- aggregate(list(frequency=xz$frequency), \n                 by=list(groups=xz$groups,\n                         level2=xz$level2), \n                 sum, na.rm=T)\n \n# Elimina los términos no encontrados\n# en el corpus\nxx &lt;- xx[xx$frequency&gt;0,]\n\n# Multiplica la frecuencia relativa por mil\n# para facilitar la visualización de los\n# valores.\nxx$frequency &lt;- round(xx$frequency*1000,2)\n\n# Genera el gráfico\nplotSankey(xx, \n           from = \"groups\", \n           to=\"level2\", \n           value = \"frequency\", \n           opacity = 0.05)\n\n\n\n\n\n\n\n\nAquí vemos cómo cada presidente utiliza los términos. Resulta muy llamativo el uso de expresiones relacionadas a “España” por Mariano Rajoy, “genero” por Pedro Sánchez o “fiscal” por Aznar. Tales códigos les destacan frente a los demás y nos permiten identificar las características de sus discursos que les singularizan.\n\n\nFiltrado de valores\nTambién podemos filtrar los valores para centrar la atención a una categoría o código específico. En algunos casos, como género o medioambiente, por ejemplo, resulta difícil ver las diferencias en un gráfico dada su pequeño peso frente a otras categorías más frecuentes. En el ejemplo abajo, seleccionamos solamente los códigos de segundo nivel relacionados al “postmaterialismo”, es decir, cuestiones de género, medioambiente y memoria histórica.\n\nCódigo# Filta el resultado de los presidentes\n# para mantener solo los valores relativos\n# a la categoría \"postmaterialismo\"\nx1 &lt;- xz[xz$level1==\"postmaterialismo\",]\n\n# Agrega las frecuencias relativas según\n# el grupo (presidentes) y el segundo nivel \n# de códigos (más detallado)\nxx &lt;- aggregate(list(frequency=x1$frequency), \n                 by=list(groups=x1$groups,\n                         level2=x1$level2), \n                 sum, na.rm=T)\n \n# Elimina los términos no encontrados\n# en el corpus\nxx &lt;- xx[xx$frequency&gt;0,]\n\n# Multiplica la frecuencia relativa por mil\n# para facilitar la visualización de los\n# valores.\nxx$frequency &lt;- round(xx$frequency*1000,2)\n\n# Genera el gráfico\nplotSankey(xx, \n           from = \"groups\", \n           to=\"level2\", \n           value = \"frequency\", \n           opacity = 0.05)\n\n\n\n\n\n\n\n\nComo vemos, en los temas postmateriales hay un predominio de presidentes de gobierno del PSOE, en especial Zapatero y Sánchez. No obstante, en algunos temas como la memoria histórica y el medioambiente, presidentes de otros partidos también aparecen con menciones, aunque en menor grado.\n\n\n\n\n3.4.3 Redes temáticas\n¿Qué códigos siempre se mencionan juntos? ¿Qué otros nunca aparecen en una misma frase, párrafo o documento? El análisis de la asociación entre categorías constituye otro recurso muy útil para identificar patrones en los textos y facilitar el análisis del contenido de los mismos. Dicha tarea constituye el núcleo del desarrollo de redes temáticas, construidas a partir de la abstracción de códigos hacia conjuntos interrelacionados de temas (Attride-Stirling 2001). En los discursos políticos términos como democracia o libertad tienden a estar asociados a otras expresiones que les califican y permiten asignar una posición ideológica concreta. Por ese motivo, el análisis de las redes de asociación temática permiten avanzar aún más en la comprensión de los patrones existentes en el contenido de los documentos que componen un corpus.\nEl panel abajo trabaja con dos niveles. El primero examina la relación entre códigos de más alto nivel como actores, instituciones, política exterior o fiscal. El segundo baja un escalón y trata de los códigos menos abstractos como laboral, europa, retorica, genero. Para cada nivel los datos se muestran tanto bajo la forma de una tabla con los términos y el número de veces que aparecen juntos como en un diagrama de cuerdas (chord diagram) que permite la visualización de redes cuyos nodos se encuentran densamente asociados entre sí (Bremer and Wu 2012).\n\n\nTabla (nivel 1)\nCuerdas (nivel 1)\nTabla (nivel 2)\nCuerdas (nivel 2)\n\n\n\nLa tabla abajo ha sido producida a partir de la función matchCodes que examina la coocurrencia de los códigos de un diccionario determinado en un corpus. Aquí se emplean los discursos de investidura organizados según sentencias y se busca mapear la asociación entre los códigos de más alto nivel del diccionario dic.pol.es. El resultado es un data.frame con tres columnas: term1, correspondiente al primer término, term2, representando el segundo código, y value, que contiene el número de veces en que esas dos categorías aparecen en una misma unidad textual del corpus (sentencia, párrafo o documento entero).\n\nCódigo# Reorganiza el corpus según\n# sentencias o frases\ncs &lt;- corpus_reshape(cp, \"sentences\")\n\n# Calcula la frecuencia en la\n# que dos codigos del mismo \n# diccionario aparecen juntos\n# en cada frase\nd1 &lt;- matchCodes(cs, \n                dic.pol.es, \n                level = 1, \n                quietly=TRUE)\n\n# Ordena los resultados de mayor a menor\nd1 &lt;- d1[order(d1$value, decreasing = T),]\n\n# Visualiza los resultados\n# Obs.: En esta versión el código resulta\n# más largo porque incluímos un gráfico de\n# barras en la tabla. Si no quisiéramos ver\n# el gráfico bastaría con el código:\n# &gt; reactable(d1)\n\nlibrary(htmltools)\n\n# Crea una función que transformará\n# los valores de frecuencia en \n# barras a ser representadas en una\n# o más columnas de la tabla.\nbar_chart &lt;- function(label, \n                      width = \"100%\", \n                      height = \"1rem\", \n                      fill = \"purple\", \n                      background = NULL) {\n  \n  bar &lt;- div(\n          style = list(\n                    background = fill, \n                    width = width, \n                    height = height)\n          )\n  \n  chart &lt;- div(\n          style = list(\n                    flexGrow = 1, \n                    marginLeft = \"0.5rem\", \n                    background = background), \n          bar)\n  \n  div(\n    style = list(\n            display = \"flex\", \n            alignItems = \"center\"), \n    label, \n    chart)\n}\n\n# Visualiza los resultados\nreactable::reactable(\n            d1, \n            resizable = T,\n            filterable = T,            \n            columns = list(\n                        value = colDef(\n                        name = \"value\", \n                        align = \"left\", \n                        cell = function(value) {\n                            width &lt;- paste0(\n                                      value / max(d1$value) * 100, \n                                      \"%\")\n                            bar_chart(value, width = width)\n                            }\n                          )\n                        )\n            )\n\n\n\n\n\n\n\n\n\nAl examinar los resultados, la díada más frecuente corresponde a discurso-instituciones, con 746 ocurrencias, seguida de discurso-exterior, con 489, y discurso-social, con 358. Las menos frecuentes son instituciones-postmaterialismo, con 4 ocurrencias, y defensa-postmaterialismo, con 5.\n\n\nEl diagrama de cuerdas abajo revela el patrón en su conjunto, algo más difícil de observar solo por el examen de la tabla anterior. Además de discurso, temas sociales, e instituciones son los que más se asocian entre sí y con las demás categorías. Postmaterialismo, defensa y tecnología los que menos.\n\nCódigo# Genera el gráfico\nplotChord(d1, \n          from = \"term1\", \n          to =\"term2\", \n          value= \"value\")\n\n\n\n\n\n\n\nLa tabla abajo repite la operación, pero ahora para las categorías de segundo nivel. Ahora, la díada parlamento-retorica predomina, con 344 apariciones. España-retorica viene en segundo lugar, con 299 ocurrencias. Se tratan claramente de referencias al mismo Congreso de los Diputados y a los españoles y a España. Tales asociaciones corresponden a lo que ya hemos visto en análisis anteriores.\n\nCódigo# Reordena el corpus según sentencia\n# o frase.\ncs &lt;- corpus_reshape(cp, \"sentences\")\n\n# Calcula las coocurrencias, pero ahora\n# para el segundo nivel del diccionario\nd2 &lt;- matchCodes(cs, \n                dic.pol.es, \n                level = 2, \n                quietly=TRUE)\n\n# Ordena de los mayores a menores valores\nd2 &lt;- d2[order(d2$value, decreasing = T),]\n\n\n# Visualiza los resultados\n# Obs.: En esta versión el código resulta\n# más largo porque incluímos un gráfico de\n# barras en la tabla. Si no quisiéramos ver\n# el gráfico bastaría con el código:\n# &gt; reactable(d1)\n\nlibrary(htmltools)\n\n# Crea una función que transformará\n# los valores de frecuencia en \n# barras a ser representadas en una\n# o más columnas de la tabla.\nbar_chart &lt;- function(label, \n                      width = \"100%\", \n                      height = \"1rem\", \n                      fill = \"purple\", \n                      background = NULL) {\n  \n  bar &lt;- div(\n          style = list(\n                    background = fill, \n                    width = width, \n                    height = height)\n          )\n  \n  chart &lt;- div(\n          style = list(\n                    flexGrow = 1, \n                    marginLeft = \"0.5rem\", \n                    background = background), \n          bar)\n  \n  div(\n    style = list(\n            display = \"flex\", \n            alignItems = \"center\"), \n    label, \n    chart)\n}\n\n# Visualiza los resultados\nreactable::reactable(\n            d2, \n            resizable = T,\n            filterable = T,\n            columns = list(\n                        value = colDef(\n                        name = \"value\", \n                        align = \"left\", \n                        cell = function(value) {\n                            width &lt;- paste0(\n                                      value / max(d2$value) * 100, \n                                      \"%\")\n                            bar_chart(value, width = width)\n                            }\n                          )\n                        )\n            )\n\n\n\n\n\n\n\n\n\n\n\nEl diagrama de cuerdas abajo señala las relaciones entre las categorías de segundo nivel. Las categorías que más se vinculan a otras son retórica, España, administración y otros temas de carácter social. Las menos asociadas son género, policía, memoria histórica y medioambiente.\n\nCódigo# Genera el gráfico\nplotChord(d2, \n          from = \"term1\", \n          to =\"term2\", \n          value= \"value\")"
  },
  {
    "objectID": "02_explora_deductivo.html#consideraciones-finales",
    "href": "02_explora_deductivo.html#consideraciones-finales",
    "title": "\n3  Codificación temática\n",
    "section": "\n3.5 Consideraciones finales",
    "text": "3.5 Consideraciones finales\nEn esta parte hemos explorado el uso de estrategias de análisis que facilitan la selección, la codificación y el análisis temático de textos. Como ya ha sido mencionado de forma extensiva, no se trata de reemplazar métodos cualitativos tradicionales, sino de ofrecer entradas alternativas para el análisis de contenido. La mayoría de las técnicas presentadas aquí tienen en mente un contexto híbrido de investigación. De un lado, se desea mantener al máximo el rigor de un examen en profundidad del corpus. De otro, se quiere expandir el abanico de opciones disponibles para la identificación de patrones, su comunicación y el desarrollo de nuevas ideas.\nCabe subrayar que, aunque las estrategias desarrolladas aquí tengan en mente corpus con un número elevado de textos, las herramientas que se han presentado pueden emplearse también para una cantidad pequeña de documentos. Entrevistas, libros, cuadernos de campo, todos pueden ser objeto de la aplicación de los métodos discutidos aquí. Para un investigador acostumbrado a estudios cuantitativos, representa una oportunidad para examinar nuevas fuentes de información con mayor profundidad. Para los que se basan en métodos cualitativos, se abre la posibilidad de expandir sus instrumentos de trabajo con nuevas formas de visualización y exploración del material empírico con el que trabajan."
  },
  {
    "objectID": "02_explora_deductivo.html#ejercicios",
    "href": "02_explora_deductivo.html#ejercicios",
    "title": "\n3  Codificación temática\n",
    "section": "\n3.6 Ejercicios",
    "text": "3.6 Ejercicios\nEjercicio 1. Keyword in Context (kwic). Utilice la función kwic para buscar el término “igualdad” en el corpus de los discursos de investidura de los presidentes de gobierno españoles. Utilice una ventana de 7 palabras antes y después.\n\nSolución# Carga los paquetes tenet y quanteda\nlibrary(tenet)\nlibrary(quanteda)\n\n# Crea un corpus (discursos inaugurales Espana)\ncp &lt;- corpus(spa.inaugural)\n\n# Crea un data.frame a partir de\n# la funcion Keyword in Context de\n# Quanteda \nd &lt;- kwic(x = tokens(cp),\n          pattern= \"igualdad\",\n          window = 7)\n\n# Visualiza los resultados\nView(d)\n\n\nEjercicio 2. Árbol de palabras. Crea un árbol de palabras empleando el mismo corpus y la misma palabra del ejercicio anterior. Puedes intentarlo también con otras palabras de tu interés.\n\nSolución# Crea un arbol de palabras en tenet\nwordtree(corpus = cp,\n         keyword = \"igualdad\",\n         height = 800)\n\n# Palabra alternativa: empleo\n# (puede ser cualquiera que elijas)\nwordtree(corpus = cp,\n         keyword = \"empleo\",\n         height = 800)\n\n\nEjercicio 3. Etiquetado de textos. Seleccione el último discurso inaugural de Pedro Sánchez (texto 15) de la base de datos spa.inaugural y emplee las palabras clave “tecnol”, “ciencia”, “científ”, y “digital”.\n\nSolución# Examina un conjunto de palabras en el\n# discurso de investidura de Adolfo Suárez\ntagText(as.character(spa.inaugural$text[15]), \n        keywords = c(\"tecnol\", \n                     \"digital\", \n                     \"ciencia\", \n                     \"científ\"),\n        title = \"Pedro Sanchez (2019)\")\n\n\nEjercicio 4. Etiqueta do textos con diccionario. Repita el ejercicio anterior. No obstante, ahora, utilice el diccionario de la sección “Diccionarios como colecciones de códigos” para etiquetar el texto de Pedro Sánchez. Utilice la paleta de colors “FantasticFox1” para colorear las categorías, un tamaño de fuente de 20 y unos márgines de 120 pixeles.\n\nSolución# Crea el diccionario\ndic &lt;- dictionary(\n  list(\n    economica=c(\"econom\",\n               \"inversion\",\n               \"empresa\",\n               \"desarroll\",\n               \"monetari\",\n               \"industri\",\n               \"agric\",\n               \"agrari\"),\n    fiscal=c(\"hacienda\",\n               \"gasto\",\n               \"impuest\",\n               \"presupuest\",\n               \"tribut\",\n               \"tasa\",\n               \"fiscal\"),\n    educacion=c(\"educa\",\n             \"profesor\",\n             \"docent\",\n             \"escuel\",\n             \"colegio\",\n             \"universi\",\n             \"formación\"),\n    sanidad=c(\"sanidad\",\n               \"salud\",\n               \"hospital\",\n               \"sanitari\",\n               \"médic\",\n               \"enfermer\",\n               \"salud\"),\n    medioambiente=c(\"sostenible\",\n                 \"cambio clima\",\n                 \"medioambient\",\n                 \"reciclaje\",\n                 \"ecológico\",\n                 \"límpia\",\n                 \"invernadero\",\n                 \"emisiones\",\n                 \"carbono\",\n                 \"plástico\",\n                 \"fósiles\")))\n\n# genera un texto para ser leído en el panel\n# Viewer de RStudio\ntagText(spa.inaugural$text[15], \n        keywords = dic, \n        palette = pal$cat.wesanderson.FantasticFox1.5,\n        font.size = 20, \n        title = \"Pedro Sanchez (2019)\",\n        margin = 120)\n\n\nEjercicio 5. Etiquetado de corpus. Ahora emplee el mismo diccionario para etiquetar todo el corpus de discursos de investidura. Reorganice los documentos en párrafos (“paragraph”) en lugar de emplear documentos enteros.\n\nSolución# La función tagCorpus etiqueta\n# todo un corpus de acuerdo con un\n# diccionario\ntagCorpus(cp,\n          dic, \n          reshape.to = \"paragraph\")\n\n\nEjercicio 6. Etiquetado de corpus. Ahora repita el último ejercicio. Pero ahora crea un nuevo diccionario con los términos que más te interesen.\n\nSolución# Crea un nuevo diccionario, he elegido\n# algunas políticas públicas concretas\ndic.nuevo &lt;- dictionary(\n  list(\n    gobierno=c(\"gobierno\",\n               \"instituciones\",\n               \"comunidades\",\n               \"autonom\",\n               \"administracion\",\n               \"entidades\"),\n    politica=c(\"politica\",\n               \"partido\",\n               \"accion\",\n               \"cortes\"),\n    patria=c(\"patria\",\n             \"nacion\",\n             \"espanoles\",\n             \"pueblo\"),\n    justicia=c(\"constituci\",\n               \"justicia\",\n               \"judic\",\n               \"ley\",\n               \"legal\"),\n    financiero=c(\"financ\",\n                 \"hacienda\",\n                 \"presupuesto\",\n                 \"tributa\",\n                 \"fiscal\"),\n    equidad=c(\"desigual\", \n              \"igualdad\")))\n\n# Etiqueta los textos según el nuevo\n# diccionario\ntagCorpus(cp,\n          dic.nuevo, \n          reshape.to = \"paragraph\")\n\n\nEjercicio 7. Estadísticas temáticas. Utilice el diccionario empleado en el ejercicio 4 para contar la frecuencia de cada código en el corpus de discursos de investidura. Recuerda que debes utilizar la función countKeywords. No elimine los códigos con frecuencia igual a cero y utilice la frecuencia absoluta.\n\nSolución# Calcula la frecuencia ABSOLUTA en que cada\n# palabra ha sido encontrada para cada nivel\n# del diccionario dic\nxy &lt;- countKeywords(cp, \n                    dic, \n                    quietly = TRUE)\n\n# Visualiza los resultados\nreactable::reactable(xy, \n                     resizable = T,\n                     filterable = T)\n\n\nEjercicio 8. Diagrama de árbol. Utilice los resultados del ejercicio anterior para crear un diagrama de árbol (Force Directed Tree). No obstante, ahora elimine las palabras clave con frecuencia igual a cero. Utilice “level1” y “keywords” como grupos y “keywords” como elementos.\n\nSolución# Elimina los valores con frecuencia\n# igual a cero\nxy &lt;- xy[xy$frequency&gt;0,]\n\n# Formatea los datos para que puedan \n# ser representados en el gráfico\njs &lt;- jsonTree(data = xy, \n               groups=c(\"level1\",\"keyword\"), \n               elements = \"keyword\", \n               value=\"frequency\")\n\n# Genera el gráfico\nforceDirectedTree(js)\n\n\nEjercicio 9. Árbol de Voronoi. Utilice los resultados del ejercicio 7 para crear ahora un diagrama de Voronoi (Voronoi Treemap). No obstante, ahora elimine las palabras clave con frecuencia igual a cero. Utilice “level1” y “keywords” como grupos y “keywords” como elementos.\n\nSolución# Genera el gráfico\nplotVoronoiTree(data = xy,\n                groups = \"level1\",\n                elements = \"keyword\",\n                value = \"frequency\")\n\n\nEjercicio 10. Estadísticas temáticas: filtro. Repita la operación del ejercicio 7, pero ahora desagrega los resultados por presidente y emplea la frecuencia relativa.\n\nSolución# Calcula la frecuencia ABSOLUTA en que cada\n# palabra ha sido encontrada para cada nivel\n# del diccionario dic\nxy &lt;- countKeywords(cp, \n                    dic,\n                    group.var = \"President\",\n                    quietly = TRUE,\n                    rel.freq = T)\n\n# Visualiza los resultados\nreactable::reactable(xy, \n                     resizable = T,\n                     filterable = T)\n\n\nEjercicio 11. Diagrama aluvial de Sankey. Genere un diagrama de Sankey con los resultados del ejercicio anterior. No olvides de agregar los valores por grupo (“groups”) y por código (“level1”) y eliminar los códigos con frecuencia igual a cero.\n\nSolución# Agrega los resultados por presidente y\n# por código del diccionario\nxx &lt;- aggregate(list(frequency=xy$frequency), \n                 by=list(groups=xy$groups,\n                         level1=xy$level1), \n                 sum, na.rm=T)\n\n# Elimina los términos sin correspondencia\n# en el corpus\nxx &lt;- xx[xx$frequency&gt;0,]\n\n# General el gráfico \nplotSankey(xx, \n           from = \"groups\", \n           to=\"level1\", \n           value = \"frequency\")\n\n\nEjercicio 12. Coocurrencia de códigos. Reorganice el corpus en el nivel de párrafos. Calcule las coocurrencias de los códigos del diccionario dic.\n\nSolución# Reorganiza el corpus según párrafos\ncs &lt;- corpus_reshape(cp, \"paragraph\")\n\n# Calcula las coocurrencias\nd1 &lt;- matchCodes(cs, \n                dic, \n                level = 1, \n                quietly=TRUE)\n\n# Ordena las frecuencias de mayor a menor\nd1 &lt;- d1[order(d1$value, decreasing = T),]\n\n# Visualiza los resultados\nView(d1)\n\n\nEjercicio 13. Diagrama de cuerdas. Utilice los resultados del ejercicio anterior para crear un diagrama de cuerdas que permita visualiza las conexiones entre los distintos términos del diccionario dic aplicado al corpus de discursos de investidura.\n\nSolución# Visualiza los resultados\nplotChord(d1, \n          from = \"term1\", \n          to =\"term2\", \n          value= \"value\")"
  },
  {
    "objectID": "02_explora_deductivo.html#lecturas-adicionales",
    "href": "02_explora_deductivo.html#lecturas-adicionales",
    "title": "\n3  Codificación temática\n",
    "section": "\n3.7 Lecturas adicionales",
    "text": "3.7 Lecturas adicionales\n\n\nAttride-Stirling J (2001). “Thematic networks: an analytic tool for qualitative research.” Qualitative research, 1(3), 385-405.\n\n\nCon un número de 8.178 citas en Google Académico, el artículo de Attride-Stirling se ha convertido en referencia casi obligatoria en el proceso de codificación temática. Se destaca por presentar los temas no como ideas sueltas creadas a partir de códigos, sino como redes jerárquicas que van desde los referentes empíricos (en nuestro caso los textos) hasta el tema central del estudio.\n\n\nKennedy BL, Thornberg R (2018). “Deduction, induction, and abduction.” In The SAGE handbook of qualitative data collection, chapter 4, 49-64. SAGE Publications, London.\n\n\nEl texto de Kennedy y Tornberg representa una excelente introducción a las lógicas inductiva, deductiva y abductiva. Su consulta resulta muy recomendable, si deseas conocer más sobre las principales características y diferencias entre esos tipos de razonamiento o métodos de construcción de conocimiento.\n\n\nSaldana J (2015). The Coding Manual for Qualitative Researchers Third Edition. SAGE, Los Angeles ; London.\n\n\nEl libro de Saldaña corresponde a una guía detallada del proceso de codificación temática. No solo detalla los distintos tipos de código como sugiere algunas heurísticas para aplicarlos a los textos. Se trata de una referencia básica para el proceso de creación de códigos.\n\n\nThompson J (2022). “A guide to abductive thematic analysis.” The Qualitative Report, 5(27), 1410-1421.\n\n\nEl breve artículo de Thompson suministra una guía paso a paso de cómo emplear un razonamiento abductivo en el análisis temático en las ciencias sociales. Constituye una lectura interesante para aquellos que desean una orientación más detallada de cómo llevar a cabo el proceso de análisis de texto.\n\n\n\n\nAttride-Stirling, Jennifer. 2001. “Thematic Networks: An Analytic Tool for Qualitative Research.” Qualitative Research 1 (3): 385–405.\n\n\nAuerbach, Carl, and Louise B. Silverstein. 2003. Qualitative Data: An Introduction to Coding and Analysis. New York: NYU press.\n\n\nBalzer, Michael, and Oliver Deussen. 2005. “Voronoi Treemaps.” In IEEE Symposium on Information Visualization, 2005. INFOVIS 2005, 49–56. IEEE. https://doi.org/10.1109/INFVIS.2005.1532128.\n\n\nBremer, Nadieh, and Shirley Wu. 2012. Data Sketches: A Journey of Imagination, Exploration, and Beautiful Data Visualizations. New York: CRC Press.\n\n\nGuest, Greg, Kathleen M. MacQueen, and Emily E. Namey. 2011. Applied Thematic Analysis. Thousand Oaks, CA: SAGE Publications.\n\n\nHolten, Danny. 2006. “Hierarchical Edge Bundles: Visualization of Adjacency Relations in Hierarchical Data.” IEEE Transactions on Visualization and Computer Graphics 12 (5): 741–48.\n\n\nKennedy, A B W, and H R Sankey. 1898. “The Thermal Efficiency of Steam Engines. Report of the Committee Appointed to the Council Upon the Subject of the Definition of a Standard or Standards of Thermal Efficiency for Steam Engines: With an Introductory Note. (Including Appendixes and Plate at Back of Volume).” Minutes of the Proceedings of the Institution of Civil Engineers 134 (1898): 278–312. https://doi.org/10.1680/imotp.1898.19100.\n\n\nKennedy, Brianna L., and Robert Thornberg. 2018. “Deduction, Induction, and Abduction.” In The SAGE Handbook of Qualitative Data Collection, 49–64. London: SAGE Publications.\n\n\nKrippendorff, Klaus. 2004. Content Analysis: An Introduction to Its Methodology. Thousand Oaks, CA: Sage publications.\n\n\nMiles, Matthew B., A. Michael Huberman, and Johnny Saldana. 2019. Qualitative Data Analysis: A Methods Sourcebook. Thousand Oaks, CA: SAGE Publications.\n\n\nNeuendorf, Kimberly A. 2017. The Content Analysis Guidebook. Thousand Oaks, CA: SAGE Publications.\n\n\nRiehmann, Patrick, Manfred Hanfler, and Bernd Froehlich. 2005. “Interactive Sankey Diagrams.” In IEEE Symposium on Information Visualization, 2005. INFOVIS 2005., 233240. IEEE.\n\n\nSaldana, Johnny. 2015. The Coding Manual for Qualitative Researchers Third Edition. Los Angeles ; London: SAGE.\n\n\nThompson, Jamie. 2022. “A Guide to Abductive Thematic Analysis.” The Qualitative Report 5 (27): 1410–21.\n\n\nWattenberg, Martin, and Fernanda B. Viégas. 2008. “The Word Tree, an Interactive Visual Concordance.” IEEE Transactions on Visualization and Computer Graphics 14 (6): 12211228."
  },
  {
    "objectID": "03_analisis_sintactico.html#introducción",
    "href": "03_analisis_sintactico.html#introducción",
    "title": "\n4  Análisis sintáctico\n",
    "section": "\n4.1 Introducción",
    "text": "4.1 Introducción\n¿Qué adjetivos emplean los políticos para calificar la economía del país, el gobierno o la oposición? ¿Qué verbos prefieren para expresar sus ideas? ¿Qué entidades, partidos o personas mencionan y cómo? Para responder a estas preguntas hace falta ir más allá de solo dividir los textos en palabras. Resulta necesario entender las palabras en su contexto o diferencias entre un nombre propio de un nombre común, un adjetivo de un verbo. El análisis sintáctico permite situar las palabras en su contexto y separar aquellas cuyo significado resulta particularmente útil para entender un discurso político o diferencias de peso entre visiones ideológicas distintas.\nEl análisis sintáctico aplicado a la minería de textos se puede entender como el conjunto de técnicas que buscan, según reglas gramaticales, describir la estructura y función de las palabras en su contexto (Atkinson-Abutridy 2022, 29). De modo concreto, consiste en etiquetar cada elemento de una frase según su función gramática o su relación de dependencia. En esta parte, examinaremos tres técnicas muy empleadas: el etiquetado de partes del discurso (Parts-of-Speech o POS Tagging), el análisis de dependencias (dependency parsing) y el reconocimiento de entidades mencionadas (NER - Named Entity Recognition). La primera define si una palabra representa un artículo, un verbo, un nombre o cualquier otra unidad gramatical. La segunda establece si se trata del sujeto, verbo complementario, objeto directo, indirecto, es decir, establece su función dentro de la frase. Finalmente, la tercera extrae aquellas expresiones que representan personas, organizaciones u otras entidades mencionadas.\nImaginemos una frase como “El Partido X ofrece las mejores propuestas para la economía.” Según el POS Tagging sabemos que Partido, X, propuestas y economía son nombres y, además, que “mejores” corresponde a un adjetivo. El análisis de dependencias revela que “Partido X” es el sujeto, “propuestas”, el objeto directo y “economía” el objeto indirecto. Finalmente, el NER nos reconoce que “Partido X” trata de una organización.\nLa próxima sección se dedica a explorar cómo se puede emplear el etiquetado de partidos del discurso (POS Tagging) en textos políticos. El apartado siguiente trata del análisis de dependencias y es seguida por una parte sobre el reconocimiento de entidades. Finalmente, concluímos con la aplicación de los conocimientos aprendidos a un caso concreto."
  },
  {
    "objectID": "03_analisis_sintactico.html#etiquetado-de-partes-del-discurso-pos-tagging",
    "href": "03_analisis_sintactico.html#etiquetado-de-partes-del-discurso-pos-tagging",
    "title": "\n4  Análisis sintáctico\n",
    "section": "\n4.2 Etiquetado de partes del discurso (POS Tagging)",
    "text": "4.2 Etiquetado de partes del discurso (POS Tagging)\nEl proceso denominado etiquetado de partes del discurso (o POS tagging, en inglés) corresponde a un método de asignación de una etiqueta o categoría sintáctica a cada una de las palabras o símbolos de un texto (Welbers, Van Atteveldt, and Benoit 2017, 18; Grimmer, Roberts, and Stewart 2022, 160). Para ello, hace falta entender el lugar que ocupa y el significado semántico de cada palabra, puntuación, número o símbolo en la estructura del texto (Kumar and Paul 2016, 70). Por ejemplo, la palabra “como” puede significar la primera persona del verbo comer, un adverbio de modo o cantidad, una conjunción o incluso como preposición.\nEn la minería de textos, el etiquetado se lleva a cabo a partir de modelos de Procesamiento de Lenguaje Natural previamente entrenados por alguna organización o grupo de investigación para realizar tareas de clasificación. Como veremos en los ejemplos, las opciones disponibles aún se equivocan en algunas palabras, aunque ayudan y mucho a avanzar en la comprensión de los significados y contextos. El resultado suele corresponder a una tabla en la que cada observación corresponde a una palabra o símbolo y diversas columnas que identifican el documento original, el párrafo, la frase, la palabra, su lema. Y, por supuesto, define la etiqueta asignada: ADJ - adjetivo, ADV - adverbio, NOUN - nombre (sustantivo) común, PROPN - nombre (sustativo) propio, VERB - verbo, PUNCT - puntuación, NUM - número, SYM - símbolo, y así sucesivamente.\nEn este ejemplo, empleamos el paquete udpipe (Wijffels 2023), puesto que poseen un elevado porcentaje de acierto y adiciona una serie de informaciones complementarias al etiquetado como, por ejemplo, el tiempo verbal, el género, si un artículo es determinado o indeterminado, entre otras informaciones que pueden resultar muy útiles a la hora de establecer el estilo discursivo de un orador o partido. Otros paquetes como spacyr (Benoit and Matsuo 2020) o qdap (Rinker 2023) también poseen funciones que realizan la tarea de anotación de textos. Sin embargo, consideramos que los resultados a día de hoy resultan inferiores en comparación con los ofrecidos por los de udpipe en las tareas de POS tagging.\n\n4.2.1 Preparación del corpus y etiquetado\nEl primer paso resulta relativamente sencillo. Se trata de abrir los documentos (ya preparados durante la fase anterior de preprocesamiento) y convertirlos al formato de un corpus. Emplearemos aquí las intervenciones de los diputados de cada partido en el debate del proyecto de Ley Orgánica de Garantía Integral de la Libertad Sexual (la denominada Ley del Sí es Sí). Para ello, filtraremos los datos del conjunto de intervenciones de la XIV legislatura del Congreso de Diputados de España (2019 y 2023) y, luego, agregaremos los textos por partido. El objetivo consiste en averiguar cuáles son los grupos favorables o opositores a la medida y qué tipo de argumentación desarrollan.\nEl código abajo detalla la preparación del corpus para su posterior etiquetado y análisis:\n\nCódigo# Filtra el archivo de intervenciones para seleccionar:\n\n# 1) la Ley de la Libertad Sexual\nsi &lt;- spa.sessions[grep(\"121/000062\",spa.sessions$issue.details),]\n\n# 2) solo el debate entre diputados\n# 3) excluye la moderación y solo mantiene el posicionamiento\n#    de los distintos partidos\nsi &lt;- si[si$session.number==124 & \n         si$issue.type==\"Debate\" &\n         si$rep.condition!=\"Miembro de la mesa\",]\n\n# Agrega las intervenciones por partido político\nsi &lt;- aggregate(list(text=si$speech.text), \n                by=list(partido=si$rep.party),\n                paste0,\n                collapse=\"\\n\")\n\n# Crea un corpus con los resultados\nci &lt;- corpus(si)\ndocnames(ci) &lt;- si$partido\n\n\nUna vez preparado el corpus, podemos realizar el etiquetado. Para ello, debemos cargar el paquete udpipe, descargar el modelo previamente preparado para predecir las etiquetas gramaticales más adecuadas de acuerdo con la posición de la palabra en la estructura de las frases. El próximo paso consiste e cargar dicho modelo y generar una base de datos de cada palabra y sus correspondiente categoría. El código y los resultados se encuentran abajo:\n\nCódigo# Abre el paquete udpipe para realizar \n# el etiquetado de los textos\nlibrary(udpipe)\n\n# Descarga el modelo previamente entrenado\nm_es   &lt;- udpipe_download_model(language = \"spanish\")\n\n# Carga el modelo\nm_es &lt;- udpipe_load_model(m_es)\n\n# Genera las anotaciones y etiquetas\nd &lt;- udpipe_annotate(m_es, \n                     x=ci, \n                     doc_id = docnames(ci)) %&gt;%\n  as.data.frame() %&gt;%\n  dplyr::select(-sentence)\n\n# Visualiza los resultados\nreactable::reactable(d[,c(\"doc_id\",\n                          \"sentence_id\",\n                          \"token_id\",\n                          \"token\",\n                          \"lemma\",\n                          \"upos\",\n                          \"feats\")], \n                     sortable = T, \n                     resizable = T, \n                     filterable = T)\n\n\n\n\n\n\nComo se puede ver, el resultado es un data.frame que contiene un conjunto de atributos de los que hemos seleccionado algunos para reseñar. El campo doc_id identifica el documento al que pertence la palabra etiquetada. En nuestro ejemplo, se trata del partido de los diputados que han intervedido durante los debates de la ley.\nSegundo, sentence_id señala la frase en el orden que aparece en el texto. Lo mismo con token_id, pero ahora para las palabras; token se refiere al término concreto anotado, que puede ser una palabra, un símbolo o número. La columna lemma revela la forma básica o esencial de un término, como puede ser el infinitivo de un verbo, por ejemplo. Fiesta y beber representan los lemas de fiestas y bebieron.\nLa variable upos, a su vez, resulta la más importante, puesto que contiene la etiqueta gramatical. Suele ser representada bajo la forma de una sigla corta: ADJ, para adjetivo; NOUN, para nombre común; VERB, para verbo. Finalmente, feats contiene detalles de la clasificación, como género, número, tiempo verbal o tipo de artículo.\n\n4.2.2 Análisis de categorías aisladas\nUna vez que disponemos de dicha clasificación, podemos emplear distintas técnicas de análisis para examinar el contenido de los discursos. El primer acercamiento que proponemos consiste en mirar las palabras más frecuentes de acuerdo con tres categorías gramaticales centrales: nombres, adjetivos y verbos. Su importancia residen en que los sustantivos ayudan a identificar temas o ideas clave mencionadas por los actores políticos en sus intervenciones. Los adjetivos califican su uso y añaden contexto (positivo, negativo, elogioso o despectivo, por ejemplo). Los verbos por su propia naturaleza indican acción o condición y sirven como conectores entre sujeto y objeto en una frase.\nEl código abajo examina esas tres categorías en dos partidos que suelen presentar posturas antagónicas en el debate político español: Podemos y Vox. Para cada grupo político, filtra las etiquetas, agrega todas las palabras que han sido etiquetadas dentro alguna de esas tres categorías y selecciona las 20 más frecuentes. Finalmente, genera un gráfico de barras con el resultado.\n\nCódigo# Crea una lista con los partidos\n# que se desean visualizar\npartido&lt;- c(\"Podemos\",\"Vox\")\n\n# Crea una lista para guardar los\n# gráficos de cada partido y no\n# tener que repetir código\ngr &lt;- list()\n\n# para cada partido en la lista\nfor(i in 1:length(partido)){\n  \n  # Selecciona nombres, verbos y adjetivos \n  # de la base d que fue etiquetada en el\n  # paso anterior\n  adj &lt;- d[which(\n            d$upos%in%c(\"NOUN\",\n                        \"VERB\",\n                        \"ADJ\") \n            & d$doc_id==partido[i]),]\n\n  # Calcula la frecuencia de cada palabra\n  # por tipo\n  ag &lt;- aggregate(list(freq=adj$lemma),\n                by=list(Tipo=adj$upos,\n                        lemma=adj$lemma),\n                length)\n\n  # Selecciona los 20 más frecuentes\n  # de cada tipo\n  ag &lt;- ag |&gt; \n    dplyr::slice_max(\n                freq,\n                n=20, \n                by=Tipo, \n                with_ties = F)\n\n  # Altera las descripciones para\n  # facilitar el entendimiento del\n  # gráfico\n  ag$Tipo[ag$Tipo==\"ADJ\"] &lt;- \"Adjetivo\"\n  ag$Tipo[ag$Tipo==\"NOUN\"] &lt;- \"Nombre\"\n  ag$Tipo[ag$Tipo==\"VERB\"] &lt;- \"Verbo\"\n\n  # Crea el gráfico de barras\n  # para cada partido y tipo \n  # gramatical\n  library(ggplot2)\n  library(forcats)\n\n\n  p &lt;- ggplot(\n    ag, aes(x=freq, y=fct_reorder(lemma, freq), fill=Tipo))+\n    geom_bar(stat=\"identity\")+\n    theme_minimal()+\n    theme(panel.grid = element_blank(),\n          plot.title = ggtext::element_markdown(),\n          legend.position = \"none\")+\n    labs(title=paste0(\"**\",partido[i],\"**\"))+\n    ylab(\"\")+xlab(\"Menciones\")+\n    facet_wrap(~Tipo, scales = \"free\", )\n\n  # Guarda el gráfico de cada partido\n  # en la lista para su posterior\n  # visualización en un panel común\n  gr[[i]] &lt;- p\n\n}\n\n# Organiza los dos gráficos como uno\nlibrary(grid)\nlibrary(gridExtra)\n\ngrid.arrange(gr[[1]], gr[[2]])\n\n\n\n\nAl analizar los resultados, vemos una diferencia clara entre los dos partidos. Aunque los términos sexual y feminista aparezcan entre los más citados por las diputadas de ambas formaciones, los demás adjetivos que los acompañan son muy distintos. Mientras que Podemos emplea “víctima”, “machista”, “fundamental”, “institucional”, “internacional”, “necesario” y “humano”, Vox prefiere expresiones más despectivas como “perverso”, “irracional”, “falso”, “inútil” o “grave”.\nCon relación a los nombres, llaman la atención el uso de algunas palabras que separan las dos formaciones. Podemos emplea los términos “libertad”, “víctima”, “cambio”, “impunidad”, “explotación” y “cuerpo”, mientras que Vox utiliza “hombre”, “consentimiento”, “igualdad”, “odio”, “cárcel”, “agresor” y “feminismo”. Al volver a los textos y contextualizar tales palabras, encontramos la defensa de Podemos de una política institucional de protección de las mujeres frente a la violencia sexual, mientras que Vox se centra en los efectos negativos sobre los hombres, que son retratados como potenciales agresores por parte del movimiento feminista. Podemos subraya la violencia hacia las mujeres y Vox trata la ley como una forma de ataque hacia los derechos de los hombres. Entre los verbos, se destacan “garantizar”, “suponer” y acabar para Podemos y “importar”, “defender”, “mirar”, “disfrazar” y “arreglar” para Vox.\nEl tono general del discurso es siempre el mismo. Podemos defiende una ley necesaria para la protección de las mujeres contra la violencia sexual y machista. Vox, por otra parte, denuesta la iniciativa legal como una persecución facciosa del feminismo hacia los hombres. Si analizamos las posiciones de otros partidos, encontramos el apoyo de partidos de izquierda, como Esquerra Republicana de Catalunya (ERC) o el Bloque Nacionalista Galego (BNG). Otros grupos políticos prefieren un tono más técnico (mala calidad del texto legal) o neutral para evitar una crítica directa a la ley, como es el caso del Partido Popular (PP) o Ciudadanos (Cs). La posición concreta de estos últimos partidos solo podrá averiguarse a partir de los datos de votación, cuando tendrán que manifestarse sea en favor, en contra o abstenerse.\n\n4.2.3 Análisis de combinaciones gramaticales\nAunque el análisis de categorías gramaticales aisladas nos ayuden a entender mejor las posiciones políticas de cada actor, este efecto se ve multiplicado cuando empleamos combinaciones de etiquetas. ¿Cuando hablan de una propuesta legislativa mencionan una “ley necesaria” o una “ley inútil”? La combinación de nombres con adjetivos nos permite responder a esa pregunta.\nEl próximo paso, por lo tanto, consiste en analizar las combinaciones entre etiquetas gramaticales para caracterizar cómo los diferentes actores emplean el lenguaje para posicionarse frente a un problema político concreto o, en el ejemplo que empleamos aquí, una propuesta legislativa que trata de cohibir la violencia sexual. ¿Qué adjetivos suelen estar asociados a los nombres? ¿Qué nombres anteceden un verbo o vice-versa?\nPara responder a esa pregunta, hemos preparado una función que encuentra las palabras de dos etiquetas que se suceden en una frase y calculan su frecuencia e el texto. De ese modo, podemos calcular una matriz de coocurrencia y establecer su grado de conexión y qué combinaciones se emplean por diferentes actores políticos. El panel abajo explica paso a paso cómo hemos creado la función para identificar tales secuencias, la visualización de las principales combinaciones y enfoca la atención en algunas categorías.\n\n\nFunción\nPatrones: Gráfico de Cordas\nFiltro: Diagrama de Sankey\n\n\n\nDenominamos aquí como selPOSTags la función que extrae todas las combinaciones entre dos etiquetas gramaticales de todas las sentencias de un corpus. La función posee siete parámetros. El primero, data, indica la base de datos generada por la anotación de udpipe que vimos más arriba en la fase de preprocesamiento.\nEl segundo, tag_first, y el tercero, tag_second, definen el orden de las etiquetas a ser buscadas en el corpus. Si se definen como tag_first=“NOUN” y tag_second=“ADJ”, por ejemplo, la función retornará todas las combinaciones de palabras en que un nombre común es inmediamente seguido por un adjetivo. Si queremos que no importe el orden, podemos definir los parámetros como tag_first=c(“NOUN”,“ADJ”) y tag_second=c(“NOUN”,“ADJ”). Esta elección garantizará que cualquiera de las dos puede anteceder a la otra en las frases, sin importar el orden.\nEl cuarto argumento, docid, indica el documento que se empleará para filtrar los resultados. En nuestro ejemplo, hemos agregado las intervenciones por partido. De ese modo, si definimos docid=“Podemos”, la función hará el conteo solamente para este partido. Se trata de un parámetro fundamental que permite la comparación de los usos específicos de la gramática por parte de diferentes actores.\nEl argumento siguiente, source trata de qué información será utilizada en los análisis. Su función consiste en definir qué variable del data.frame anotado será empleada para contabilizar las frecuencias. Si se elige “lemma”, como está predeterminado, se calculará el total de formas infinitivas o básicas de las palabras. Por ejemplo, “creemos”, “creen”, “creyeron” se transforman en el lemma “creer” y se contabilizan como tres observaciones. Esta opción resulta preferible si queremos saber el peso general de los término. Si, por otra parte, preferimos el token, cada una de esas palabras aparecería como una ocurrencia. Sin embargo, permite revelar nuanzas o usos específicos que los lemas ocultarían por la estandarización previa.\nFinalmente, exclude.dyads y n.words definen qué información se retornará a los usuarios, una vez calculadas las frecuecias. El primero define que una combinación aislada de términos que aparecen juntos una sola vez sean excluidos de los resultados. Esta opción resulta útil en el caso de que deseemos visualizar tales informaciones en un gráfico. Lo mismo sucede con n.words, que establece el número máximo de observaciones que retornen al usuario.\nEl resultado final será un objeto de tipo data.frame con tres columnas: from, to, y value. La primera contiene las palabras correspondientes a la etiqueta definida en tag_first. La segunda informa los términos clasificados bajo la categoría de tag_second. La variable final, value, contiene la frecuencia en la que esas dos palabras aparecen juntas.\nEl código abajo detalla cómo se ha construido la función. ¿Se podría mejorar? ¿Cómo?\n\nCódigo# Función selPOSTags - selecciona dos etiquetas que\n# aparecen una luego de la otra en una frase\nselPOSTags &lt;- function(data, \n                       tag_first, \n                       tag_second, \n                       docid=NULL, \n                       source=\"lemma\",\n                       exclude.dyads=TRUE,\n                       n.words=NULL){\n\n  # retira las tildes y convierte a\n  # minúsculas los tokens o lemas \n  data[[source]] &lt;- tolower(\n                      stringi::stri_trans_general(\n                        data[[source]], \n                        \"ascii\"))\n  \n  # Unifica las etiquetas para filtrar\n  # los datos\n  tags &lt;- c(tag_first,tag_second)\n  \n  # filta los datos\n  dx &lt;- data[data$upos%in%tags,]\n  \n  # Si se define un texto o documento\n  # selecciona solo los elementos de ese\n  # documento \n  if(!is.null(docid)){\n    dx &lt;- dx[dx$doc_id==docid,]\n  }\n  \n  # genera columnas para:\n  # etiqueta inmediatamente siguiente\n  # término inmediatamente siguiete\n  # número de la setencia del término \n  # inmediatamente siguiente\n  dx$pos_next &lt;- c(dx$upos[2:nrow(dx)],NA)\n  dx$term_next &lt;- c(dx[[source]][2:nrow(dx)],NA)\n  dx$sen_next &lt;- c(dx$sentence_id[2:nrow(dx)],NA)\n  \n  # Mantiene solo combinaciones que se \n  # encuentren en una misma frase\n  dx &lt;- dx[dx$sentence_id==dx$sen_next,]\n  \n  # Selecciona las combinaciones de \n  # etiquetas (first=ADJ es second=NOUN, \n  # por ejemplo). Se pueden utilizar más\n  # de un tag de cada lado (eso aumenta \n  # las configuraciones posibles y hace\n  # el análisis más complejo).\n  dx &lt;- dx[\n            dx$upos%in%tag_first & \n            dx$pos_next%in%tag_second,]\n  \n  # Crea un contador para averiguar la\n  # frecuencia y agrega por la combinacion\n  # entre terminos  \n  dx$count &lt;- 1\n  ag &lt;- aggregate(list(value=dx$count), \n                  by=list(from=dx[[source]],\n                          to=dx$term_next),\n                  sum)\n\n  # Cuenta el número de veces que cada\n  # termino aparece. Esto resulta vital\n  # para eliminar las díadas aisladas\n  d1 &lt;- data.frame(table(ag$from))\n  names(d1) &lt;- c(\"from\",\"ffrm\")\n  d2 &lt;- data.frame(table(ag$to))\n  names(d2) &lt;- c(\"to\",\"fto\")\n  ag &lt;- merge(ag, d1, by=\"from\")\n  ag &lt;- merge(ag, d2, by=\"to\")\n\n  # Define cuántas veces aparecen las\n  # dos palabras juntas. Si menos de \n  # dos, significa que son díadas\n  # aisladas\n  ag$order &lt;- (ag$ffrm+ag$fto/2)\n  \n  # Si queremos excluirlas, aquí se hace \n  if(exclude.dyads==TRUE){\n    ag &lt;- ag[ag$order&gt;2,]\n  }\n  \n  # Ordena según la frecuencia de forma\n  # descendiente\n  ag &lt;- ag[order(ag$order,\n                 ag$value, \n                 decreasing = T),]\n  \n  # Si no se establece un número\n  # máximo de palabras a retornar,\n  # selecciona todas.\n  if(is.null(n.words)){\n    n.words &lt;- nrow(ag)\n  }else if(n.words&gt;nrow(ag)){\n    n.words &lt;- nrow(ag)\n  }\n  \n  # Realiza la selección\n  ag &lt;- ag[1:n.words,\n           c(\"from\", \"to\",\"value\")]\n\n  # Retorna los resultados\n  # al usuario\n  return(ag)\n}\n\n\n\n\nUna vez que ya tenemos la función adecuada para generar los datos de coocurrencia entre categorías gramaticales, resulta interesante visualizarlos. Como se encuentran bajo el formato desde-hacia-valor, podemos emplear algunos de los gráficos que ya hemos empleado en ejemplos anteriores para visualizar su interconexión. El diagrama de cordas puede ser una buena opción.\n\nCódigo# Emplea la función que acabamos de definir\n# para seleccionar las 100 principales\n# combinaciones entre nombres y adjetivos\nag &lt;- selPOSTags(d, \n                 n.words = 100,\n                 tag_first = c(\"NOUN\"), \n                 tag_second = \"ADJ\")\n\n# Visualizamos los resultados\nlibrary(tenet)\n\nplotChord(ag)\n\n\n\n\n\nNo resulta para nada sorprendente que los términos que más aparezcan asociados sean sexual, violencia, mujer, libertad, derecho, ley y agresión. Se trata de palabras directamente vinculadas a la propuesta legislativa. No obstante, encontramos asociaciones interesantes al examinar qué expresiones se vinculan a ellas.\nCon relación a sexual, abundan referencias a violencia, libertad, agresión, explotación o abuso sexual. Otras referencias también revelan temas adicionales cubiertos por la ley, como el trabajo sexual, el tema del consentimiento, la orientación sexual, entre otros.\nTambién resulta revelador mirar los adjetivos asociados a mujer: víctima, buena, sensata, miente (este es un ejemplo de error del modelo), malo, violado, migrante, viejo, desempleado, discapacitado. Llaman atención también las expresiones derecho feminista, humano y fundamental.\nEn cuanto a la ley, existe un abanico más amplio de posiciones: sensata, ambiciosa, pionera, pero también, infame, fangosa, ideológica, maniquea, inútil o imposible. Indican una clara divergencia de posiciones sobre la propuesta, pues reúnen tanto elogios como injurias.\nAdemás, algunos elementos de política pública aparecen cuando examinamos el término social, que se asocia con pacto, red, asistencia, justicia, pero también con daño, situación, y altura.\n\n\nPodemos filtrar los resultados para incluir solo a un partido o algunos de los términos. Esto facilita el examen de aspectos concretos del discurso. En el ejemplo abajo, visualizamos las conexiones entre verbos y nombres empleados por las diputadas de Vox con relación a “hombre” y “mujer”. Se queda claro el retrato que hacen de cada uno. Algunos verbos que anteceden a hombre, es decir, que tienen a los hombres como objeto de la acción: acusar, señalar, demonizar, culpabilizar y criminalizar. Tales asociaciones refuerzan la idea de los hombres como víctimas de una persecución por parte de la ley que se pretende aprobar. Por otro lado, las a las mujeres se les asocia: respetar, proteger, entender, desagradar, creer, contemplar o considerar. ¿Qué vemos si aplicamos el mismo filtro, pero para otros partidos, como Podemos, el PSOE o el PP, por ejemplo? Y, ¿qué pasaría si invirtiéramos la posición de objeto a sujeto de la acción? Es decir, ¿cuáles serían los resultados si los nombres vinieran primero y luego los verbos?\n\nCódigoag &lt;- selPOSTags(d, \n                 docid = \"Vox\", \n                 exclude.dyads = F,\n                 source = \"lemma\",\n                 tag_first = c(\"VERB\"), \n                 tag_second = \"NOUN\")\n\nag &lt;- ag[ag$to%in%c(\"hombre\",\"mujer\"),]\n\n\nag &lt;- ag[order(ag$from, ag$value, decreasing = T),]\n\nlibrary(tenet)\n\nplotSankey(ag)\n\n\n\n\n\n\n\n\nEn el ejemplo arriba, hemos empleado una secuencia de dos etiquetas. No obstante, algunos autores (Handler et al. 2016) señalan la utilidad de emplear cadenas semánticas más largas como, por ejemplo, artículos + (nombres o adjetivos) + verbos + artículos + (nombres o adjetivos). Su principal ventaja proviene de la mayor facilidad de interpretación de las palabras en su contexto. Una vez seleccionados los términos o expresiones, podemos emplearlos como referencia para análisis como o keyness o simplemente cual la probabilidad de su aparición en un texto del partido A frente al partido B (Monroe, Colaresi, and Quinn 2008).\n\nCódigo# Añade una clasificación de las\n# etiquetas que permiten seleccionar\n# frases secuenciales\nd$phrase_tag &lt;- as_phrasemachine(\n                          d$upos, \n                          type = \"upos\")\n\n# Seleccionamos el texto de Vox\nd1 &lt;- d[d$doc_id==\"Vox\",]\n\n# Utilizamos expresiones regulares y\n# Buscamos un patrón completo (en orden):\n# 1) Uno o más: P - adposición (en, con, a) o\n#               D - artículo\n# 2) Un: A - Adjetivo o N - Nombre\n# 3) Uno o más: V - verbos\n# 4) Una: P - adposición (en, con, a) o\n#         D - artículo\n# 5) Uno o más: A - Adjetivo o N - Nombre\npat &lt;- \"(P|D)*(A|N)(V+)(P|D)(A|N)*\"\n\n# Encontramos la frase y calculamos\n# la frecuencia\nstats &lt;- keywords_phrases(x = d1$phrase_tag, \n                          term = tolower(d1$token), \n                          pattern = pat, \n                          is_regex = TRUE, \n                          detailed = FALSE)\n\n# Visualizamos los resultados\nreactable::reactable(stats,\n                     resizable = T, \n                     filterable = T)"
  },
  {
    "objectID": "03_analisis_sintactico.html#análisis-de-dependencias",
    "href": "03_analisis_sintactico.html#análisis-de-dependencias",
    "title": "\n4  Análisis sintáctico\n",
    "section": "\n4.3 Análisis de dependencias",
    "text": "4.3 Análisis de dependencias\n“A dependency parser converts each sentence into a graph where each node represents a word, and the edges express grammatical dependency relations between the nodes.” (Van Atteveldt et al. 2017, 210)\n“Dependency parsing is the process of extracting a directed graph that explains the way words and phrases are syntactically connected to each other within a sentence (Jursfsky and Martin, 2009, Chapter 14). Dependency parsing identifies a root word (often the primary verb) and then describes the other words in relation to that word and to each other.” (Grimmer, Roberts, and Stewart 2022, 166)"
  },
  {
    "objectID": "03_analisis_sintactico.html#reconocimiento-de-nombres-y-entidades",
    "href": "03_analisis_sintactico.html#reconocimiento-de-nombres-y-entidades",
    "title": "\n4  Análisis sintáctico\n",
    "section": "\n4.4 Reconocimiento de nombres y entidades",
    "text": "4.4 Reconocimiento de nombres y entidades\n“Named entity recognition is a technique for identifying whether a word or sequence of words represents an entity and what type of entity, such as a person or organization.” (Welbers, Van Atteveldt, and Benoit 2017, 18)\n“NER is the process of tagging people, organizations, and places within texts” (Grimmer, Roberts, and Stewart 2022, 164)\n“Named entity recognition helps identify the important entities in a text, to be able to derive the meaning from the unstructured data.” (Kumar and Paul 2016, 199)\n“Entity recognition is a sub process in the chain of information extraction process. NER is one of the important and vital parts of the information extraction process. NER is sometimes also called entity extraction or entity chunking .The main job of NER is to extract the rigid designators in the document and classify these elements in the text to a predefined category.” (Kumar and Paul 2016, 200)"
  },
  {
    "objectID": "04_redes.html#introducción",
    "href": "04_redes.html#introducción",
    "title": "\n5  De textos a redes de política\n",
    "section": "\n5.1 Introducción",
    "text": "5.1 Introducción\n¿Qué obtenemos al considerar los textos como un conjunto de redes?\nIntroducción"
  },
  {
    "objectID": "04_redes.html#análisis-de-redes-sociales-aplicados-a-textos-políticos",
    "href": "04_redes.html#análisis-de-redes-sociales-aplicados-a-textos-políticos",
    "title": "\n5  De textos a redes de política\n",
    "section": "\n5.2 Análisis de redes sociales aplicados a textos políticos",
    "text": "5.2 Análisis de redes sociales aplicados a textos políticos\nTeoría o manuales\n(Wasserman and Faust 1994)\n(Knoke and Kuklinski 1982)\n(Scott 1991, 2012)\n(Crossley et al. 2015)\n(Borgatti, Everett, and Johnson 2018)\nRedes políticas\n(Yang, Keller, and Zheng 2016, cap. 8; Knoke 1990)\n\nCódigolibrary(quanteda)\nlibrary(tenet)\n\ncp &lt;- corpus(spa.inaugural)\n\n# Reorganiza el corpus según\n# sentencias o frases\ncs &lt;- corpus_reshape(cp, \"sentences\")\n\nd1 &lt;- matchCodes(cs, \n                 dic.pol.es, \n                 level = 2, \n                 quietly=TRUE)\n\n# Ordena los resultados de mayor a menor\nd1 &lt;- d1[order(d1$value, decreasing = T),]\n\nplotSolar(\n          d1, \n          from = \"term1\", \n          to = \"term2\", \n          value = \"value\",\n          title = \"**Centralidad de temas en los discursos de investidura&lt;br&gt;de los presidentes de gobierno de España (1979-2019)**\",\n          subtitle = \"Nivel de centralidad -según coocurrencias- (órbitas), variación entre *rankings* (arcos)&lt;br&gt;y número de menciones a temas (tamaño de los puntos) en los discursos&lt;br&gt;de investidura de los presidentes de gobierno de España entre 1979 y 2019.\",\n          caption = \"Elaboración propia a partir de Moncloa (2023) y empleando las siguientes&lt;br&gt;medidas de centralidad: degree, betweenness, closeness, eigenvector y PageRank.\",\n          value.lab = \"Coocurrencias\")"
  },
  {
    "objectID": "04_redes.html#ssss",
    "href": "04_redes.html#ssss",
    "title": "\n5  De textos a redes de política\n",
    "section": "\n5.3 SSSS",
    "text": "5.3 SSSS"
  },
  {
    "objectID": "04_redes.html#ssss-1",
    "href": "04_redes.html#ssss-1",
    "title": "\n5  De textos a redes de política\n",
    "section": "\n5.4 SSSS",
    "text": "5.4 SSSS"
  },
  {
    "objectID": "05_clasificacion_tematica.html#introducción",
    "href": "05_clasificacion_tematica.html#introducción",
    "title": "\n6  Clasificación temática\n",
    "section": "\n6.1 Introducción",
    "text": "6.1 Introducción\nIntroducción"
  },
  {
    "objectID": "05_clasificacion_tematica.html#extracción-y-clasificación-temática",
    "href": "05_clasificacion_tematica.html#extracción-y-clasificación-temática",
    "title": "\n6  Clasificación temática\n",
    "section": "\n6.2 Extracción y clasificación temática",
    "text": "6.2 Extracción y clasificación temática"
  },
  {
    "objectID": "05_clasificacion_tematica.html#extracción-de-temas",
    "href": "05_clasificacion_tematica.html#extracción-de-temas",
    "title": "\n6  Clasificación temática\n",
    "section": "\n6.3 Extracción de temas",
    "text": "6.3 Extracción de temas\n\nCódigolibrary(tenet)\n# Filtra el archivo de intervenciones para seleccionar:\n# 1) la Ley de la Libertad Sexual\n# 2) solo el debate entre diputados\n# 3) excluye la moderación y solo mantiene el posicionamiento\n#    de los distintos partidos\nsi &lt;- spa.sessions\n\nsi &lt;- si[grep(\"121/000062\",si$issue.details),]\n\nsi &lt;- si[si$session.number==124 & \n           si$issue.type==\"Debate\" &\n           si$rep.condition!=\"Miembro de la mesa\",]\n\n\n# Agrega las intervenciones por partido político\nsi &lt;- aggregate(list(text=si$speech.text), \n                by=list(partido=si$rep.party),\n                paste0,\n                collapse=\"\\n\")\n\n# Crea el corpus\nci &lt;- corpus(si)\ndocnames(ci) &lt;- si$partido\n\n\n\npcaScatter( ci, palette=pal$cat.brewer.Dark2.8[1:4],\n            title = \"Ley del Sí es Sí\",            \n            min.freq = 5)\n\n\n\n\n\n\n6.3.1 LSA\ntextmineR::FitLsaModel\n\n6.3.2 CTM\ntopicmodels\n\n6.3.3 LDA\ntopicmodels\nver Termite (Chuang, Manning, and Heer 2012) - visualización muy interesante y relativamente fácil de replicar en R.\nTambién LDAVis (Sievert and Shirley 2014), que tiene implementación en R ya.\n\n6.3.4 El método Reinert\nrainette\n\n6.3.5 Item Response Theory (IRT)\nltm"
  },
  {
    "objectID": "05_clasificacion_tematica.html#clasificación",
    "href": "05_clasificacion_tematica.html#clasificación",
    "title": "\n6  Clasificación temática\n",
    "section": "\n6.4 Clasificación",
    "text": "6.4 Clasificación\n\n6.4.1 Naive Bayes\n\n6.4.2 Support Vector Machines\n\n6.4.3 Deep Learning\n\n\n\n\nChuang, Jason, Christopher D. Manning, and Jeffrey Heer. 2012. “Termite: Visualization Techniques for Assessing Textual Topic Models.” Proceedings of the International Working Conference on Advanced Visual Interfaces 1 (1): 74–77. https://doi.org/10.1145/2254556.2254572.\n\n\nSievert, Carson, and Kenneth Shirley. 2014. “LDAvis: A Method for Visualizing and Interpreting Topics.” Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces 1 (1): 6370. https://aclanthology.org/W14-3110.pdf."
  },
  {
    "objectID": "06_clasificacion_textos.html#introducción",
    "href": "06_clasificacion_textos.html#introducción",
    "title": "\n7  Clasificación de textos\n",
    "section": "\n7.1 Introducción",
    "text": "7.1 Introducción\nIntroducción"
  },
  {
    "objectID": "06_clasificacion_textos.html#métodos-para-clasificar-textos-políticos",
    "href": "06_clasificacion_textos.html#métodos-para-clasificar-textos-políticos",
    "title": "\n7  Clasificación de textos\n",
    "section": "\n7.2 Métodos para clasificar textos políticos",
    "text": "7.2 Métodos para clasificar textos políticos"
  },
  {
    "objectID": "06_clasificacion_textos.html#métodos-supervisados",
    "href": "06_clasificacion_textos.html#métodos-supervisados",
    "title": "\n7  Clasificación de textos\n",
    "section": "\n7.3 Métodos supervisados",
    "text": "7.3 Métodos supervisados"
  },
  {
    "objectID": "06_clasificacion_textos.html#métodos-no-supervisados",
    "href": "06_clasificacion_textos.html#métodos-no-supervisados",
    "title": "\n7  Clasificación de textos\n",
    "section": "\n7.4 Métodos no supervisados",
    "text": "7.4 Métodos no supervisados\n\n7.4.1 Análisis de cluster"
  },
  {
    "objectID": "07_positio_ideologia.html#introducción",
    "href": "07_positio_ideologia.html#introducción",
    "title": "\n8  La ideología de los textos\n",
    "section": "\n8.1 Introducción",
    "text": "8.1 Introducción\nIntroducción"
  },
  {
    "objectID": "07_positio_ideologia.html#métodos-para-clasificar-textos-políticos",
    "href": "07_positio_ideologia.html#métodos-para-clasificar-textos-políticos",
    "title": "\n8  La ideología de los textos\n",
    "section": "\n8.2 Métodos para clasificar textos políticos",
    "text": "8.2 Métodos para clasificar textos políticos"
  },
  {
    "objectID": "07_positio_ideologia.html#métodos-supervisados",
    "href": "07_positio_ideologia.html#métodos-supervisados",
    "title": "\n8  La ideología de los textos\n",
    "section": "\n8.3 Métodos supervisados",
    "text": "8.3 Métodos supervisados"
  },
  {
    "objectID": "07_positio_ideologia.html#métodos-no-supervisados",
    "href": "07_positio_ideologia.html#métodos-no-supervisados",
    "title": "\n8  La ideología de los textos\n",
    "section": "\n8.4 Métodos no supervisados",
    "text": "8.4 Métodos no supervisados\n\n8.4.1 Análisis de cluster"
  },
  {
    "objectID": "08_positio_politicas.html#introducción",
    "href": "08_positio_politicas.html#introducción",
    "title": "\n9  Posiciones sobre políticas públicas\n",
    "section": "\n9.1 Introducción",
    "text": "9.1 Introducción\nIntroducción"
  },
  {
    "objectID": "08_positio_politicas.html#métodos-para-clasificar-textos-políticos",
    "href": "08_positio_politicas.html#métodos-para-clasificar-textos-políticos",
    "title": "\n9  Posiciones sobre políticas públicas\n",
    "section": "\n9.2 Métodos para clasificar textos políticos",
    "text": "9.2 Métodos para clasificar textos políticos"
  },
  {
    "objectID": "08_positio_politicas.html#métodos-supervisados",
    "href": "08_positio_politicas.html#métodos-supervisados",
    "title": "\n9  Posiciones sobre políticas públicas\n",
    "section": "\n9.3 Métodos supervisados",
    "text": "9.3 Métodos supervisados"
  },
  {
    "objectID": "08_positio_politicas.html#métodos-no-supervisados",
    "href": "08_positio_politicas.html#métodos-no-supervisados",
    "title": "\n9  Posiciones sobre políticas públicas\n",
    "section": "\n9.4 Métodos no supervisados",
    "text": "9.4 Métodos no supervisados\n\n9.4.1 Análisis de cluster"
  },
  {
    "objectID": "A01_corpora.html#introducción",
    "href": "A01_corpora.html#introducción",
    "title": "\n10  Fuentes: Textos empleados\n",
    "section": "\n10.1 Introducción",
    "text": "10.1 Introducción\nLos textos que interesan al análisis político poseen una enorme variedad en su formato. En algunos casos, nos encontramos ante textos legales (constituciones, leyes, propuestas legislativas o decretos). En otros, discursos o mensajes cortos en plataformas de microblogging (Twitter o Facebook), correos electrónicos (como los del escándalo de Enron o de Hillary Clinton, por ejemplo), manifiestos de partidos, diarios de sesiones parlamentarias, entrevistas, grupos focales o debates electorales.\nPor esa razón, he decidido incorporar al paquete tenet una variedad de textos políticos que pudiera permitir trabajar con diferentes formatos y técnicas de análisis de textos. En este apéndice, describimos con más detalle cada uno de esos corpus tanto para que los lectores tengan una idea de qué fuentes de información se emplean en los ejemplos como sus principales usos en el libro. En su mayoría, se encuentran en español. No obstante, también se han incorporado algunas referencias en portugués."
  },
  {
    "objectID": "A01_corpora.html#discursos-de-investidura",
    "href": "A01_corpora.html#discursos-de-investidura",
    "title": "\n10  Fuentes: Textos empleados\n",
    "section": "\n10.2 Discursos de investidura",
    "text": "10.2 Discursos de investidura\n\n10.2.1 Definición\nLos discursos de investidura corresponden a mensajes, generalmente orales, proferidos por los presidentes de gobierno en el momento en que asumen el cargo. Puesto que se tratan de monólogos, no existe debate, solo interviene el presidente. Los receptores suelen ser los diputados o parlamentarios. Por esa razón, representan un programa de gobierno (más que electoral), buscan marcar las líneas generales de la futura administración y señalan los diagnósticos sobre los principales problemas a enfrentar por parte del ganador. Poseen, además, una extensión media, aunque con variaciones perceptibles entre distintos presidentes. Algunos son muy locuaces, mientras que otros adoptan un estilo más escueto. No son tan cortos como los tweets o correos electrónicos, pero tampoco corresponden a documentos más extensos como libros o informes largos.\n\n10.2.2 Usos\n¿Para qué y cómo se emplean los discursos de investidura en análisis textual? La gran cantidad de textos que emplean esta fuente de información revela un interés por identificar rasgos como ideología, programas o posiciones políticas, el efecto de la religión, seguridad nacional, entre otros muchos temas.\nEn los ejemplos, hemos empleado los discursos inaugurales como fuente para la codificación y el análisis temático…\n\n10.2.3 Corpus: España\nLa base de datos spa.inaugural contiene el conjunto de discursos de investidura de los presidentes de gobierno de España entre 1979 y 2019. La fuente es la Moncloa. Se encuentra en formato data.frame y contiene las siguientes variables:\ndoc_id\nNombre del documento con el discurso de investidura.\ntext\nTexto integral del discurso.\nPresident\nNombre del Presidente.\nLegislature\nNúmero de la legislatura a la que pertenece el discurso.\n\n10.2.4 Corpus: Brasil\nLos discursos de investidura brasileños se encuentran en la base bra.inaugural. Se trata de un objeto corpus (del paquete quanteda) conteniendo todos los discursos de investidura de los presidentes brasileños desde 1889 hasta 2023. La principal fuente ha sido la biblioteca de la Presidencia de la República y el libro de João Bonfim (2004), Palavra de Presidente.\nText\nIdentificador del documento.\nTypes\nNúmero de palabras únicas en el texto.\nTokens\nNúmero total de palabras.\nSentences\nNúmero de frases en el discurso.\nYear\nAño en el cual el discurso ha sido proferido.\nPresident\nNombre del presidente que ha dado el discurso.\nDate\nFecha del discurso\nMilitary\nBinario. Indica si el presidente ha pertenecido a las Fuerzas Armadas.\nParty\nPartido del presidente.\nInterrupted\nBinario. Indica si el mandato del presidente ha sido interrumpido.\nInterrupt.Cause\nCausa de la interrupción del mandato (en caso positivo).\nElected\nBinario. Indica si el presidente ha sido elegido o si ha ocupado la posición por otros medios (golpe de Estado, elección indirecta, muerte o impedimiento del titular).\nlink_photo\nEnlace para la foto oficial del presidente."
  },
  {
    "objectID": "A01_corpora.html#diarios-de-sesiones-parlamentarias",
    "href": "A01_corpora.html#diarios-de-sesiones-parlamentarias",
    "title": "\n10  Fuentes: Textos empleados\n",
    "section": "\n10.3 Diarios de sesiones parlamentarias",
    "text": "10.3 Diarios de sesiones parlamentarias\n\n10.3.1 Definición\nLos diarios de las sesiones parlamentarias constituye otra fuente fundamental para entender los diferentes posicionamientos políticos. Contiene todos los debates, preguntas, requerimientos e intervenciones de los diputados. Cubren las posiciones de los miembros del gobierno y de la oposición no solo en la tramitación de leyes, sino también en las funciones de control de las acciones del ejecutivo. Además, revela el posicionamiento de cada legislador en la votación de temas cruciales para la ciudadanía. Se conforma como una fuente no solo de opinión, sino de posicionamiento político concreto. Al organizarse de forma cronológica, permiten, además, acompañar los ciclos y la evolución de los temas en debate y reconstruir la agenda legislativa del gobierno, por ejemplo.\n\n10.3.2 Usos\nSe emplean para medir emotividad, polarización, posiciones ideológicas, cohesión partidaria, diferencias de género en las intervenciones, entre otros usos.\n\n10.3.3 Corpus: España\nLa base de datos spa.sessions contiene todas las intervenciones de los diputados registrados en los diarios de sesiones de la XIV Legislatura del Congreso de Diputados de España (entre diciembre de 2019 y junio de 2023). Los datos han sido tratados para incluir un conjunto de metadatos que ayudan a identificar el partido, si ocupaba algún cargo en el gobierno, el distrito electoral.\nleg.number\nNúmero de la legislatura. En esta base de datos será siempre 14.\nsession.date\nFecha de la sesión legislativa en formato YYYY-MM-DD.\nsession.type\nTipo de sesión: “Plenaria” o “Diputación Permanente”.\nsession.number\nNúmero de la sesión. Secuencial de acuerdo con el tipo.\nissue.type\nTema de la intervención. Determina el contexto más general en el que se sitúa el habla del diputado:\n\nAvocación: solicitud de aprobación legislativa sin debate previo.\nComisión: intervención sobre el trabajo de las comisiones parlamentarias.\nComparecencia: debate en torno a la comparecencia del Presidente de Gobierno u otros miembros del ejecutivo.\nComunicación: informes generales sobre un tema concreto.\nConvalidación o derogación: debates acerca del proceso legislativo.\nDebate: preguntas y respuestas de diputados. Suele ser el espacio con mayor conflictividad entre los representantes. Excluye los debates de investidura, que poseen una categoría propia.\nEnmiendas del senado: discusión sobre las enmiendas del senado.\nEstado de alarma: este tópico resulta muy especial, puesto que la pandemia de COVID-19 ha tenido lugar durante esta legislatura y el Estado de alarma ha sido el instrumento elegido por el gobierno para atajar el problema.\nInterpelaciones: Requerimiento de informaciones a los miembros del gobierno o de la mesa del Congreso por parte de un representante.\nInvestidura: Discurso, debate y votación de la investidura del Presidente de Gobierno.\nMoción de censura: debate alrededor de una moción de censura al Presidente.\nOtros: otros temas.\nPetición: solicitud de un representante a la mesa.\nPreguntas: preguntas de los diputados.\nProposiciones no ley: Propuestas de resolución por parte de los grupos parlamentarios sin la participación necesaria de miembros del gobierno.\nToma en consideración: decisión de seguir adelante con la tramitación de una propuesta legislativa.\nTramitación: proceso de procesamiento y debate de las propuestas de ley en el Congreso.\nVotación: debates e intervenciones realizadas durante la votación.\n\nissue.details\nDetalle sobre el tópico. Tema del debate, pregunta concreta, entre otros.\nspeech.order\nOrden de la intervención en la sesión.\nspeech.text\nTexto completo de la intervención.\nrep.name\nNombre del representante.\nrep.district\nDistrito electoral por el cual ha sido elegido.\nrep.party\nPartido político o coalición electoral a la que pertenece el representante.\nrep.group\nGrupo parlamentario del representante.\nrep.condition\nDefine cuál es la condición del representante en el momento de su intervención: Candidato (a presidente de gobierno), Diputado, Miembro de la mesa, Ministro, Otros o Presidente.\nrep.institution\nEntidad concreta a la que se encuentra vinculado el representante: Presidencia, alguno de los ministerios, o al Congreso de Diputados.\nspeech.tokens\nTotal de palabras de la intervención."
  },
  {
    "objectID": "A01_corpora.html#debates-electorales",
    "href": "A01_corpora.html#debates-electorales",
    "title": "\n10  Fuentes: Textos empleados\n",
    "section": "\n10.4 Debates electorales",
    "text": "10.4 Debates electorales\n\n10.4.1 Definición\nLos debates electorales televisionados constituyen uno de los eventos más destacados de las campañas políticas, especialmente para las disputas presidenciales o para el ejecutivo. Aunque varíen mucho de formato en cada país, representan una fuente importante de información sobre las posiciones de los distintos candidatos y sus programas políticos. Además, permiten identificar conflictos, la centralidad de ciertos candidatos con relación a otros, así como rastrean los temas que aparecen en el debate público en distintos momentos del tiempo.\n\n10.4.2 Usos\nLos debates se pueden emplear para trazar los programas políticos de los candidatos, los temas centrales de movilización, las estrategias retóricas frente a los opositores y el abanico de promesas hacia el futuro.\n\n10.4.3 Corpus: Brasil\nLa base de datos bra.debate contiene los debates televisionados entre los candidatos a la Presidencia de la República en Brasil."
  },
  {
    "objectID": "A01_corpora.html#decretos-presidenciales",
    "href": "A01_corpora.html#decretos-presidenciales",
    "title": "\n10  Fuentes: Textos empleados\n",
    "section": "\n10.5 Decretos presidenciales",
    "text": "10.5 Decretos presidenciales\n\n10.5.1 Definición\nLa actividad legislativa representa otro tipo de texto político que interesa a los investigadores. Los decretos presidenciales o ejecutivos aún más, puesto que revelan la intención del gobierno de asumir funciones que típicamente se restringen al poder legislativo. En algunas democracias, los presidentes intentan centralizar poder por medio del abuso de medidas ejecutivas. De un lado, intentan asumir la función legislativa del Estado, reduciendo el espacio que los legisladores deberían ocupar. De otro, concentran la decisión del mismo ejecutivo en la figura del presidente, relegando a los ministros y otros órganos burocráticos a un segundo plano. El análisis de tales textos, por lo tanto, permite entender cómo funcionan los proceso deliberativos al interior del Estado.\n\n10.5.2 Usos\nComo hemos mencionado anteriormente, los decretos presidenciales pueden utilizarse para identificar la amplitud y los temas preferentes del ejercicio del poder por medio de quiénes ocupan el gobierno. ¿Se enfocan en un área de política concreta, como la seguridad o la política fiscal o son extensos? ¿Se necesita un decreto presidencial para comprar un bolígrafo o la arquitectura normativa del Estado resulta descentralizada y permite que tales decisiones se tomen de forma descentralizada? Los decretos ayudan a responder este tipo de preguntas, así como muchas otras acerca de grupos de poder, intereses y centralidad política.\n\n10.5.3 Corpus: Paraguay\nEl data.frame llamado pry.decree contiene una muestra con 1.000 decretos ejecutivos de Paraguay.\n\n\n\n\nBonfim, Joao Bosco Bezerra. 2004. Palavra de Presidente: Discursos de Posse de Deodoro a Lula. Brasília: LGE Editora."
  },
  {
    "objectID": "A02_preprocesa.html#introducción",
    "href": "A02_preprocesa.html#introducción",
    "title": "\n11  Preprocesamiento: Fundamentos\n",
    "section": "\n11.1 Introducción",
    "text": "11.1 Introducción\nUna vez tengamos los archivos descargados y guardados en la nube o en una carpeta en el disco duro de nuestro ordenador, podemos ir un paso más allá para abrirlos en R, extraer información y limpiarlos si resulta necesario. En esta sección, veremos cómo abrir archivos con distintos formatos (txt, PDF, docx entre otros) y extraer texto de PDFs sin tratar y limpiar los archivos antes de pasar a la siguiente fase de organización de los corpora.\nEste pequeño apartado tiene tres secciones. La primera abre archivos de texto en R que serán luego empleados para la creación de un todo coherente, relativamente comparable, que se someterá al análisis (corpus). La segunda realiza un OCR en archivos en formato PDF para, luego, extraer el texto. Finalmente, la tercera utiliza un conjunto de funciones de para la limpieza y búsqueda sistemática de texto, así como introduce las expresiones regulares."
  },
  {
    "objectID": "A02_preprocesa.html#abrir-archivos-de-textos",
    "href": "A02_preprocesa.html#abrir-archivos-de-textos",
    "title": "\n11  Preprocesamiento: Fundamentos\n",
    "section": "\n11.2 Abrir archivos de textos",
    "text": "11.2 Abrir archivos de textos\nEl primer paso de cualquier análisis de texto consiste en abrir los textos en el R para su posterior procesamiento y análisis. Afortunadamente, existe una serie de opciones que facilitan mucho la apertura de una cantidad grande de textos de un solo golpe, sin la necesidad de ir de uno en uno.\nLa función readtext() del paquete homónimo lee desde archivos de textos a PDFs, documentos de Word y otros formatos como planillas de Excel o json. No solo lee un archivo de cada vez, sino todavía mejor. Basta con suministrar el camino hasta la carpeta y la función trata de importar todos los archivos ahí contenidos de un golpe.\nEn la sección sobre web scraping hemos bajado el texto completo de más de 800 libros en español disponibles en los servidores del Proyecto Gutenberg. Los hemos guardado todos en la carpeta “Gut_txt/Archivos/”. Ahora podemos abrirlos de una vez en R utilizando la función readtext(). Veamos cómo se hace:\n\nCódigo# Abre el paquete readtext\nlibrary(readtext)\nlibrary(reactable)\n\n# Abre todos los archivos\ngt &lt;- readtext(\"../Text_Classify/Data/Source/Scraping/Gut_txt/Archivos/\")\n\n# Visualiza los primeros 10\nreactable(gt[1:10,], wrap=F, resizable = T)\n\n\n\n\n\n\n\nEl mismo procedimiento se puede llevar a cabo con archivos PDF que ya han sido sometidos a un OCR o desde un primer momento son digitales. Como vemos abajo, el código es exactamente el mismo. Lo único que cambia es la dirección de la carpeta, que en esta ocasión contiene solamente archivos PDF:\n\nCódigo# Carga los paquetes\nlibrary(readtext)\nlibrary(reactable)\n\n# Extrae los textos de todos los archivos en la carpeta\ngt &lt;- readtext(\"../Text_Classify/Data/Source/Scraping/PDFs/\")\n\n# Visualiza los 10 primeros\nreactable(gt[1:10,], wrap=F, resizable = T)\n\n\n\n\n\n\n\nComo en el caso anterior, el R genera un data.frame con dos variables: doc_id, conteniendo el nombre del archivo, y text con el texto completo. Este nuevo objeto será utilizado luego para la creación de objeto de tipo corpus en la tercera parte de esta sección."
  },
  {
    "objectID": "A02_preprocesa.html#ocr-y-extracción-de-texto",
    "href": "A02_preprocesa.html#ocr-y-extracción-de-texto",
    "title": "\n11  Preprocesamiento: Fundamentos\n",
    "section": "\n11.3 OCR y extracción de texto",
    "text": "11.3 OCR y extracción de texto\nSin embargo, el mundo sería un lugar más aburrido si las cosas siempre fueran tan sencillas. En muchos casos, nos encontraremos con archivos PDF escaneados con una resolución baja y sin reconocimiento de caracteres. En estos casos, nos vemos forzados a procesar los archivos antes de poder llevar a cabo cualquier análisis.\nEn R, el paquete tesseract permite realizar el reconocimiento óptico de caracteres (OCR, en su acrónimo original en inglés) en múltiples archivos y en distintas lenguas. Combinado con el paquete pdftools, permiten extraer el texto desde fuentes difíciles de tratar.\nUtilizaremos los mismos PDFs para realizar el OCR y luego extraer los textos. El análisis se dividirá en dos partes. En la primera, generaremos una lista de los archivos a ser procesados y descargaremos el modelo de OCR para español.\n\nCódigo# Carga los paquetes\nlibrary(tesseract)\nlibrary(pdftools)\n\n# Genera la lista de todos los PDFs\nfl &lt;- list.files(\"../Text_Classify/Data/Source/Scraping/PDFs/\")\n\n# Baja el modelo para realizar el \n# OCR en espaniol (solo una vez)\ntesseract_download(\"spa\")\n\n# Establece el espaniol como \n# lengua para el OCR\nesp &lt;- tesseract(\"spa\")\n\n\nEn la segunda parte, utilizaremos un bucle for para ir de archivo en archivo, realizar el OCR, extraer el texto y guardarlo en un nuevo formato (.txt) en una nueva carpeta.\n\nCódigo# Para cada PDF\nfor (i in 1:length(fl)){\n  \n  # Informa el avace\n  print(paste0(i, \" of \", length(fl)))\n  \n  # Extrae el nombre del archivo\n  ls &lt;- unlist(strsplit(fl[i],\"/\"))\n  ls &lt;- gsub(\".pdf\",\"\", ls)\n  ls &lt;- ls[length(ls)]\n  \n  # Realiza el OCR\n  text &lt;- tesseract::ocr(\n    paste0(\"../Text_Classify/Data/Source/Scraping/PDFs/\",fl[i]), \n    engine = esp)\n  \n  # Guarda el resultado en formato texto \n  write(text, paste0(\"../Text_Classify/Data/Source/Scraping/PDFs/OCR_txt/\",ls,\".txt\"))\n  \n}\n\n\nAhora podemos averiguar los resultados obtenidos por medio de la función readtext() utilizando el mismo código que hemos visto antes:\n\nCódigo# Carga los paquetes\nlibrary(readtext)\nlibrary(reactable)\n\n# Extrae los textos de todos los archivos en la carpeta\ngt &lt;- readtext(\"../Text_Classify/Data/Source/Scraping/PDFs/OCR_txt/\")\n\n# Visualiza los 10 primeros\nreactable(gt[1:10,], wrap=F, resizable = T)"
  },
  {
    "objectID": "A02_preprocesa.html#manipulación-y-limpieza-de-textos",
    "href": "A02_preprocesa.html#manipulación-y-limpieza-de-textos",
    "title": "\n11  Preprocesamiento: Fundamentos\n",
    "section": "\n11.4 Manipulación y limpieza de textos",
    "text": "11.4 Manipulación y limpieza de textos\nLa limpieza de los datos resulta fundamental para obtener un análisis adecuado de los textos. Se trata de un proceso laborioso, pero muy importante para la obtención de datos comparables. Aúna un conjunto de tareas concretas de manipulación que incluye: remover espacios en blanco, tildes, saltos de línea innecesarios o la extracción de datos o metadatos.\nLo que veremos aquí es un conjunto de técnicas que se pueden adaptar a textos de distinta estructura y naturaleza. No existe una solución universal de tratamiento de datos que funcione igual para tweets o para textos legales. En el caso de los primeros, habrá que tratar los elementos no textuales o los “emojis” antes de analizar el contenido. En los segundos, suele haber mucho ruido por la repetición de los encabezados de página por su publicación en archivos PDF.\nAdemás, cada estructura nos brindará oportunidades distintas de extracción y análisis de los datos. Por ejemplo, textos legales suelen ser muy estructurados y contienen la identificación de actores, títulos, capítulos, etc. Podemos utilizar tales informaciones como “marcadores” o “etiquetas” a la hora de extraer datos de forma sistemática. Por esa razón, resulta muy útil empezar por la sencilla tarea de explorar y describir cuál es la estructura del texto. ¿Se trata de un texto uniforme o segmentado (divisiones de capítulos, partes, títulos, artículos o cualquier otra)? ¿El texto tiene un formato digital desde el principio o tenemos que tratar encabezados u otros elementos comunes en PDFs y documentos Word? ¿El texto abre con todas las letras legibles o aparecen símblos raros en las tildes? O sea, ¿está en la codificación de caracteres adecuada o tengo que abrirlo utilizando una codificación específica (“LATIN1” es la más común para los que trabajamos textos en español)? La función stri_enc_list() del paquete stringi proporciona un listado completo de las codificaciones.\nEn esta parte del laboratorio, veremos algunas técnicas de manipulación de textos que permiten prepararlos para el análisis. Dividiremos el contenido en tres secciones. La primera examina las funciones de manipulación de texto de R y de los paquetes stringr y stringi. La segunda introduce brevemente las expresiones regulares, que representan un recurso muy útil para la identificación de patrones en textos. Finalmente, la tercera aplica el contenido de las dos anteriores en los textos que empleamos de ejemplo: los libros en español del Proyecto Gutenmberg y los decretos presidenciales de Paraguay.\n\n11.4.1 Cuenta, busca, extrae, divide, combina, sustituye, compara\nExiste un número amplio de funciones en R para la manipulación de texto. Podemos hacer casi cualquier operación desde buscar expresiones concretas hasta combinar textos o transformarlos en otras estructuras. Aquí exploraremos algunas tareas básicas muy útiles para trabajar con textos en R.\n\n11.4.1.1 Cuenta\nUna tarea de análisis de texto consiste en contar las veces que determinados temas, contenidos o conceptos aparecen. Esto se puede hacer utilizando ciertas palabras o diccionarios que ayudan a definir el peso de un tópico en el conjunto de elementos de un texto.\nPor ejemplo, ¿cuántas veces aparecen palabras que empiezan con “demo” en una variable? La función stri_count() del paquete stringi retorna el número de texto que un patrón cualquiera (en nuestro caso “demo”) aparece en un texto o en una variable.\n\nCódigo# Crea una variable de texto\ntx &lt;- \"La democracia es la forma de gobierno originada a partir del demos, o pueblo.\"\n\n# Carga el paquete stringr\nlibrary(stringi)\n\n# Cuenta las palabras que contienen \"demo\"\nstri_count(tx, regex = \"demo\")\n\n[1] 2\n\nCódigo# Ahora con una variable\ntx &lt;- c(\"democracia\",\"demostenes democrático\",\"nada\",\"demora\")\n\n# Cuenta las palabras que contienen \"demo\"\n# para cada elemento\nstri_count(tx, regex = \"demo\")\n\n[1] 1 2 0 1\n\n\nComo vemos, en el primer caso, el R nos ha retornado las dos veces en las que alguna palabra conteniendo “demo” aparecía en la frase. En el segundo, dos elementos llaman la atención. Primero, ya no es el total de veces en general, sino que el número se divide por observación de la variable. Segundo, debemos tener cuidado con la raíz que utilizamos para evitar ambiguedades y generar falsos positivos. Por ejemplo, demostenes y demora no tienen ninguna relación con democracia.\nOtra forma de contar que puede ser útil en algunos procesos de manipulación de texto. Por ejemplo, los códigos INE de los municipios de España incluyen dos caracteres iniciales con el código de la provincia y luego tres caracteres con el orden alfabético del municipio. Así que Almería tiene el código “04” y está en el 13º puesto en orden alfabético. No obstante, muchas veces, ciertas agencias informan el código como “04013” mientras otras lo informan como “4013”. Sin tratamiento, esto resulta un problema a la hora de comparar los datos.\nLa función stri_length() del paquete stringi soluciona el problema al contar cuántos caracteres hay en cada observación de una variable de texto. A partir de ese dato, podemos identificar cuáles elementos debemos tratar. En el ejemplo abajo añadimos un 0 al texto solo para aquellos códigos que son menores de 5 caracteres. De ese modo, uniformizamos el sistema de acuerdo con el estándar definido por el INE:\n\nCódigo# Crea una variable con los códigos INE para los municipios de\n# Almería, Barcelona, Madrid, Salamanca y Zamora\ntx &lt;- c(\"4013\",\"8019\",\"28079\",\"37274\",\"49275\")\n\n# Carga el paquete\nlibrary(stringi)\n\n# Cuenta los caracteres\nstri_length(tx)\n\n[1] 4 4 5 5 5\n\nCódigo# Incluye un cero en el codigo del municipio\ntx[stri_length(tx)&lt;5] &lt;- paste0(\"0\", tx[stri_length(tx)&lt;5]) \n\n# Inspecciona los resultados\ntx\n\n[1] \"04013\" \"08019\" \"28079\" \"37274\" \"49275\"\n\n\n\n11.4.1.2 Busca\nEn otras ocasiones, lo que deseamos es saber cuáles elementos del texto contienen ciertas ideas o palabras-clave que buscamos. En ese caso, se trata de identificar o si dichas expresiones se encuentran o no en el texto o, al reves, aquellos textos que contienen la palabra.\nPor ejemplo, ¿cuáles elementos de una variable contienen la palabra ministerio o ministro? La función stri_detect() del paquete stringi lleva a cabo dicha tarea.\n\nCódigo# Crea una variable con distinto contenido\ntx &lt;- c(\"ministro de telecomunicaciones\",\n        \"secretaria adjunto de la presidencia\", \n        \"ministerio de agricultura\", \n        \"director de la policía nacional\",\n        \"ministerio de seguridad social\",\n        \"ministra de educación\",\n        \"secretaría nacional de derechos humanos\",\n        \"directoría de asuntos exteriores\")\n\n# Carga el paquete sgtringi\nlibrary(stringi)\n\n# Detecta cuáles elementos contienen ministro o ministerio\nstri_detect(tx, regex = \"minist\")\n\n[1]  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE FALSE\n\nCódigo# Podemos seleccionarlos si queremos\ntx[stri_detect(tx, regex = \"minist\")]\n\n[1] \"ministro de telecomunicaciones\" \"ministerio de agricultura\"     \n[3] \"ministerio de seguridad social\" \"ministra de educación\"         \n\n\nComo se puede observar, el R retorna los elementos de la variable que contienen el patrón “minist”. En la primera forma es solamente una indicando TRUE o FALSE. En la segunda, hemos pedido que nos regrese el texto completo de cada observación.\nEjercicio: Podéis ejercitar el nuevo conocimiento intentando buscar “secretario” o “secretaría” y “director” o “diretoría”.\n\n11.4.1.3 Extrae\nEn otras ocasiones, queremos extraer los patrones para, por ejemplo, contar el número de veces que ocurren. En el siguiente ejemplo, extraeremos del discurso de investidura de Pedro Sánchez todas las palabras empezadas por igual (igualdad, igualitario, etc.) y por libert (libertad, libertades). Esto es posible gracias a la función stri_extract_all() del paquete stringi.\n\nCódigo# Carga el paquete readtext\nlibrary(readtext)\n\n# Lee el discurso de investidura de Pedro Sánchez de 2020\ntx &lt;- readtext(\"../Text_Classify/Data/Source/Scraping/Otros/Discursos_Presidentes/Espana/15_XIV_Leg_Sanchez.txt\",encoding =\"LATIN1\")\n\n# Carga el paquete stringi\nlibrary(stringi)\n\n# Extrae las palabras con raiz igual\nfr &lt;- unlist(stri_extract_all(tx, regex = \"igual[a-z]+\"))\n\n# Cuenta la frecuencia\ntable(fr)\n\nfr\n  igualdad igualdades    iguales igualmente \n        26          2          2          1 \n\nCódigo# Extrae las palabras con raiz libert\nfr &lt;- unlist(stri_extract_all(tx, regex = \"libert[a-z]+\"))\n\n# Cuenta la frecuencia\ntable(fr)\n\nfr\n  libertad libertades \n        15          3 \n\n\nSe ve como hay una frecuencia mayor de palabras relacionadas a la igualdad que a la libertad, aunque estas últimas también estén presentes en una proporción no muy inferior.\n\n11.4.1.4 Divide\nOtra tarea de manipulación de textos consiste en dividirlos según diferentes criterios que requieren cada análisis. Por ejemplo, una tarea muy común consiste en fragmentar los textos en palabras, algo que se denomina tokenization. Hay dos funciones en el paquete stringi que nos permiten dividir un texto: stri_split(), que utiliza un patrón para dividirlo y stri_split_fixed() que limita el número de fragmentos. Veamos un ejemplo:\n\nCódigo# Crea una variable de texto\ntx &lt;- \"Pepi, Luci, Bom y otras chicas del montón.\"\n\n# Carga el paquete stringr\nlibrary(stringi)\n\n# Separa utilizando la coma\nstri_split(tx, regex =\",\")\n\n[[1]]\n[1] \"Pepi\"                            \" Luci\"                          \n[3] \" Bom y otras chicas del montón.\"\n\nCódigo# Separa utilizando la coma, pero en solo dos fragmentos\nstri_split_fixed(str = tx, pattern = \", \", n = 2)\n\n[[1]]\n[1] \"Pepi\"                                \n[2] \"Luci, Bom y otras chicas del montón.\"\n\nCódigo# Un poco más avanzado - separa utilizando tanto la coma\n# como la y\nstri_split(tx, regex =\"[,y]\")\n\n[[1]]\n[1] \"Pepi\"                      \" Luci\"                    \n[3] \" Bom \"                     \" otras chicas del montón.\"\n\n\n\n11.4.1.5 Combina\nEn algunas ocasiones, necesitamos combinar distintos textos para trabajar con términos compuestos, bigramas o cualquier otra finalidad. El código abajo nos enseña cómo hacerlo utilizando la función stri_join() del paquete stringi.\n\nCódigo# Crea una variable de cantidades\nval &lt;- c(1, 2, 3, 4)\n\n# Crea una variable de texto\ntx &lt;- c(\"coche\", \"bicicletas\", \"hijos\", \"libros\")\n\n# Carga el paquete stringr\nlibrary(stringi)\n\n# Combina los dos textos \nstri_join(val, tx, sep=\" \", collapse=\", \")\n\n[1] \"1 coche, 2 bicicletas, 3 hijos, 4 libros\"\n\n\n\n11.4.1.6 Sustituye\nLa sustitución resulta muy útil para trabajar textos cuando se require el reemplazo de un valor por otro. Imaginemos que eres profesor y tienes una clase de 130 estudiantes. Como eres atento, les enviarás un informe con las notas por correo electrónico. Ya tienes un archivo Excel con sus nombres y calificaciones. No obstante, resulta muy trabajoso escribir a cada uno copiando y pegando el mismo texto.\nLa función stri_replace, del paquete que ya conocéis, permite reemplazar los datos como nombre y nota y facilitar el trabajo de redacción. Luego, se pueden utilizar otros paquetes como el gmailr para enviar los correos de forma automatizada (este último paso no lo haremos aquí).\n\nCódigo# Crea una variable de texto\ntx &lt;- \"EstimadART NOMBRE,\\n\\nEspero que este correo le encuentre bien.\\n\\nComo prometido, envío la calificación de la asignatura.\\nSu nota final ha sido NOTA.EMOJI\\n\\nReciba un cordial saludo,\\n\\nRodrigo\\n\\n\"\n\n# Pongamos unos emojis solo para divertirnos.\nemo &lt;- c(\"\\U1F937\",\"\\U1F64C\",\"\\U1F44D\",\"\\U1F947\")\n\n# Crea una lista de nombres\nnm &lt;- c(\"Pepe\", \"Manuel\",\"María\",\"Lola\")\n\n# Lista de notas\nnota &lt;- c(0, 5, 7.5, 8)\n\n# Artículo definido\nart &lt;- c(\"o\",\"o\",\"a\",\"a\")\n\n# Carga el paquete stringr\nlibrary(stringi)\n\n# Reemplaza el nombre\nst &lt;- stri_replace(tx, nm, regex =\"NOMBRE\")\n\n# Reemplaza el artículo definido\nst &lt;- stri_replace(st, art, regex =\"ART\")\n\n# Reemplaza el emoji\nst &lt;- stri_replace(st, emo, regex =\"EMOJI\")\n\n# Ahora reemplaza la nota\nst &lt;- stri_replace(st, nota, regex =\"NOTA\")\n\n# Imprime los resultados\ncat(st)\n\nEstimado Pepe,\n\nEspero que este correo le encuentre bien.\n\nComo prometido, envío la calificación de la asignatura.\nSu nota final ha sido 0.🤷\n\nReciba un cordial saludo,\n\nRodrigo\n\n Estimado Manuel,\n\nEspero que este correo le encuentre bien.\n\nComo prometido, envío la calificación de la asignatura.\nSu nota final ha sido 5.🙌\n\nReciba un cordial saludo,\n\nRodrigo\n\n Estimada María,\n\nEspero que este correo le encuentre bien.\n\nComo prometido, envío la calificación de la asignatura.\nSu nota final ha sido 7.5.👍\n\nReciba un cordial saludo,\n\nRodrigo\n\n Estimada Lola,\n\nEspero que este correo le encuentre bien.\n\nComo prometido, envío la calificación de la asignatura.\nSu nota final ha sido 8.🥇\n\nReciba un cordial saludo,\n\nRodrigo\n\n\n\n11.4.1.7 Compara\nOtra tarea muy útil consiste en comparar textos y determinar su similitud. Imaginemos que comparamos direcciones, o nombres de personas o entidades en fuentes que pueden contener errores ortográficos o de digitación. En esos casos, resulta fundamental poder medir el grado de similitud o diferencia para tomar una decisión sobre si se trata de la misma entidad o no.\nEl paquete stringdist posee diversas funciones orientadas a esta finalidad. Que permiten comparar desde dos textos entre sí hasta múltiple textos entre ellos.\nIlustremos cómo hacerlo utilizando dos textos literarios. En junio de 1580 muere en Lisboa el poeta Luis de Camões. En septiembre de este mismo año, Madrid asiste a la llegada al mundo de otro inmenso escritor, Francisco de Quevedo. Cualquier nativo o hablante fluyente de portugués o español no puede dejar de sorprenderse por la similitud entre dos sonetos de ambos autores sobre el amor. Incluso, en algunas estrofas, la redacción es idéntica.\nEl objetivo del código abajo resulta comparar ambos sonetos, estrofa por estrofa, y determinar el grado de similitud entre ellas. Nos restringiremos aquí solamente a algoritmos de similitud que comparan palabras sin atenernos a su función sintáctica o la carga semántica que conlleva. Por lo tanto, se trata de un análisis sencillo de la estructura de las estrofas.\n\nCódigo# Soneto del amor (Luis de Camões, 1598 - póstumo)\ncam1598 &lt;- c(\"amor es fuego que arde sin verse\",\n            \"es herida que duele y no se siente\",\n            \"es un contentamiento descontento\",\n            \"es dolor que lastima sin doler\",\n            \"es un no querer mas que bien querer\",\n            \"es andar solitario entre la gente\",\n            \"es nunca contentarse de contento\",\n            \"es un cuidar que gana en perderse\",\n            \"es querer estar aprisionado por voluntad\",\n            \"es servir a quien vence, el vencedor\",\n            \"es tener con quien nos mata, lealtad\",\n            \"pero cómo causar puede su favor\",\n            \"en los corazones humanos amistad\",\n            \"si tan contrario a si mismo es el amor\")\n  \n# Soneto del amor (Francisco de Quevedo, 1670 - póstumo)  \nqev1670 &lt;- c(\"es yelo abrasador, es fuego helado\", \n            \"es herida que duele y no se siente\",\n            \"es un soñado bien, un mal presente\",\n            \"es un breve descanso muy cansado\",\n            \"es un descuido que nos da cuidado\",\n            \"un cobarde, con nombre de valiente\",\n            \"un andar solitario entre la gente\",\n            \"un amar solamente ser amado\",\n            \"es una libertad encarcelada\",\n            \"que dura hasta el postrero parasismo\",\n            \"enfermedad que crece si es curada\",\n            \"este es el nino amor, este es su abismo\",\n            \"mirad cual amistad tendra con nada\",\n            \"el que en todo es contrario de si mismo\")  \n\n\n# Carga los paquetes stringdist - para calcular la similitud \n# y reshape2 - para cambiar el formato de un data.frame\nlibrary(stringdist)\nlibrary(reshape2)\n\n# Calcula la matriz de similitud entre los dos textos\n# La matriz permitirá identificar estrofas incluso si\n# se ha cambiado el orden.\nrd &lt;- round(stringsimmatrix(cam1598, \n                            qev1670, \n                            method = \"lcs\"),2)\n\n# Establece los nombres del las lineas como de Camões\n# y el nombre de las columnas como de Quevedo\nrownames(rd) &lt;- cam1598\ncolnames(rd) &lt;- qev1670\n\n# Transforma la matriz en un data.frame\ndrd &lt;- melt(rd)\n\n# Da nombre a las variables\nnames(drd) &lt;- c(\"Camoes\",\"Quevedo\",\"Similitud\")\n\n# Selecciona solamente los resultados cuya \n# similitud resulta superior a 50%.\ndrd &lt;- drd[drd$Similitud&gt;0.5,]\n\n# Ordena las estrofas restantes de la más\n# similar a la menos\ndrd &lt;- drd[order(drd$Similitud, decreasing = T),]\n\n# Inspecciona los resultados\nreactable(data = drd, resizable = T, striped = T)\n\n\n\n\n\n\n\nSe puede ver que, al menos cuatro estrofas de las 14 (28,6%) son muy similares. Resulta claro que esos dos textos presentan un fuerte parentesco e indican que Quevedo ha sido lector de Camões. Este mismo método puede ser aplicado para cualquier otro tipo de texto. El aspecto crucial es la elección de la unidad de comparación básica. En este ejemplo, la estrofa se empleó como unidad de análisis. En otras fuentes quizás párrafos o cuasi-frases sean las más indicadas. Siempre hay que explorar diferentes posibilidades y métodos antes de aplicar un algoritmo a un número amplio de casos.\n\n11.4.2 Expresiones regulares\nLas expresiones regulares son formas de sintaxis que permiten encontrar patrones en textos. Resultan tremendamente útiles a la hora de eliminar espacios en blanco, remover puntuación o acentos. Permite, además, encontrar palabras o números según patrones concretos. Su uso nos facilita buscar información, eliminar secciones que no nos sirven y evitar errores.\nPor ejemplo, el código abajo remueve los dobles espacios en blanco del texto:\n\nCódigo# Crea una variable con muchos espacios\ntx &lt;- \"Este    texto      tiene    muchos espacios en      blanco.\"\n\n# Sustituye los múltiples espacios por solo uno \ngsub(\"\\\\s+\",\" \", tx)\n\n[1] \"Este texto tiene muchos espacios en blanco.\"\n\nCódigo# Sustituye dos espacios por uno \ngsub(\"\\\\s{2}\",\" \", tx)\n\n[1] \"Este  texto   tiene  muchos espacios en   blanco.\"\n\n\nLa función gsub() sirve para reemplazar textos en una variable. En el primer ejemplo, la expresión regular \\\\s+ indica al R que busque cualquier secuencia de texto en la que haya un espacio en blanco o más y la reemplaza por solo un espacio. En la segunda, \\\\s{2} busca dos espacios y los sustituye por uno. Como vemos, los resultados son distintos porque hemos solicitado que R hiciera búsquedas diferentes.\nImaginemos que hay una variable de texto y necesitamos encontrar todos los números contenidos en ella. La función str_extract_all() del paquete stringi permite extraer información de una variable de texto. Si la combinamos con la expresión regular \\\\d+ (dígitos numéricos), el resultado es un conjunto de números.\n\nCódigo# Crea una variable textos conteniendo números\ntx &lt;- c(\"Tengo 10 euros y debo 1000.\",\n        \"De los 18 equipos, sono 1 puede llegar a campeón.\", \n        \"Más vale 8 que 80.\")\n\n# Carga el paquete\nlibrary(stringi)\n\n# Extrae los números \nstri_extract_all(tx, regex = \"\\\\d+\")\n\n[[1]]\n[1] \"10\"   \"1000\"\n\n[[2]]\n[1] \"18\" \"1\" \n\n[[3]]\n[1] \"8\"  \"80\"\n\n\nEn el ejemplo abajo, se utiliza otra expresión regular [A-Z] (mayúsculas), luego *[a-z]** (seguida de minúsculas) para encontrar y extraer las palabras iniciadas en mayúsculas en el texto.\n\nCódigo# Carga el paquete\nlibrary(stringi)\n\n# Crea un texto de ejemplo\ntx &lt;- \"Aqui pondremos algunos Ministerios, la Presidencia y el presidente.\"\n\n## Extrae del texto expresiones con mayusculas\nstri_extract_all(tx,regex = \"[A-Z][a-z]*\")\n\n[[1]]\n[1] \"Aqui\"        \"Ministerios\" \"Presidencia\"\n\n\nTambién se puede dividir un texto utilizando un caracter o palabra. La función stri_split() fragmenta una variable de texto a partir de un patrón que puede ser un caracter, como un espacio, una palabra, o un símbolo, como el de salto de linea.\n\nCódigo# Carga el paquete\nlibrary(stringi)\n\n## Define el texto a ser dividido\ntx &lt;- c(\"Esta es la primera frase.\\nEsta es la segunda frase.\")\n\n## Divide utilizando salto de linea\nstri_split(tx, regex = \"\\n\")\n\n[[1]]\n[1] \"Esta es la primera frase.\" \"Esta es la segunda frase.\"\n\nCódigo## Divide utilizando espacio\nstri_split(tx, regex = \" \")\n\n[[1]]\n[1] \"Esta\"         \"es\"           \"la\"           \"primera\"      \"frase.\\nEsta\"\n[6] \"es\"           \"la\"           \"segunda\"      \"frase.\"      \n\n\n¿Ya has intentado comparar los términos con o sin tildes? Acentuación y puntuación representan obstáculos comunes para la comparación de textos, especialmente cuando se aplican técnicas como la de bag-of-words. En estos casos, aquí, aqui y aquí. son consideradas palabras distintas. Para ello, hace falta remover la puntuación y los acentos para poder compararlas y encontrar su semejanza.\n\nCódigo# Carga el paquete\nlibrary(stringi)\n\n## Declara el texto\ntx &lt;- c(\"José, María y Elena quieren ir a la fiesta de ensueño. Pero, ¿de qué fiesta hablas, Pepe?\")\n\n# Elimina la puntuacion\nstri_replace_all(tx, regex = \"[:punct:]\",\"\")\n\n[1] \"José María y Elena quieren ir a la fiesta de ensueño Pero de qué fiesta hablas Pepe\"\n\nCódigo# Elimina todos los acentos\nstri_trans_general(tx, \"Latin-ASCII\")\n\n[1] \"Jose, Maria y Elena quieren ir a la fiesta de ensueno. Pero, ?de que fiesta hablas, Pepe?\"\n\n\nEstos ejemplos constituyen una pequeña introducción a las expresiones regulares. Hay un mundo de referencias a ser exploradas y hace falta tener siempre a mano un conjunto de chuletas para ayudarnos a buscar patrones de texto dependiendo del tipo de texto que estamos utilizando en cada momento.\nReferencias adicionales\nExiste un enorme material disponible sobre expresiones regulares, os recomiendo los siguientes:\n\nWickam - “Strings” En R for Data Science\n“Regular Expressions” en la documentación del paquete stringr\n“Regular Expressions” en el laboratorio LADAU\n\nTambién vale la pena consultar las referencias de los dos paquetes más importantes para la manipulación de datos en R, el stringr y el stringi:\n\nstringi: Fast and Portable Character String Processing in R\nstringr\n\n11.4.3 Ejemplos de textos reales\nEn esta sección realizaremos la limpieza y extracción de datos de los textos empleados en los ejemplos anteriores de web scraping y de lectura de PDFs. Utilizaremos, primero, los libros en español contenidos en la página del Proyecto Gutenberg y, segundo, los decretos presidenciales de Paraguay.\n\n11.4.3.1 Libros del Proyecto Gutenberg\nUna breve inspección a los libros contenidos en el Proyecto Gutenberg revela que, tanto al principio como al final de cada archivo, se pueden encontrar textos descriptivos en inglés relativos a la licencia o otros metatados. Antes de iniciar cualquier análisis, por lo tanto, resultaría muy útil remover estos pasajes de los originales para que solo contuvieran los textos en español.\nEl primer paso consiste en abrir los archivos y buscar indicadores claros y sistemáticos que puedan servir para automatizar la limpieza de los archivos. Después de una rápida búsqueda, hemos podido identificar dos frases que marcan de forma explícita el inicio y el fin del texto en español. Casi todos los libros empiezan con ““*** START OF THIS PROJECT GUTENBERG EBOOK” y terminan con “End of the Project Gutenberg EBook”. Sin embargo, ni todos contienen tales textos. De ese modo, el algoritmo debe buscar esas dos frases para delimitar el texto que seguirá y eliminar los pasajes en inglés. También debe mantener todo el texto en el caso de que no se encuentre ninguna de las dos o solo la frase inicial o final.\nEl código abajo lleva a cabo la tarea mencionada:\n\nCódigo# Extrae los textos de todos los archivos en la carpeta\ngt &lt;- readtext(\"../Text_Classify/Data/Source/Scraping/Gut_txt/Archivos/\")\n\n# Carga el paquete\nlibrary(stringi)\n\n# Define el texto que identifica el inicio del texto en español\nst &lt;- \"START OF TH\"\n\n# Define el texto que identifica el fin del texto en español\ned &lt;- \"End of the Project Gutenberg EBook\"\n\n# Para cada texto\nfor(i in 1:nrow(gt)){\n\n  # Informa el texto\n  print(i)\n  \n  # Divide el texto en líneas\n  tx &lt;- stri_split_lines(gt$text[i])\n  \n  # Convierte en un largo vector de texto\n  tx &lt;- unlist(tx)\n  \n  # Identifica la posicion INICIAL del texto en español\n  nst &lt;- grep(st, tx, fixed = F, ignore.case = F)\n  \n  # Identifica la posición FINAL del texto en español\n  ned &lt;- grep(ed, tx, fixed = F, ignore.case = T)\n  \n  # Si no encuentra la identificación del inicio\n  if(length(nst)==0){\n    \n    # Define como inicio la línea 0\n    nst &lt;-0\n    }\n\n  # Si no encuentra la identificacion del final\n  if(length(ned)==0){\n    \n    # Define como final la última línea más 1\n    ned &lt;- length(tx)+1\n  }\n  \n  # Selecciona solo el texto deste la posición de inicio\n  # más una línea y la de final menos una línea\n  tx &lt;- tx[(nst+1):(ned-1)]\n  \n  # Busca el titulo en mayusculas\n  n &lt;- which(stri_detect(tx, regex = \"[A-Z]{2,}\\\\s+[A-Z]{2,}\")==T)[1]\n  \n  # Si encuentra un titulo en mayusculas, lee desde la posicion\n  # del titulo encontrado\n  if(! is.na(n)) tx &lt;- tx[n:length(tx)]\n\n  # Remueve la división en líneas\n  tx &lt;- paste(tx, collapse = \" \")\n  \n  # Reemplaza dos espacios por un salto de línea\n  # Reproduce los párrafos originales\n  tx &lt;- gsub(\"\\\\s{2,}\",\"\\n\",tx, fixed = F)\n\n  # Actualiza el texto en la base de datos\n  gt$text[i] &lt;- tx\n\n}\n\n\n\n11.4.3.2 Decretos presidenciales de Paraguay\nEn el último ejemplo de esta parte, seleccionaremos algunos datos clave para clasificar los decretos presidenciales de Paraguay: el órgano a que se refiere el decreto (en general un ministerio), la exposición de motivos o el resumen del mismo y el texto de sus artículos.\nEl código abajo aplica uno o más estrategias introducidas anteriormente para llevar a cabo dicha tarea.\n\nCódigo# Extrae los textos de todos los archivos en la carpeta\ngt &lt;- readtext(\"../Text_Classify/Data/Source/Scraping/PDFs/\")\n\n# Selecciona solo los PDFs\ngt &lt;- gt[grep(\".pdf\",gt$doc_id),]\n\n# Crea tres variables nuevas\ngt$organo &lt;- NA    # Órgano a que se refiere\ngt$motivo &lt;- NA    # Motivo expuesto\ngt$teor &lt;- NA      # Artículos del decreto\n\n# Para cada decreto\nfor(i in 1:nrow(gt)){\n\n  # Divide los textos en líneas\n  tx &lt;- stri_split_lines1(gt$text[i])\n\n  # Elimina los espacios en blanco\n  tx &lt;- sapply(tx, trimws, USE.NAMES = F)\n\n  # Elimina los múltiples espacios en blanco\n  tx &lt;- gsub(\"\\\\s{2,}\",\" \", tx)\n  \n  # Prepara las partes del texto para que\n  # se dividan según un salto de línea\n  tx &lt;- gsub(\"VISTO\",\"  VISTO\", tx)\n  tx &lt;- gsub(\"Asunc\",\"  Asunc\", tx)\n  tx &lt;- gsub(\"DECRETA:\",\"  DECRETA: \", tx)\n  \n  # Elimina las líneas\n  tx &lt;- paste(tx, collapse = \" \")\n  \n  # Reemplaza los dobles espacios por \n  # saltos de línea\n  tx &lt;- gsub(\"\\\\s{2,}\",\"\\n\",tx)\n  \n  # Vuelve a dividir, pero ahora en\n  # párrafos consistentes\n  tx &lt;- stri_split_lines1(tx)\n  \n  # Averigua si contiene la expresión cexter\n  ce &lt;- grep(\"cexter\",tolower(tx), fixed = T)\n\n  # En caso positivo, la elimina\n  if(length(ce)&gt;0){\n    tx &lt;- tx[-ce]\n  }\n  \n  # Encuentra la posicion de los motivos\n  # en el texto\n  mo &lt;- tx[grep(\"POR EL CUAL\", tx)[1]]\n  \n  # Encuentra la información sobre el órgano\n  pres &lt;- tolower(tx[grep(\"PRESIDEN\", tx)[1]])\n  \n  # Elimina la presidencia de la identificación\n  # del órgano o ministerio\n  pres &lt;- gsub(\"presidencia de la república del paraguay\",\"\", pres)\n  pres &lt;- trimws(pres)\n\n  # Encuentra el texto de los artículos\n  teor &lt;- tx[(grep(\"DECRETA:\", tx)+1):length(tx)]\n  \n  # Lo convierte en un párrafo\n  teor &lt;- paste(teor, collapse = \"\\n\")\n\n  # Si queda texto de ruido, lo elimina\n  teor &lt;- gsub(\"NA\\nDECRETA: \",\"\", teor, fixed = T)\n  \n  # Atribuye cada información a su respectiva\n  # variable en la base de datos\n  gt$organo[i] &lt;- pres\n  gt$motivo[i] &lt;- mo\n  gt$teor[i] &lt;- teor\n  \n}\n\n# Visualiza los resultados\nreactable(gt, wrap = F, resizable = T, defaultPageSize = 10)"
  },
  {
    "objectID": "A03_dialogos_redes.html#introducción",
    "href": "A03_dialogos_redes.html#introducción",
    "title": "\n12  Preprocesamiento: conversión a redes\n",
    "section": "\n12.1 Introducción",
    "text": "12.1 Introducción"
  },
  {
    "objectID": "A03_dialogos_redes.html#la-conectividad-de-los-diálogos",
    "href": "A03_dialogos_redes.html#la-conectividad-de-los-diálogos",
    "title": "\n12  Preprocesamiento: conversión a redes\n",
    "section": "\n12.2 La conectividad de los diálogos",
    "text": "12.2 La conectividad de los diálogos"
  },
  {
    "objectID": "A03_dialogos_redes.html#preparación-anterior-de-los-textos",
    "href": "A03_dialogos_redes.html#preparación-anterior-de-los-textos",
    "title": "\n12  Preprocesamiento: conversión a redes\n",
    "section": "\n12.3 Preparación anterior de los textos",
    "text": "12.3 Preparación anterior de los textos\n\n12.3.1 Inclusión de marcadores"
  },
  {
    "objectID": "A03_dialogos_redes.html#conversión-a-data.frame",
    "href": "A03_dialogos_redes.html#conversión-a-data.frame",
    "title": "\n12  Preprocesamiento: conversión a redes\n",
    "section": "\n12.4 Conversión a data.frame\n",
    "text": "12.4 Conversión a data.frame"
  },
  {
    "objectID": "A03_dialogos_redes.html#de-data.frame-a-redes",
    "href": "A03_dialogos_redes.html#de-data.frame-a-redes",
    "title": "\n12  Preprocesamiento: conversión a redes\n",
    "section": "\n12.5 De data.frame a redes",
    "text": "12.5 De data.frame a redes"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referencias",
    "section": "",
    "text": "Atkinson-Abutridy, John. 2022. Text Analytics: An Introduction to\nthe Science and Applications of Unstructured Information Analysis.\nBoca Raton: CRC Press.\n\n\nAttride-Stirling, Jennifer. 2001. “Thematic Networks: An Analytic\nTool for Qualitative Research.” Qualitative Research 1\n(3): 385–405.\n\n\nAuerbach, Carl, and Louise B. Silverstein. 2003. Qualitative Data:\nAn Introduction to Coding and Analysis. New York: NYU press.\n\n\nBalzer, Michael, and Oliver Deussen. 2005. “Voronoi\nTreemaps.” In IEEE Symposium on Information Visualization,\n2005. INFOVIS 2005, 49–56. IEEE. https://doi.org/10.1109/INFVIS.2005.1532128.\n\n\nBenoit, Kenneth, and Akitaka Matsuo. 2020. Spacyr: Wrapper to the\n’spaCy’ ’NLP’ Library. https://CRAN.R-project.org/package=spacyr.\n\n\nBonfim, Joao Bosco Bezerra. 2004. Palavra de Presidente: Discursos\nde Posse de Deodoro a Lula. Brasília: LGE Editora.\n\n\nBorgatti, Stephen P., Martin G. Everett, and Jeffrey C. Johnson. 2018.\nAnalyzing Social Networks. London: SAGE Publications.\n\n\nBremer, Nadieh, and Shirley Wu. 2012. Data Sketches: A Journey of\nImagination, Exploration, and Beautiful Data Visualizations. New\nYork: CRC Press.\n\n\nBrewer, Cynthia. 2005. Desiging Better Maps: A Guide for GIS\nUsers. New York: ESRI Press.\n\n\nBrewer, Cynthia, Geoffrey W. Hatchard, and Mark Harrower. 2003.\n“ColorBrewer in Print: A Catalog of Color Schemes for\nMaps.” Cartography and Geographic Information Science 50\n(1): 5–32.\n\n\nBrewer, Cynthia, and Trudy Suchan. 2001. Mapping Census 2000: The\nGeography of u.s. Diversity. Redlands, CA: ESRI Press.\n\n\nCrossley, Nick, Elisa Bellotti, Gemma Edwards, Martin G. Everett, Johan\nKoskinen, and Mark Tranmer. 2015. Social Network Analysis for\nEgo-Nets: Social Network Analysis for Actor-Centred Networks.\nThousand Oaks, CA: SAGE.\n\n\nGrimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. 2022.\nText as Data: A New Framework for Machine Learning and the Social\nSciences. New Haven: Princeton University Press.\n\n\nGuest, Greg, Kathleen M. MacQueen, and Emily E. Namey. 2011. Applied\nThematic Analysis. Thousand Oaks, CA: SAGE Publications.\n\n\nHandler, Abram, Matthew Denny, Hanna Wallach, and Brendan O’Connor.\n2016. “Bag of What? Simple Noun Phrase Extraction for Text\nAnalysis.” In, 114124. https://aclanthology.org/W16-5615.pdf.\n\n\nHarrower, Mark, and Cynthia Brewer. 2003. “ColorBrewer.org: An\nOnline Tool for Selecting Colour Schemes for Maps.” The\nCartographic Journal 40 (1): 27–37.\n\n\nHolten, Danny. 2006. “Hierarchical Edge Bundles: Visualization of\nAdjacency Relations in Hierarchical Data.” IEEE Transactions\non Visualization and Computer Graphics 12 (5): 741–48.\n\n\nKennedy, A B W, and H R Sankey. 1898. “The Thermal Efficiency of\nSteam Engines. Report of the Committee Appointed to the Council Upon the\nSubject of the Definition of a Standard or Standards of Thermal\nEfficiency for Steam Engines: With an Introductory Note. (Including\nAppendixes and Plate at Back of Volume).” Minutes of the\nProceedings of the Institution of Civil Engineers 134 (1898):\n278–312. https://doi.org/10.1680/imotp.1898.19100.\n\n\nKennedy, Brianna L., and Robert Thornberg. 2018. “Deduction,\nInduction, and Abduction.” In The SAGE Handbook of\nQualitative Data Collection, 49–64. London: SAGE Publications.\n\n\nKnoke, David. 1990. Political Networks. Cambridge, MA:\nCambridge University Press.\n\n\nKnoke, David, and James H. Kuklinski. 1982. Network Analysis.\nVol. 28. London: Sage Publications.\n\n\nKrippendorff, Klaus. 2004. Content Analysis: An Introduction to Its\nMethodology. Thousand Oaks, CA: Sage publications.\n\n\nKumar, Ashish, and Avinash Paul. 2016. Mastering Text Mining with\nr. Birmingham: Packt Publishing.\n\n\nLin, Greg. 2023. Reactable: Interactive Data Tables for r. https://glin.github.io/reactable/authors.html#citation.\n\n\nMiles, Matthew B., A. Michael Huberman, and Johnny Saldana. 2019.\nQualitative Data Analysis: A Methods Sourcebook. Thousand Oaks,\nCA: SAGE Publications.\n\n\nMonroe, Burt L., Michael P. Colaresi, and Kevin M. Quinn. 2008.\n“Fightin’words: Lexical Feature Selection and Evaluation for\nIdentifying the Content of Political Conflict.” Political\nAnalysis 16 (4): 372403.\n\n\nNaerland, Torgeir Uberg. 2020. “The Political Significance of Data\nVisualization: Four Key Perspectives.” In Data Visualization\nin Society, 63–73. Amsterdam: Amsterdam University Press.\n\n\nNeuendorf, Kimberly A. 2017. The Content Analysis Guidebook.\nThousand Oaks, CA: SAGE Publications.\n\n\nRiehmann, Patrick, Manfred Hanfler, and Bernd Froehlich. 2005.\n“Interactive Sankey Diagrams.” In IEEE Symposium on\nInformation Visualization, 2005. INFOVIS 2005., 233240. IEEE.\n\n\nRinker, Tyler W. 2023. qdap:\nQuantitative Discourse Analysis Package. Buffalo, New\nYork. https://github.com/trinker/qdap.\n\n\nSaldana, Johnny. 2015. The Coding Manual for Qualitative Researchers\nThird Edition. Los Angeles ; London: SAGE.\n\n\nScott, John. 1991. Social Network Analysis. London: Sage\nPublications.\n\n\n———. 2012. What Is Social Network Analysis? London: Bloomsbury\nAcademic. https://library.oapen.org/handle/20.500.12657/58730.\n\n\nSilge, Julia, and David Robinson. 2017. Text Mining with r: A Tidy\nApproach. London: O’Reilly Media. https://www.tidytextmining.com/.\n\n\nThompson, Jamie. 2022. “A Guide to Abductive Thematic\nAnalysis.” The Qualitative Report 5 (27): 1410–21.\n\n\nVan Atteveldt, Wouter, Tamir Sheafer, Shaul R. Shenhav, and Yair\nFogel-Dror. 2017. “Clause Analysis: Using Syntactic Information to\nAutomatically Extract Source, Subject, and Predicate from Texts with an\nApplication to the 20082009 Gaza War.” Political\nAnalysis 25 (2): 207222.\n\n\nWasserman, Stanley, and Katherine Faust. 1994. Social Network\nAnalysis: Methods and Applications. Cambridge: Cambridge University\nPress.\n\n\nWattenberg, Martin, and Fernanda B. Viégas. 2008. “The Word Tree,\nan Interactive Visual Concordance.” IEEE Transactions on\nVisualization and Computer Graphics 14 (6): 12211228.\n\n\nWelbers, Kasper, Wouter Van Atteveldt, and Kenneth Benoit. 2017.\n“Text Analysis in r.” Communication Methods and\nMeasures 11 (4): 245265.\n\n\nWickham, Hadley. 2023. Ggplot2: Elegant Graphics for Data\nAnalysis. 3rd ed. Berlin: Springer. https://ggplot2-book.org/.\n\n\nWijffels, Jan. 2023. Udpipe: Tokenization, Parts of Speech Tagging,\nLemmatization and Dependency Parsing with the ’UDPipe’ ’NLP’\nToolkit. https://CRAN.R-project.org/package=udpipe.\n\n\nWilkinson, Leland. 1999. The Grammar of Graphics. New York:\nSpringer.\n\n\nYang, Song, Franziska B. Keller, and Lu Zheng. 2016. Social Network\nAnalysis: Methods and Examples. Thousand Oaks, CA: SAGE\nPublications."
  },
  {
    "objectID": "A04_visualizacion.html#introducción",
    "href": "A04_visualizacion.html#introducción",
    "title": "\n13  Herramientas: visualización\n",
    "section": "\n13.1 Introducción",
    "text": "13.1 Introducción\nDurante todo el trabajo, hemos explorado un conjunto bastante variado de formas de visualización de texto. Algunas tradicionales o consolidadas, como los árboles y las nubes de palabras. En otros casos, hemos experimentado con gráficos que suelen ser empleados en estudios cuantitativos o para otras aplicaciones. Además, hemos desarrollado algunos instrumentos que ayudan en la aprehensión de los patrones, la interacción o la reusabilidad de los resultados.\nNaerland (2020, 63) señala cuatro aspectos en los que la visualización de datos adquiere un peso político: como instrumento que facilita la deliberación pública, como ideología (o propaganda), como medio de intervención ciudadana (por ejemplo, proyectos de mapeo comunitario) y como herramienta de maniobra político-administrativa. No se trata, por lo tanto, de un instrumento pasivo o encerrado en el ambiente académico. El gráfico warming stripes representa un ejemplo claro. Su aspecto técnico no resulta para nada sofisticado. Se ha hecho con líneas verticales paralelas representando la variación anual de temperatura frente a la media de un período de referencia de 30 años y utilizando una paleta que va de azul (más frío) a rojo (más caliente). Muestra claramente que desde hace unas décadas, la temperatura de la tierra ha ido en aumento de forma inequívoca y constante. Su sencillez e impacto visual han sido cruciales para comunicar a la mayoría de los ciudadanos que sí hay cambio climático y desde cuándo.\nEn este apéndice, examinamos las herramientas empleadas para la visualización de datos textuales. En primer lugar, hacemos la distinción entre visualizar los textos mismos y representar datos que han sido extraídos de los primeros. Segundo, presentamos los tipos de paleta de color y su empleo en la visualización de datos. Tercero, listamos los principios de interactividad y modularidad empleados en la construcción de los gráficos del libro. Finalmente, describimos los diferentes tipos de gráfico empleados durante todo el libro y su utilización en la síntesis o representación gráfica de textos."
  },
  {
    "objectID": "A04_visualizacion.html#paletas-de-color",
    "href": "A04_visualizacion.html#paletas-de-color",
    "title": "\n13  Herramientas: visualización\n",
    "section": "\n13.2 Paletas de color",
    "text": "13.2 Paletas de color\nEl primero consiste en suministHemos clasificado los colores de acuerdo con su tipología (secuencial, divergente o cualitativo) y la cantidad de colores (monocolor, multicolor)"
  },
  {
    "objectID": "A04_visualizacion.html#interactividad",
    "href": "A04_visualizacion.html#interactividad",
    "title": "\n13  Herramientas: visualización\n",
    "section": "\n13.3 Interactividad",
    "text": "13.3 Interactividad\n\n13.3.1 Tablas\nreactable\n\n13.3.2 Gráficos\namcharts 5"
  },
  {
    "objectID": "A04_visualizacion.html#reusabilidad",
    "href": "A04_visualizacion.html#reusabilidad",
    "title": "\n13  Herramientas: visualización\n",
    "section": "\n13.4 Reusabilidad",
    "text": "13.4 Reusabilidad\nEl marco de ggplot2"
  },
  {
    "objectID": "A04_visualizacion.html#tipos-de-gráfico",
    "href": "A04_visualizacion.html#tipos-de-gráfico",
    "title": "\n13  Herramientas: visualización\n",
    "section": "\n13.5 Tipos de gráfico",
    "text": "13.5 Tipos de gráfico\n(quizás aquí crear un diagrama de árbol como el de data-to-viz)\n\n13.5.1 Flujo\n\nFunción:\n\nRepresentar cambios o transiciones de un estado o momento a otro. Sirven para visualizar el diálogo y la interacción entre ideas, textos, personajes o términos.\n\nTipos empleados:\n\nDiagrama de Sankey\nDiagrama de cordas\nSociogramas\nWordtree\nDiagrama de arco\nstreamChart\n\n13.5.2 Jerarquía\n\nfunción:\n\nPermiten visualizar un orden o jerarquía entre valores. La codificación temática en muchas ocasiones opera a partir de una estructura jerárquica que va desde los términos empíricos concretos (palabras o frases) hasta temas de mayor nivel de abstracción. Las redes de interacción entre parlamentarios, por otra parte, revelan distintos protagonismos y actores más centrales en la configuración de los diferentes discursos político-partidarios.\n\nTipos empleados:\n\ndiagrama de cordas\nforce directed tree\nVoronoi Tree\nDendrograma de Cluster\n\n13.5.3 Proporción\n\nfunción:\n\nLos gráficos de proporción, como su mismo nombre revela, señalan el peso relativo de un atributo -que puede ser un concepto, una palabra, un documento o un actor- en comparación con otros. Nos permite medir la intensidad y comparar las posiciones de las distintas unidades representadas.\n\ntipos empleados:\n\nVoronoi tree map\nGráfico de barras\nForce Directed tree\nstreamChart\nSolar plot\n\n13.5.4 Asociación\n\nfunción:\n\nEste tipo de gráfico permite averiguar la asociación entre dos variables. Permite ubirar una categoría, palabra o actor en dos dimensiones medidas cuantitativamente.\n\ntipos empleados:\n\npcaScatter\nplotLogOddsRatio\n\n13.5.5 Etiquetado o localización\n\nfunción:\n\nEste tipo de gráfico permite señalar en los textos dónde se encuentran ideas, palabras-clave o expresionnes de interés. Más aún, posibilitan clasificar las unidades de medida -que van desde una palabra única a frases, párrafos, capítulos hasta documentos enteros- según conceptos o temas generales. En ese sentido, facilitan la localización de ideas y su posterior reordenamiento y selección según los criterios de investigación deseados.\n\ntipos empleados:\n\ntagText\ntagCorpus\ndispersión léxica\n\n\n\n\nBrewer, Cynthia. 2005. Desiging Better Maps: A Guide for GIS Users. New York: ESRI Press.\n\n\nBrewer, Cynthia, Geoffrey W. Hatchard, and Mark Harrower. 2003. “ColorBrewer in Print: A Catalog of Color Schemes for Maps.” Cartography and Geographic Information Science 50 (1): 5–32.\n\n\nBrewer, Cynthia, and Trudy Suchan. 2001. Mapping Census 2000: The Geography of u.s. Diversity. Redlands, CA: ESRI Press.\n\n\nHarrower, Mark, and Cynthia Brewer. 2003. “ColorBrewer.org: An Online Tool for Selecting Colour Schemes for Maps.” The Cartographic Journal 40 (1): 27–37.\n\n\nLin, Greg. 2023. Reactable: Interactive Data Tables for r. https://glin.github.io/reactable/authors.html#citation.\n\n\nNaerland, Torgeir Uberg. 2020. “The Political Significance of Data Visualization: Four Key Perspectives.” In Data Visualization in Society, 63–73. Amsterdam: Amsterdam University Press.\n\n\nWickham, Hadley. 2023. Ggplot2: Elegant Graphics for Data Analysis. 3rd ed. Berlin: Springer. https://ggplot2-book.org/.\n\n\nWilkinson, Leland. 1999. The Grammar of Graphics. New York: Springer."
  },
  {
    "objectID": "A04_visualizacion.html#color-y-gráficos",
    "href": "A04_visualizacion.html#color-y-gráficos",
    "title": "\n13  Herramientas: visualización\n",
    "section": "\n13.3 Color y gráficos",
    "text": "13.3 Color y gráficos\nEl primero consiste en suminist\n\n13.3.1 Paletas de color\nDefinición de paleta de color y por qué resultan importantes para obtener gráficos efectivos. (Brewer, Hatchard, and Harrower 2003; Harrower and Brewer 2003)\nCartografía - Brewer desarrolla las paletas para tener en cuenta los posibles problemas de visualización en mapas. Su trabajo ha sido tan efectivo que, al final, logró resultados que van mucho más allá de la cartografía y que se extienden a la visualización de datos en general (Brewer 2005; Brewer and Suchan 2001)\n\n13.3.2 Tipos de paleta\nExisten tres tipos de paleta de color para la visualización de datos.\n\nsecuencial - corresponde a paletas que incluyen gradaciones de un mismo color o de colores complementarios que sugieren o representan cambios de intensidad o proporción. Ejemplo: porcentaje de votos.\ndivergente - se emplean cuando existe un punto medio desde el que los valores se distancias. En general se emplean dos o tres colores, siendo dos complementarias -para los extremos- y una intermedia que indica el centro. Ejemplo: escala ideológica, de nacionalismo o populismo.\ncualitativa - se utilizan colores contrastantes (complementarios) para indicar diferentes cualidades o categorías. Ejemplo: partidos o regiones.\n\n13.3.3 Las paletas empleadas\nExiste un oferta enorme de paletas de color en R. Hemos seleccionado aquí algunas basadas en paquetes\nhttps://github.com/EmilHvitfeldt/r-color-palettes\nRColorBrewer\nwesanderson\nghigli\nlisa\nMetBrewer\nrtist\nHemos clasificado los colores de acuerdo con su tipología (secuencial, divergente o cualitativo)"
  },
  {
    "objectID": "A04_visualizacion.html#principios-de-construcción",
    "href": "A04_visualizacion.html#principios-de-construcción",
    "title": "\n13  Herramientas: visualización\n",
    "section": "\n13.4 Principios de construcción",
    "text": "13.4 Principios de construcción\n\n13.4.1 Interactividad\nAunque no esté presente en todas las visualizaciones empleadas en el libro, la interactividad se ha utilizado de forma extensiva. Se trata de un recurso muy valioso a la hora de permitir a los usuarios explorar los detalles de la visualización, conocer mejor los datos y extraer nuevas ideas o encontrar otros patrones que, a un principio, se encontraban fuera de las intenciones iniciales del autor.\n\n13.4.1.1 Tablas\nLa casi totalidad de las tablas empleadas permite algún tipo de interactividad. En algunos casos, consiste en solamente reordenar los valores al hacer clic sobre el título de la columna. No obstante, en otros también se permite filtrar los valores, buscar o redimensionar las columnas. Para ello, se ha empleado el paquete de R reactable (Lin 2023) que incorpora una serie de recursos para la creación de tablas interactivas en R.\n\n13.4.1.2 Gráficos\nHemos adaptado los gráficos de la biblioteca de gráficos en JavaScript Amcharts (amcharts.com). Representa un de las más variadas y sus gráficos son estéticamente atractivos y de fácil utilización. Entre los formatos utilizados, se destacan aquellos que representan redes y datos jerárquicos, como se puede constatar en los ejemplos empleados durante todo el trabajo.\n\n13.4.2 Modularidad\nEn otros gráficos, hemos puesto hincapié en desarrollar códigos que incorporaran la gramática de los gráficos (Wilkinson 1999). Consiste en una forma de creación de gráficos que permite la composición a partir de elementos básicos o modulares como los datos, las geometrías, el tema. El paquete de R ggplot2 (Wickham 2023) incorpora en su diseño este lenguaje visual y permite una enorme flexibilidad a la hora de crear visualizaciones potentes. Por esa razón, a los gráficos que hemos desarrollado en los ejemplos, se pueden añadir temas y otros elementos (textos, puntos, líneas, etc.). Esta característica permite reutilizarlos para diferentes textos, formatos, fuentes de información o propósitos (incluso más allá de la visualización de textos)."
  },
  {
    "objectID": "A05_paquetes_R.html#introducción",
    "href": "A05_paquetes_R.html#introducción",
    "title": "\n14  Herramientas: paquetes de R\n",
    "section": "\n14.1 Introducción",
    "text": "14.1 Introducción"
  },
  {
    "objectID": "A05_paquetes_R.html#quanteda",
    "href": "A05_paquetes_R.html#quanteda",
    "title": "\n14  Herramientas: paquetes de R\n",
    "section": "\n14.2 Quanteda",
    "text": "14.2 Quanteda"
  },
  {
    "objectID": "A05_paquetes_R.html#tenet",
    "href": "A05_paquetes_R.html#tenet",
    "title": "\n14  Herramientas: paquetes de R\n",
    "section": "\n14.3 tenet",
    "text": "14.3 tenet"
  },
  {
    "objectID": "A05_paquetes_R.html#ggplot2",
    "href": "A05_paquetes_R.html#ggplot2",
    "title": "\n14  Herramientas: paquetes de R\n",
    "section": "\n14.4 ggplot2",
    "text": "14.4 ggplot2"
  },
  {
    "objectID": "04_redes.html#morfología-de-las-redes",
    "href": "04_redes.html#morfología-de-las-redes",
    "title": "\n5  De textos a redes de política\n",
    "section": "\n5.3 Morfología de las redes",
    "text": "5.3 Morfología de las redes\n\n5.3.1 Unidades básicas\nnodo, vértice, díada, tríada, clique, puente (bridge), motivo (motif), automorfismo y comunidades.\n\n5.3.2 Tipos de red\nRedes direccionales, no direccionales, ego-céntricas\n\n5.3.3 Topología de redes\nAnálisis de la estructura de conectividad\n\n5.3.4 Propiedades generales\nConmutabilidad, densidad, transitividad, centralización, eccentricidad, asortividad, cohesión, diversidad, coreness, reciprocidad, modularidad…\n\n5.3.5 Propiedades comparativas\nequivalencia estructural, conectividad, centralidad alfa, cocitación, distancia, camino más corto, vecinos, vecindario, similitud,"
  },
  {
    "objectID": "04_redes.html#diagnóstico-general",
    "href": "04_redes.html#diagnóstico-general",
    "title": "\n5  De textos a redes de política\n",
    "section": "\n5.4 Diagnóstico general",
    "text": "5.4 Diagnóstico general"
  },
  {
    "objectID": "04_redes.html#análisis-de-una-red-ego-céntrica",
    "href": "04_redes.html#análisis-de-una-red-ego-céntrica",
    "title": "\n5  De textos a redes de política\n",
    "section": "\n5.5 Análisis de una red ego-céntrica",
    "text": "5.5 Análisis de una red ego-céntrica\n\n\n\n\nBorgatti, Stephen P., Martin G. Everett, and Jeffrey C. Johnson. 2018. Analyzing Social Networks. London: SAGE Publications.\n\n\nCrossley, Nick, Elisa Bellotti, Gemma Edwards, Martin G. Everett, Johan Koskinen, and Mark Tranmer. 2015. Social Network Analysis for Ego-Nets: Social Network Analysis for Actor-Centred Networks. Thousand Oaks, CA: SAGE.\n\n\nKnoke, David. 1990. Political Networks. Cambridge, MA: Cambridge University Press.\n\n\nKnoke, David, and James H. Kuklinski. 1982. Network Analysis. Vol. 28. London: Sage Publications.\n\n\nScott, John. 1991. Social Network Analysis. London: Sage Publications.\n\n\n———. 2012. What Is Social Network Analysis? London: Bloomsbury Academic. https://library.oapen.org/handle/20.500.12657/58730.\n\n\nWasserman, Stanley, and Katherine Faust. 1994. Social Network Analysis: Methods and Applications. Cambridge: Cambridge University Press.\n\n\nYang, Song, Franziska B. Keller, and Lu Zheng. 2016. Social Network Analysis: Methods and Examples. Thousand Oaks, CA: SAGE Publications."
  },
  {
    "objectID": "07_positio_ideologia.html#cómo-medir-ideología-a-partir-de-textos",
    "href": "07_positio_ideologia.html#cómo-medir-ideología-a-partir-de-textos",
    "title": "\n8  La ideología de los textos\n",
    "section": "\n8.2 ¿Cómo medir ideología a partir de textos?",
    "text": "8.2 ¿Cómo medir ideología a partir de textos?"
  },
  {
    "objectID": "07_positio_ideologia.html#estrategias-empíricas",
    "href": "07_positio_ideologia.html#estrategias-empíricas",
    "title": "\n8  La ideología de los textos\n",
    "section": "\n8.3 Estrategias empíricas",
    "text": "8.3 Estrategias empíricas\n\n8.3.1 Fuentes\n\n8.3.2 Técnicas"
  },
  {
    "objectID": "07_positio_ideologia.html#resultados",
    "href": "07_positio_ideologia.html#resultados",
    "title": "\n8  La ideología de los textos\n",
    "section": "\n8.4 Resultados",
    "text": "8.4 Resultados"
  },
  {
    "objectID": "07_positio_ideologia.html#comparación-con-otras-fuentes",
    "href": "07_positio_ideologia.html#comparación-con-otras-fuentes",
    "title": "\n8  La ideología de los textos\n",
    "section": "\n8.5 Comparación con otras fuentes",
    "text": "8.5 Comparación con otras fuentes\nComparación con encuestas con legisladores, expertos y otras fuentes como el Manifesto Project"
  },
  {
    "objectID": "07_positio_ideologia.html#consideraciones-finales",
    "href": "07_positio_ideologia.html#consideraciones-finales",
    "title": "\n8  La ideología de los textos\n",
    "section": "\n8.6 Consideraciones finales",
    "text": "8.6 Consideraciones finales"
  },
  {
    "objectID": "03_analisis_sintactico.html#estudio-de-caso",
    "href": "03_analisis_sintactico.html#estudio-de-caso",
    "title": "\n4  Análisis sintáctico\n",
    "section": "\n4.5 Estudio de caso",
    "text": "4.5 Estudio de caso\nAplicación del método al análisis de un corpus concreto."
  },
  {
    "objectID": "03_analisis_sintactico.html#consideraciones-finales",
    "href": "03_analisis_sintactico.html#consideraciones-finales",
    "title": "\n4  Análisis sintáctico\n",
    "section": "\n4.6 Consideraciones finales",
    "text": "4.6 Consideraciones finales"
  },
  {
    "objectID": "03_analisis_sintactico.html#ejercicios",
    "href": "03_analisis_sintactico.html#ejercicios",
    "title": "\n4  Análisis sintáctico\n",
    "section": "\n4.7 Ejercicios",
    "text": "4.7 Ejercicios"
  },
  {
    "objectID": "03_analisis_sintactico.html#lecturas-adicionales",
    "href": "03_analisis_sintactico.html#lecturas-adicionales",
    "title": "\n4  Análisis sintáctico\n",
    "section": "\n4.8 Lecturas adicionales",
    "text": "4.8 Lecturas adicionales\n\n\n\n\nAtkinson-Abutridy, John. 2022. Text Analytics: An Introduction to the Science and Applications of Unstructured Information Analysis. Boca Raton: CRC Press.\n\n\nBenoit, Kenneth, and Akitaka Matsuo. 2020. Spacyr: Wrapper to the ’spaCy’ ’NLP’ Library. https://CRAN.R-project.org/package=spacyr.\n\n\nGrimmer, Justin, Margaret E. Roberts, and Brandon M. Stewart. 2022. Text as Data: A New Framework for Machine Learning and the Social Sciences. New Haven: Princeton University Press.\n\n\nHandler, Abram, Matthew Denny, Hanna Wallach, and Brendan O’Connor. 2016. “Bag of What? Simple Noun Phrase Extraction for Text Analysis.” In, 114124. https://aclanthology.org/W16-5615.pdf.\n\n\nKumar, Ashish, and Avinash Paul. 2016. Mastering Text Mining with r. Birmingham: Packt Publishing.\n\n\nMonroe, Burt L., Michael P. Colaresi, and Kevin M. Quinn. 2008. “Fightin’words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict.” Political Analysis 16 (4): 372403.\n\n\nRinker, Tyler W. 2023. qdap: Quantitative Discourse Analysis Package. Buffalo, New York. https://github.com/trinker/qdap.\n\n\nVan Atteveldt, Wouter, Tamir Sheafer, Shaul R. Shenhav, and Yair Fogel-Dror. 2017. “Clause Analysis: Using Syntactic Information to Automatically Extract Source, Subject, and Predicate from Texts with an Application to the 20082009 Gaza War.” Political Analysis 25 (2): 207222.\n\n\nWelbers, Kasper, Wouter Van Atteveldt, and Kenneth Benoit. 2017. “Text Analysis in r.” Communication Methods and Measures 11 (4): 245265.\n\n\nWijffels, Jan. 2023. Udpipe: Tokenization, Parts of Speech Tagging, Lemmatization and Dependency Parsing with the ’UDPipe’ ’NLP’ Toolkit. https://CRAN.R-project.org/package=udpipe."
  },
  {
    "objectID": "A04_visualizacion.html#visualizar-textos-vs.-datos-sobre-textos",
    "href": "A04_visualizacion.html#visualizar-textos-vs.-datos-sobre-textos",
    "title": "\n13  Herramientas: visualización\n",
    "section": "\n13.2 Visualizar textos vs. datos sobre textos",
    "text": "13.2 Visualizar textos vs. datos sobre textos\nVisualización de textos - el texto mismo permanece como base de la visualización. Podemos enriquecerlo con etiquetas y anotaciones, pero permanece como base de inspección.\nVisualización de datos a partir de textos - se generan una serie de datos cuantitativos a partir del procesamiento del corpus original y esto es lo que se visualiza.\nParece una distinción trivial. No obstante, resulta crucial para distintas tareas de análisis. Investigadores con un enfoque más cualitativo preferirán disponer de herramientas que les permitan enriquecer los textos y poder volver a ellos desde una perspectiva distinta. Por otra parte, otros con una postura más cuantitativa buscarán opciones que permitan sintetizar y representar de forma estadística la estructura o aspectos que consideran centrales en los documentos.\n(*quizás aquí crear un diagrama de árbol como el de data-to-viz)"
  },
  {
    "objectID": "A04_visualizacion.html#visualizar-textos-y-datos-derivados",
    "href": "A04_visualizacion.html#visualizar-textos-y-datos-derivados",
    "title": "\n13  Herramientas: visualización\n",
    "section": "\n13.2 Visualizar textos y datos derivados",
    "text": "13.2 Visualizar textos y datos derivados\nVisualización de textos - el texto mismo permanece como base de la visualización. Podemos enriquecerlo con etiquetas y anotaciones, pero permanece como base de inspección.\nVisualización de datos a partir de textos - se generan una serie de datos cuantitativos a partir del procesamiento del corpus original y esto es lo que se visualiza.\nParece una distinción trivial. No obstante, resulta crucial para distintas tareas de análisis. Investigadores con un enfoque más cualitativo preferirán disponer de herramientas que les permitan enriquecer los textos y poder volver a ellos desde una perspectiva distinta. Por otra parte, otros con un acercamiento más cuantitativo buscarán opciones que permitan sintetizar y representar de forma estadística la estructura o aspectos que consideran centrales en los documentos."
  },
  {
    "objectID": "03_analisis_sintactico.html#reconocimiento-de-entidades-nombradas-ner",
    "href": "03_analisis_sintactico.html#reconocimiento-de-entidades-nombradas-ner",
    "title": "\n4  Análisis sintáctico\n",
    "section": "\n4.4 Reconocimiento de entidades nombradas (NER)",
    "text": "4.4 Reconocimiento de entidades nombradas (NER)\n“Named entity recognition is a technique for identifying whether a word or sequence of words represents an entity and what type of entity, such as a person or organization.” (Welbers, Van Atteveldt, and Benoit 2017, 18)\n“NER is the process of tagging people, organizations, and places within texts” (Grimmer, Roberts, and Stewart 2022, 164)\n“Named entity recognition helps identify the important entities in a text, to be able to derive the meaning from the unstructured data.” (Kumar and Paul 2016, 199)\n“Entity recognition is a sub process in the chain of information extraction process. NER is one of the important and vital parts of the information extraction process. NER is sometimes also called entity extraction or entity chunking .The main job of NER is to extract the rigid designators in the document and classify these elements in the text to a predefined category.” (Kumar and Paul 2016, 200)"
  }
]